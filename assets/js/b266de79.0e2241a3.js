"use strict";(self.webpackChunkblog_vertexcover=self.webpackChunkblog_vertexcover||[]).push([[518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"ml-infra-for-the-gpu-poor","metadata":{"permalink":"/ml-infra-for-the-gpu-poor","source":"@site/blog/2025-07-31-gpu-poor.mdx","title":"ML Infra design for the GPU Poor","description":"ML Infra design for the GPU Poor","date":"2025-07-31T00:00:00.000Z","tags":[{"inline":true,"label":"infra","permalink":"/tags/infra"}],"readingTime":4.22,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"ml-infra-for-the-gpu-poor","title":"ML Infra design for the GPU Poor","description":"ML Infra design for the GPU Poor","date":"2025-07-31T00:00:00.000Z","tags":["infra"],"draft":false},"unlisted":false,"nextItem":{"title":"Strot - The Review Parser","permalink":"/strot-is-a-review-parser"}},"content":"import TLDR from \'@site/src/components/TLDR\';\\nimport QueueingTheoryGraph from \'@site/src/components/QueueingTheoryGraph\';\\nimport QueueWaitChart from \'@site/src/components/QueueWaitChart\';\\nimport MultiQueueSystem from \'@site/src/components/MultiQueueSystem\';\\nimport SmartScalingSimulator from \'@site/src/components/SmartScalingSimulator\';\\n\\n## Taming the Beast: How to Design a Queueing System for GPU-Intensive Workloads\\n\\n<TLDR>\\nWhen designing for scale, the limiting factor is the GPU availability. So all rate limits / queueing must be designed around GPU availability.\\n</TLDR> \\n\\n**A guide for the aspiring software engineer on managing demand when your most critical resource is scarce.**\\n\\nImagine you\'re building a revolutionary video generation service. Your platform is a hit, but you have a problem\u2014a good problem, but a problem nonetheless. You have three types of customers, all knocking on your door at once:\\n\\n*   **B2C Customers:** Individuals using your web app to create short, fun videos. They expect results in seconds.\\n*   **B2B Giants:** Large enterprise clients who want to process massive workloads of tens of thousands of videos via your API.\\n\\nHere\u2019s the catch: the heart of your operation, the GPU, is a fixed and expensive resource. Unlike CPU and memory, which are elastic and can be scaled on demand, your GPU capacity is limited. One B2B giant\'s request could monopolize your entire system, leaving your B2C users staring at a loading spinner for hours.\\n\\nHow do you design a system that ingests all these requests, keeps every customer type happy, and doesn\'t crumble under the pressure of this GPU bottleneck? The answer lies in the mathematical study of waiting lines: **Queueing Theory**.\\n\\n### The Counter-Intuitive Truth About Being Busy\\n\\nBefore we design our system, we must understand a fundamental insight from queueing theory: **wait times skyrocket as utilization exceeds 80%**.\\n\\nIn simple terms, queueing theory uses a few key variables:\\n*   **Arrival Rate (\u03bb - Lambda):** How quickly tasks enter the queue.\\n*   **Service Rate (\u03bc - Mu):** How quickly a server can process tasks.\\n*   **Utilization (\u03c1 - Rho):** How busy a server is (\u03bb/\u03bc).\\n\\n<QueueWaitChart />\\n\\n### The Architect\'s Solution: From One Big Line to a Multi-Lane Highway\\n\\n#### 1. The Foundational Shift: The \\"Video-Minute\\" as the True Unit of Work\\n\\nThe system was re-architected around the concept of a \\"video-minute.\\" Instead of measuring queue length by the number of jobs, it was measured by the total duration of all videos waiting to be processed. This immediately provided a far more accurate picture of the outstanding load on the GPU fleet.\\n\\n#### 2. Architecture: The Two-Lane Highway\\nThe core idea of separating UI and batch users was validated and implemented.\\nAction: Two completely parallel infrastructures were created, each with its own dedicated queue and GPU fleet. This prevented any possibility of a \\"noisy neighbor\\" from the batch API world affecting the premium UI user experience. The UI queue was fed by a small, reserved pool of GPUs, while the batch queue was serviced by an auto-scaling fleet.\\n\\n\\n#### 3. Smart Scaling: Beyond Simple Queue Length\\n\\n<SmartScalingSimulator />\\n\\nYes, we should auto-scale the GPU, but how much? There are few places we can look for data to make informed choice.\\n\\n- Historical Learning: The scaling logic was made parametric to learn from historical provisioning times, helping it make smarter predictions about when to initiate scaling.\\n- Metric-Driven Scaling: The auto-scaler was triggered based on \\"video-minutes\\".\\n- Cold Start problem: There is 2-4 minute cold-start time for GPUs. So, provisioning a GPU only makes sense if the `video-minutes` to be processed are `X` times of cold start time. That way, frequent GPU provisioning does not lead inefficient compute. It\'s economically inefficient to provision a new GPU that takes 3 minutes to start, only to process a 2-minute video.\\n\\n#### 4. Predictability Through Pragmatism: Working backwards from limits\\n\\nThe team acknowledged a hard truth: their ability to autoscale was not infinite. Based on historical data and cloud provider limits, they identified a realistic maximum number of GPUs they could reliably provision.\\nThe fixed maximum capacity (Max GPUs * video-minutes per GPU) became the system\'s total throughput budget. This budget was then divided among the maximum number of potential concurrent batch users.\\nResult: This calculation yielded a crucial metric: the maximum video-minutes per minute that could be allocated to a single user. This became the foundation for providing realistic SLAs.\\n\\n#### 5. The SLAs: Per-Tenant Throttling and Dynamic ETAs\\n\\nThis per-user capacity allocation was not just a theoretical number; it was enforced at the ingestion layer.\\nAction: A user-aware rate limiter was placed at the front of the batch queue. If a user was allocated a capacity of \\"1 video-minute per minute\\", this gatekeeper would only release that amount of work from that user\'s batch into the main queue each minute.\\nAccurate SLAs: This system allowed for incredibly predictable SLAs. If a user with a 1 video-minute/minute capacity submitted a batch of 100 jobs totaling 200 video-minutes, the system could confidently return an ETA of ~200 minutes plus a safety buffer, because it knew exactly how fast that user\'s jobs would be fed to the processors."},{"id":"strot-is-a-review-parser","metadata":{"permalink":"/strot-is-a-review-parser","source":"@site/blog/2025-07-28-strot-review-parser.mdx","title":"Strot - The Review Parser","description":"This is a breakdown into what went into making Strot - The Review Parser.","date":"2025-07-28T00:00:00.000Z","tags":[],"readingTime":7.79,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"strot-is-a-review-parser","title":"Strot - The Review Parser","date":"2025-07-28T00:00:00.000Z","draft":false},"unlisted":false,"prevItem":{"title":"ML Infra design for the GPU Poor","permalink":"/ml-infra-for-the-gpu-poor"},"nextItem":{"title":"Teaching Claude Code to Work Independently","permalink":"/claude-code-context-engineering-v2"}},"content":"import ComparisonTable from \'../src/components/ComparisonTable\';\\nimport NumberedList from \'../src/components/NumberedList\';\\nimport TLDR from \'../src/components/TLDR\';\\nimport HierarchySection from \'../src/components/HierarchySection\';\\nimport SubSection from \'../src/components/SubSection\';\\nimport DeepDive from \'../src/components/DeepDive\';\\nimport TechnicalCard from \'../src/components/TechnicalCard\';\\n\\n\\nThis is a breakdown into what went into making Strot - The Review Parser. \\n\\n<TLDR>\\nStrot works in two steps:\\n\\n<NumberedList items={[\\n  \\"Intercepts the right ajax calls from the website based on both visual analysis and via captured HAR logs\\",\\n  \\"Uses those calls to fetch the reviews subsequently. So, getting top 100 reviews doesn\'t need to scrape the website AGAIN.\\"\\n]} />\\n</TLDR>\\n\\n\\n{/* truncate */}\\n\\n\\n## What if ... ?\\n\\nWhen is comes to scraping reviews, things CAN BE slightly different than running playwright scripts. That\'s the idea we started with! \\n\\nIt stems from the insights on browsing shopify. Since, shopify has plugin ecosystem, a lot of reviews come from plugins. They give the reviews in html / json form via api that the page renders. \\nSince, reviews - if they are worthwhile - would be in 100s for a given product, it would be very unlikely that someone with good web-dev practices will send all of them on one shot. There MUST be a second call (pagination) that requests for further reviews for the given product. \\n\\nWhat if ... we could scrape reviews via intercepting AJAX calls that are made from the browser? \\n\\n\\n## The Genesis \\n\\nHence, an experiement is born - called `strot`. In sanskrit it means source. We are trying to get to the ajax calls that gets us reviews. \\n\\nThe review scraping problem sits at a unique position. There are always MANY reviews which either causes cost ballooning no matter which existing approach you take.\\n\\n<DeepDive title=\\"System Architecture & Technical Approach\\">\\n\\nStrot\'s architecture consists of three main components working in sequence:\\n\\n**1. HAR File Analysis Pipeline**\\n- Captures network traffic using browser automation (Playwright/Puppeteer)\\n- Filters AJAX/XHR requests that match review-related patterns\\n- Builds a candidates list of potential review API endpoints\\n\\n**2. Visual-Content Correlation Engine**\\n- Takes screenshot of the page showing reviews\\n- Uses vision models (Claude-4-sonnet) to extract review text from screenshots\\n- Performs fuzzy matching between screenshot content and API response data\\n\\n**3. Pagination Strategy Detection**\\n- Analyzes request parameters to identify pagination patterns\\n- Common patterns: `page`, `offset`, `cursor`, `after`, `next_token`\\n- Reverse engineers pagination logic from multiple API calls\\n\\n</DeepDive>\\n\\n<TechnicalCard type=\\"architecture\\" title=\\"Why AJAX Interception vs DOM Scraping\\">\\n\\nTraditional DOM scraping faces several fundamental limitations:\\n\\n- **Rate Limiting**: Websites throttle based on page loads, not API calls\\n- **Bot Detection**: Full page loads trigger anti-bot measures (Cloudflare, etc.)\\n- **Resource Overhead**: Loading entire DOM + assets vs lightweight JSON responses\\n- **Maintenance Burden**: CSS selectors break with UI changes, APIs rarely change\\n\\nAJAX interception bypasses these by operating at the data layer, not presentation layer.\\n\\n</TechnicalCard> \\n\\n<ComparisonTable \\n  data={[\\n    {\\n      feature: \'Cost\',\\n      playwright: { text: \'Infrastructure costs for automation\', score: \'medium\' },\\n      llmScraper: { text: \'Pay per page + LLM inference\', score: \'poor\' },\\n      strot: { text: \'One-time API discovery\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Maintenance\',\\n      playwright: { text: \'High - UI changes break scripts\', score: \'poor\' },\\n      llmScraper: { text: \'Medium - prompt engineering needed\', score: \'medium\' },\\n      strot: { text: \'Low - APIs rarely change\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Intelligence Required\',\\n      playwright: { text: \'High - manual scraper\', score: \'poor\' },\\n      llmScraper: { text: \'High - per page analysis\', score: \'poor\' },\\n      strot: { text: \'One-time - for API discovery\', score: \'medium\' }\\n    },\\n    {\\n      feature: \'Scalability\',\\n      playwright: { text: \'Poor - loads full pages\', score: \'poor\' },\\n      llmScraper: { text: \'Expensive - each page costs $\', score: \'poor\' },\\n      strot: { text: \'Excellent - pagination via API\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Setup Complexity\',\\n      playwright: { text: \'Medium - browser automation\', score: \'medium\' },\\n      llmScraper: { text: \'Low - plug and play\', score: \'good\' },\\n      strot: { text: \'Low - plug and play\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Uniqueness\',\\n      playwright: { text: \'Common approach\', score: \'poor\' },\\n      llmScraper: { text: \'Common approach\', score: \'poor\' },\\n      strot: { text: \'Unique to us\', score: \'good\' }\\n    }\\n  ]}\\n  columns={[\'playwright\', \'llmScraper\', \'strot\']}\\n  columnHeaders={[\'Playwright\', \'LLM Scraper\', \'Strot (Ours)\']}\\n/>\\n\\nSo, IF we could find the ajax call \u2192 we only have to spend time figuring out the API call. and voila! You can simply call API - this is cheapest, fastest, most reliable approach of all. \\n\\nAnd the unique to us. Hence, we took a bite. \\n\\nBut the challenge is which api call is most relevnt for scraping reviews ? \\nwe solve this by capturing screenshot, visually analysing if review is preseng in the screenshot. Then we find a matching ajax call that has a similar content. Voila! \\n\\n<TechnicalCard type=\\"implementation\\" title=\\"AJAX Call Matching Algorithm\\">\\n\\nOur matching algorithm works in stages:\\n\\n1. **Content Extraction**: Extract review text from screenshots using OCR + Vision LLM\\n2. **API Response Analysis**: Parse all captured AJAX responses for text content\\n3. **Fuzzy String Matching**: Use algorithms like Levenshtein distance, Jaccard similarity\\n4. **Confidence Scoring**: \\n   - Text overlap ratio (>90% = high confidence)\\n   - JSON structure analysis (nested objects, review-like fields)\\n   - Response size correlation with visible review count\\n\\n**Edge Case Handling**: \\n- Paginated responses (partial matches expected)\\n- Localized content (different languages)\\n- Dynamic timestamps/IDs (filter out volatile fields)\\n\\n</TechnicalCard>\\n\\n<TechnicalCard type=\\"performance\\" title=\\"Why Screenshot Analysis vs Pure Network Analysis\\">\\n\\nSince we have to do the hard part of finding the right AJAX call ONLY once, it means that we can use augmentation from vision understanding to make it more accurate. \\n\\nThis helps by:\\n- Confirms which API calls actually render user-visible content. \\n- Handles cases where multiple API calls contain review data\\n- Eliminates false positives from internal/admin API calls\\n\\n</TechnicalCard>\\n\\n## The challenges of current approaches \\n\\nEach traditional approach faces fundamental limitations that compound at scale:\\n\\n<TechnicalCard type=\\"edge-case\\" title=\\"The Review Volume Problem\\">\\n\\nMost products with worthwhile reviews have 100-10,000+ reviews. Traditional approaches cost:\\n\\n- **Playwright**: 100 reviews \xd7 3 pages \xd7 10s load time = 30+ minutes\\n- **LLM Scraper**: 100 reviews \xd7 $0.10 per page = $10+ per product\\n- **Manual API**: 100 reviews \xd7 15min engineer time = $200+ per product\\n\\nStrot\'s approach: 100 reviews \xd7 0.1s API call = 10 seconds total.\\n\\n</TechnicalCard>\\n\\n## The codegen \\n\\nWe internally represent all the relevant information as json. This helps us to generate http client in any language of choice. Calling this API endpoint in succession will give all the reviews.\\n\\n<DeepDive title=\\"Code Generation Pipeline & Implementation\\">\\n\\nOur codegen system transforms discovered API patterns into production-ready code:\\n\\n**1. API Pattern Extraction**\\n```json\\n{\\n  \\"endpoint\\": \\"https://example.com/api/reviews\\",\\n  \\"method\\": \\"POST\\",\\n  \\"headers\\": {\\n    \\"content-type\\": \\"application/json\\",\\n    \\"x-requested-with\\": \\"XMLHttpRequest\\"\\n  },\\n  \\"pagination\\": {\\n    \\"type\\": \\"offset\\",\\n    \\"param\\": \\"offset\\",\\n    \\"limit_param\\": \\"limit\\",\\n    \\"max_per_request\\": 20\\n  }\\n}\\n```\\n\\n**2. Language-Specific Generation**\\n- **Python**: Uses `requests`. Can use `rnet` if want to bypass TLS fingerprinting.\\n- **Node.js**: `axios` with async/await patterns\\n- **cURL**: Direct shell commands with proper escaping\\n\\n</DeepDive>\\n\\n<HierarchySection title=\\"Iteratively Improving via Error Analysis and Evals\\">\\n\\nEvals were central to us while building this. We had three tiered system for evals that served us well.\\n\\n<NumberedList items={[\\n  \\"Does the analysis identify the right ajax call?\\",\\n  \\"Is the pagination strategy detection done correctly?\\",\\n  \\"Is the http api (codegen) able to get all the reviews?\\"\\n]} />\\n\\n<DeepDive title=\\"Comprehensive Evaluation System Architecture\\">\\n\\nOur evaluation system operates at three levels with automated scoring:\\n\\n**Level 1: AJAX Call Detection**\\n- **Ground Truth**: Manual verification of correct API endpoints (part of **golden dataset** for evals)\\n- **Automated Checks**:\\n  - Response contains review-like JSON structure (text, rating, author)\\n  - Content overlap with screenshot text >85%\\n\\n**Level 2: Pagination Strategy**\\n- **Test Suite**: Tested pagination patterns (cursor, limit, offset) across major e-commerce sites\\n- **Validation Logic**:\\n  - Do we hit natural boundaries (page N returns empty/error)?\\n\\n**Level 3: End-to-End Codegen**\\n- **Integration Tests**: Generated code fetches reviews from 10 test products\\n- **Quality Metrics**:\\n  - Duplicate detection (same review across pages)\\n  - Data completeness (rating, text, date, author)\\n\\n</DeepDive>\\n\\n<SubSection title=\\"Error Analysis\\">\\n\\nThis was the most consuming element for us until we vibe-coded initial version of this dashboard to see the logs as data.\\n\\n<TechnicalCard type=\\"implementation\\" title=\\"Observability Dashboard\\">\\n\\nOur observability dashboard processes shows data from:\\n\\n\\n<NumberedList items={[\\n  \\"Application logs (structured JSON)\\",\\n  \\"LLM API call traces (OpenAI/Anthropic)\\",\\n  \\"Playwright screenshots\\"\\n]} />\\n\\n\\nThis helps to do the error analysis and improve the system iteratively.\\n</TechnicalCard>\\n\\nHere is a quick peek over the dashboard we made to do the error analysis:\\n\\nThis shows us the cost and token usage:\\n<a href=\\"/img/strot-dashboard-analysis.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-analysis.png\\" alt=\\"Strot dashboard cost and token analysis\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\nThis shows us the step by step observability into what is happening:\\n<a href=\\"/img/strot-dashboard-step.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-step.png\\" alt=\\"Strot dashboard step by step analysis\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\n\\n</SubSection>\\n\\n<SubSection title=\\"Pagination\\">\\n\\n<a href=\\"/img/strot-dashboard-pagination.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-pagination.png\\" alt=\\"Strot dashboard pagination\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\n</SubSection>\\n \\n<SubSection title=\\"Codegen\\">\\n\\n<a href=\\"/img/strot-dashboard-codegen.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-codegen.png\\" alt=\\"Strot dashboard codegen\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\n</SubSection>\\n\\n</HierarchySection>\\n\\n\\nWe are also learning. This was our attempt at scraping the webpage and using the visual understanding of image models to get to the reviews faster. if you feel this can be improved, feel free to share those with us on [github-issues](https://github.com/vertexcover-io/strot/issues).\\n\\n\\n## Roadmap \\n\\n<NumberedList items={[\\n  \\"We are already on a path to make this a generic scraper. Currently, our evals sets show the tracking progress on various websites. It will be released soon.\\",\\n  \\"if you would like us to host this as a service for you, we are more than happy to. Come chat with us at [contact email]\\"\\n]} />"},{"id":"claude-code-context-engineering-v2","metadata":{"permalink":"/claude-code-context-engineering-v2","source":"@site/blog/2025-07-26-claude-code.mdx","title":"Teaching Claude Code to Work Independently","description":"<TldrCallout","date":"2025-07-21T00:00:00.000Z","tags":[{"inline":false,"label":"Context Engineering","permalink":"/tags/context-engineering","description":"Context Engineering"}],"readingTime":6.94,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"claude-code-context-engineering-v2","title":"Teaching Claude Code to Work Independently","date":"2025-07-21T00:00:00.000Z","authors":["abeeshake"],"tags":["context-engineering"],"draft":false},"unlisted":false,"prevItem":{"title":"Strot - The Review Parser","permalink":"/strot-is-a-review-parser"},"nextItem":{"title":"AI for End-to-End Tests (Mobile too!) with Auto Healing","permalink":"/ai-agent-end-to-end-automated-tests-mobile"}},"content":"import TldrCallout from \'@site/src/components/TldrCards/TldrCallout\';\\nimport TipsSection from \'@site/src/components/TipsSection\';\\nimport MinimalDetails from \'@site/src/components/MinimalDetails\';\\nimport ScriptAttribution from \'@site/src/components/ScriptAttribution\';\\nimport CalendlyButton from \'@site/src/components/CalendlyButton\';\\n\\n\\n\\n<TldrCallout \\n  problem=\\"You\'re trapped micromanaging Claude Code instead of building\\"\\n  solution=\\"Train Claude Code once, then let it work autonomously\\"\\n  how=\\"Strategic context engineering: CLAUDE.md files, systematic workflows, and learning capture\\"\\n  benefit=\\"Claude Code becomes your autonomous teammate\\"\\n/>\\n\\nHere is [CLAUDE.md](https://gist.github.com/tripathi456/bfaf9add4b70bff131cd574c2f93cfac)\\n\\n\\n{/* truncate */}\\n\\n## Day 0: Just moved from cursor to claude code\\n\\n*It\'s 11 PM on a Tuesday. My token budget just hit zero. Claude Code is asking me the same question for the fourth time: \\"What coding style does this project use?\\" I\'ve spent 6 hours being a glorified copy-paste machine, explaining the same context over and over.*\\n\\n*Sound familiar?*\\n\\nThat night, I realized something critical: **Claude Code is teachable**. But I was the worst teacher on Earth.\\n\\n## Day 1: Fighting Claude Code (The Problem)\\n\\nEvery conversation was Groundhog Day:\\n\\n- \\"What\'s our testing framework again?\\"\\n- \\"How do we name files in this project?\\"  \\n- \\"What\'s the deployment process?\\"\\n\\nI was Claude Code\'s personal Wikipedia. **This had to change.**\\n\\n## Day 2: The Teaching Template (The Mentor)\\n\\n> What if Claude Code could remember your project like a team member who\'s been there for years?\\n\\nInstead of explaining everything every time, I wrote a template. The gist is:\\n\\n```md\\n# Communication: Be concise, reference past learnings from docs/work/\\n# File Naming: YYYY-MM-DD-[001]-[category]-[summary].md  \\n# Never code without checking docs/work/ for similar past solutions\\n```\\n\\nThat\'s it. **This eliminated hours of repetitive context.**\\n\\n<MinimalDetails summary=\\"See the prompt \u2192\\">\\n\\n```md\\n# Project: [Your Project Name]\\n\\n## Tech Stack & Tooling\\n- **Language**: Python 3.11+\\n- **Package Manager**: `uv` (use `uv add <dependency>`, `uv run <script>`)\\n- **Testing**: pytest with coverage\\n- **Linting**: ruff + mypy\\n\\n## Systematic File Naming\\nFormat: `YYYY-MM-DD-[001-999]-[category]-[four-word-summary].md`\\nFolder: `docs/work/`\\nCategories: `bug` | `feature` | `task` | `research` | `learnings`\\n\\nExamples:\\n- `2025-07-18-001-feature-user-authentication-system.md`\\n- `2025-07-18-002-bug-database-connection-timeout.md`\\n\\n## Communication Style\\n- **Concise**: No fluff, direct responses\\n- **Evidence-based**: Show, don\'t just tell\\n- **Contextual**: Reference past learnings from `docs/work/`\\n\\n## Planning Protocol\\n1. **Context Gathering**: Check `docs/work/` for relevant past decisions\\n2. **Assumption Documentation**: Explicit assumptions in plan files\\n3. **Execution Gate**: Only proceed after planning is complete\\n```\\n\\n</MinimalDetails>\\n\\n**The transformation was instant.** Claude Code started referencing past decisions, avoiding repeated mistakes, and building on previous work. **It finally felt like working with a teammate.**\\n\\n## Day 3: The Learning Moment - 30% context - (The Training)\\n\\n> What if Claude Code could learn from every mistake and never repeat it?\\n\\nWhen your context hits 30%, you have one chance to crystallize everything learned. Miss it, and you go back to day 1.\\n\\n\\n\\n<MinimalDetails summary=\\"See the prompt \u2192\\">\\n\\n```md\\n\\nWhen context drops below 30%: \\n\\n1. Document every decision made\\n2. List what failed (with code snippets)  \\n3. Note what worked brilliantly\\n4. Write handoff notes for next session\\n\\nUse the `Systematic File Naming` given above.\\n```\\n\\n</MinimalDetails>\\n\\n\\n## Day 4: The Autonomous Engineer (The Victory)\\n\\nI saw this tweet - [@svs used Claude Code as an MCP client to write an MCP server](https://x.com/_svs_/status/1928753160337637726). \\n\\nSomething magical discovered: **When put in verifiable workflows, Claude Code started working much better!**:\\n\\n- Write code \u2192 Run tests \u2192 Fix failures \u2192 Repeat\\n- Build feature \u2192 Deploy to staging \u2192 Check logs \u2192 Iterate  \\n- Analyze data \u2192 Generate insights \u2192 Verify against sources \u2192 Summarize\\n\\n**I wasn\'t micromanaging anymore. I was collaborating.**\\n\\n## Stories from other folks\\n\\nMet awesome folks at [Fifth Elephant Conference](https://hasgeek.com/fifthelephant/2025/) where we shared claude code learnings.\\n\\n<TipsSection contributor=\\"Rajesh\\" contributorUrl=\\"https://www.linkedin.com/in/codingnirvana/\\">\\n\\n### The Token Budget Hack That Doubled My Productivity\\n\\nRajesh was burning through tokens like crazy at his startup. Then he discovered something weird about Claude\'s [5-hour windows](https://support.anthropic.com/en/articles/11145838-using-claude-code-with-your-pro-or-max-plan).\\n\\n**His discovery:** Start sessions at 7 AM instead of 9 AM.\\n\\nWhy? The overlapping windows create a \\"double token zone\\" during peak hours:\\n\\n**Before:** 9am-2pm, 2pm-7pm (standard)  \\n**After:** 7am-12pm, 12pm-5pm, 5pm-10pm (overlapping)\\n\\n**Result:** Between 9am-5pm = **double tokens available**\\n\\n\\n\\n<img src=\\"/img/timeline.svg\\" />\\n\\n\\n\\n### The CSV Strategy That Saved Hours\\n\\nRajesh\'s team was dealing with lots of data analysis requests. CSV files everywhere. Claude kept hitting context limits trying to process raw data.\\n\\n**The breakthrough:** Stop feeding Claude data. Feed it scripts.\\n\\n**Old way:** \\"Here\'s a 10MB CSV, analyze it\\"  \\n**New way:** \\"Write a script to analyze this CSV type, then run it\\"\\n\\n**Why it works:** Scripts are tiny. Results are focused. Claude guides itself using its own analysis output. \\n\\n</TipsSection>\\n\\n<TipsSection contributor=\\"Ashwant\\" contributorUrl=\\"https://www.shelfradar.ai/\\">\\n\\n### The Accidental Discovery That Changed Everything\\n\\nAshwant was debugging a frustrating session. In a moment of rage, he accidentally hit ESC four times.\\n\\n**What happened next blew his mind.**\\n\\nClaude Code showed him a prompt history he\'d never seen before. Every conversation. Every context. **Time travel for developers.**\\n\\n**The magic combo:** ESC + ESC + ESC + ESC = Prompt history navigation\\n\\n**Game changer:** You can resurrect any previous session state instantly.\\n\\n### Claude Code as Your Database Whisperer\\n\\nAt ShelfRadar, Ashwant deployed Claude Code as their internal SQL agent. \\n\\n**The setup:** Claude Code + database schema = autonomous query optimizer\\n\\n**The result:** \\n\\n- Ad-hoc queries refined automatically\\n- Schema changes don\'t break queries  \\n- Claude evolves with your database\\n\\n</TipsSection>\\n\\n**The question isn\'t whether Claude Code is teachable.**  \\n**The question is: Are you ready to become its teacher?**\\n\\nUpdate 1: Claude code introduced hooks. Here is a short collection of hooks that I find useful for my workflow.\\n\\n1. bash script to notify the end of claude code turns. \\n\\n\\n<MinimalDetails summary=\\"Full bash script \u2192\\">\\n```bash\\n#!/bin/bash\\n\\n# Read hook input data\\nINPUT=$(cat)\\nSESSION_DIR=$(basename \\"$(pwd)\\")\\n\\n# Extract message from transcript if available\\nTRANSCRIPT_PATH=$(echo \\"$INPUT\\" | jq -r \'.transcript_path\')\\nif [ -f \\"$TRANSCRIPT_PATH\\" ]; then\\n  MSG=$(tail -10 \\"$TRANSCRIPT_PATH\\" |\\n    jq -r \'select(.message.role == \\"assistant\\") | .message.content[0].text\' |\\n    tail -1 | tr \'\\\\n\' \' \' | cut -c1-60)\\n  MSG=${MSG:-\\"Task completed\\"}\\nelse\\n  MSG=\\"Task completed\\"\\nfi\\n\\n# Show Linux desktop notification (requires notify-send)\\nnotify-send \\"Claude Code ($SESSION_DIR) Done\\" \\"$MSG\\"\\n```\\n\\n</MinimalDetails>\\n\\n<ScriptAttribution\\n  title=\\"Useful Claude Code Hooks & Scripts\\"\\n  description=\\"\\"\\n  links={[\\n    {\\n      url: \\"https://gist.github.com/glennmatlin/fadc41edc3bb9ff68ff9cfa5d6b8aca7\\",\\n      title: \\"Making Claude use uv instead of pip\\",\\n      description: \\"Script for making Claude use uv package manager instead of pip\\"\\n    },\\n    {\\n      url: \\"https://conductor.build/\\",\\n      title: \\"Running multiple Claude Code sessions in parallel\\",\\n      description: \\"Using git worktrees for parallel Claude Code sessions\\"\\n    },\\n    {\\n      url: \\"https://www.reddit.com/r/ClaudeAI/comments/1loodjn/claude_code_now_supports_hooks/\\",\\n      title: \\"Claude Code Hooks Discussion\\",\\n      description: \\"Reddit discussion about Claude Code hooks support\\"\\n    }\\n  ]}\\n/>\\n\\n\\n---\\n\\n*This revolution moves fast. By the time you read this, someone\'s already teaching Claude Code to do things we haven\'t imagined yet.*\\n\\n**Further Reading:**\\n- [Context Engineering by Manus](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus) \\n- [Hrishi\'s Claude Code Analysis](https://southbridge-research.notion.site/claude-code-an-agentic-cleanroom-analysis)\\n- [Claude Code Sub-Agents Documentation](https://docs.anthropic.com/en/docs/claude-code/sub-agents)\\n- [Claude Code Hooks Guide](https://docs.anthropic.com/en/docs/claude-code/hooks-guide)\\n\\n---\\n\\n## Are Your Developers Working FOR Claude Code? Or Is Claude Code Working FOR Them?\\n\\nRight now, your engineers spend 30% of their AI time being Claude\'s personal assistants - explaining context, re-describing architecture, and hand-holding every request. **Meanwhile, Anthropic\'s teams reduced research time by 80% and debug 3x faster** because Claude Code works FOR them.\\n\\n**The Reality Check:** Your team bought the most powerful coding AI ever built, then turned themselves into its unpaid interns.\\n\\n**The Transformation:** Stop being Claude\'s employee. Make Claude Code your team\'s autonomous coding partner that knows your codebase better than your junior developers.\\n\\n**Flip the Script: Make Claude Code Work FOR Your Team**\\n\\nTransform your developers from AI babysitters into AI commanders:\\n- Setting up CLAUDE.md files that eliminate context re-explaining  \\n- Building verifiable workflows that turn debugging from hours to minutes\\n- Token optimization strategies that double your effective usage\\n- Advanced automation that makes Claude Code your team\'s autonomous teammate\\n\\n*Read how [Anthropic\'s teams achieve these results](https://www.anthropic.com/news/how-anthropic-teams-use-claude-code) and learn to implement the same strategies for your startup.*\\n\\n<CalendlyButton \\n  url=\\"https://calendly.com/abhishek-vertexcover/claude-code\\"\\n  text=\\"Book Your 30-Minute Consultation Call\\"\\n  description=\\"Transform from Claude Code user to Claude Code teacher. Your future autonomous development workflow starts with one conversation.\\"\\n/>"},{"id":"ai-agent-end-to-end-automated-tests-mobile","metadata":{"permalink":"/ai-agent-end-to-end-automated-tests-mobile","source":"@site/blog/2025-07-10-flowtest/index.mdx","title":"AI for End-to-End Tests (Mobile too!) with Auto Healing","description":"AI for End-to-End Tests (Mobile too!)","date":"2025-07-10T00:00:00.000Z","tags":[{"inline":false,"label":"AI Agents","permalink":"/tags/ai-agents","description":"AI Agents"},{"inline":false,"label":"End-to-End Test","permalink":"/tags/end-to-end-test","description":"End-to-End Test"}],"readingTime":3.67,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"ai-agent-end-to-end-automated-tests-mobile","title":"AI for End-to-End Tests (Mobile too!) with Auto Healing","description":"AI for End-to-End Tests (Mobile too!)","date":"2025-07-10T00:00:00.000Z","tags":["ai-agents","end-to-end-testing"]},"unlisted":false,"prevItem":{"title":"Teaching Claude Code to Work Independently","permalink":"/claude-code-context-engineering-v2"}},"content":"### AI Agent for End-to-End Testing to Deliver Flawless Digital Experiences\\n\\n---\\n\\nWhat if Ai Agent could write tests for your codebase? End-to-end? and for mobile too? and it auto heals / auto-adjusts when your codebase changes?\\n\\nWe share nuggets we learnt while building an AI Agent to solve one of the most persistent challenges in software development: making UI test automation accessible, reliable, and scalable across platforms and devices.\\n\\n{/* truncate */}\\n\\n\\n### The Problem\\n\\nTest automation has historically been:\\n\\n- **Too technical**: requiring code expertise\\n- **Time-consuming**: for authoring and maintaining scripts\\n- **Platform-limited**: with fragmented support for web vs. mobile\\n- **Fragile**: breaking with minor UI changes or incomplete user flows\\n\\nExisting tools were not built for the demands of today\'s fast-moving, multi-platform development cycles. They struggled particularly with hybrid apps, dynamic interfaces, and gesture-driven experiences.\\n\\n---\\n\\n### The Solution: AI Agent\\n\\nAI agent designed from the ground up as an intelligent, prompt-driven automation system with key capabilities:\\n\\n### Natural Language to Automation Code\\n\\nUsers describe test scenarios in plain English. It translates them into precise, executable test scripts\u2014including test data, validations, and edge cases.\\n\\n![flow-diagram](flow-diagram.png)\\n\\n### Web & Mobile App Testing\\n\\nSupports both web (Selenium, Playwright) and mobile (Appium) frameworks, making it one of the few solutions that bridges the gap between platforms seamlessly.\\n\\n### Multi-Language Support\\n\\nGenerates scripts in Java, Python, JavaScript, and other frameworks\u2014tailored to the team\'s existing tech stack.\\n\\n### Smart Debugging\\n\\nExecutes each script line in real-time as it\'s generated, identifying and correcting issues on the fly.\\n\\n### Cross-Device Execution\\n\\nRun tests instantly across 5,000+ combinations of real browsers and devices (when connected to cloud infrastructure).\\n\\n### Self-Healing Automation\\n\\nDetects and updates selectors and steps automatically as the application evolves, eliminating the need for manual maintenance.\\n\\n---\\n\\n### Core Technical Challenges Solved\\n\\n### UI Automation for Flutter Web and Hybrid Mobile Apps\\n\\nMost automation tools break down on platforms like Flutter Web, where the UI is rendered inside a `<canvas>` instead of standard HTML elements, and on hybrid apps without accessible DOM trees.\\n\\n**Our agent solved this by enabling interaction with non-standard UIs using a combination of visual, contextual, and heuristic techniques**\u2014delivering true end-to-end automation where no other solution worked.\\n\\n---\\n\\n### Accurate and Reusable Script Generation\\n\\nRunning LLMs for every test execution is expensive and error-prone.\\n\\nThis AI Agent implemented a **novel templating and generation system** that decouples script generation from execution. This allowed:\\n\\n- Complete and correct scripts on the first pass\\n- Reusability across test runs and frameworks\\n- Fast, low-cost, scalable test creation\\n\\n---\\n\\n### Complex Gestures and UI Behaviors\\n\\nSimulating gestures like pinch, zoom, drag-and-drop, or multi-touch is notoriously hard\u2014especially in custom components.\\n\\nThis agent provided **fine-grained control** over gesture simulation, going beyond the abstractions of typical libraries, enabling accurate testing of sliders, carousels, maps, and more.\\n\\n---\\n\\n### Mapping Natural Language to UI Actions Reliably\\n\\nNatural language like \u201cClick the Pay button\u201d can be ambiguous\u2014especially in large, dynamic UIs.\\n\\n**Being very smart, it combined multiple modalities\u2014DOM structure, visual layout, and semantic cues**\u2014to reliably identify elements even when conventional locators failed. This enabled it to handle vague prompts and incomplete context with high precision.\\n\\n---\\n\\n### Stability in Dynamic & Incomplete User Flows\\n\\nIn real-world apps, pop-ups appear unexpectedly, elements load asynchronously, and flows can be interrupted.\\n\\nIt was was designed to **recover intelligently** from such cases using retry logic, timeout strategies, and partial flow handling. This brought production-grade resilience to end-to-end test execution.\\n\\n---\\n\\n### Scalable Evaluation & Debugging Infrastructure\\n\\nA major limitation of LLM-based systems is the lack of robust evaluation.\\n\\nAI Agent addressed this by building a **custom evaluation framework** that:\\n\\n- Validated script correctness at each step\\n- Enabled partial re-execution of scripts\\n- Provided fine-grained feedback for model improvement\\n    \\nThis drastically accelerated iteration speed and allowed for deeper validation of system accuracy.\\n    \\n\\n---\\n\\n### Conclusion\\n\\nAI Agent redefines what\'s possible in test automation. By combining the reasoning power of LLMs with robust engineering for execution, gesture control, and UI resilience, it makes test automation accessible to non-engineers while retaining power for experts.\\n\\nFrom tackling the hardest UI platforms like Flutter Web to enabling precise, reusable test generation and execution at scale, our custom AI agent is a leap forward in the world of quality engineering.\\n\\nIt\'s not just a tool\u2014it\'s a full-stack AI agent that understands, adapts, and evolves with your application."}]}}')}}]);