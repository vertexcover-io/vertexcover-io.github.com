"use strict";(self.webpackChunkblog_vertexcover=self.webpackChunkblog_vertexcover||[]).push([[518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"ai-observability-evaluation-framework","metadata":{"permalink":"/ai-observability-evaluation-framework","source":"@site/blog/2025-09-02-strot-ai-observability-evaluation-system/index.mdx","title":"How We Built Observability and Evaluation for Strot","description":"When you build AI systems that reverse-engineer websites \u2014 like Strot \u2014 you need to know exactly where things break and how to fix them fast.","date":"2025-09-02T00:00:00.000Z","tags":[],"readingTime":4.83,"hasTruncateMarker":true,"authors":[{"name":"Harsh Verma","title":"Software Engineer","url":"https://github.com/synacktraa","page":{"permalink":"/authors/synacktra"},"socials":{"linkedin":"https://www.linkedin.com/in/synacktra/","github":"https://github.com/synacktraa"},"imageURL":"https://github.com/synacktraa.png","key":"synacktra"}],"frontMatter":{"slug":"ai-observability-evaluation-framework","title":"How We Built Observability and Evaluation for Strot","authors":["synacktra"],"date":"2025-09-02T00:00:00.000Z","draft":false},"unlisted":false,"nextItem":{"title":"Speed up Docker Builds on Github actions","permalink":"/speed-up-docker-builds-on-github-actions"}},"content":"import NumberedList from \'@site/src/components/NumberedList\';\\r\\nimport TLDR from \'@site/src/components/TLDR\';\\r\\nimport TechnicalCard from \'@site/src/components/TechnicalCard\';\\r\\nimport DeepDive from \'@site/src/components/DeepDive\';\\r\\nimport ImageSlider from \'@site/src/components/ImageSlider\';\\r\\n\\r\\n<TLDR>\\r\\nWhen you build AI systems that reverse-engineer websites \u2014 like [Strot](https://github.com/vertexcover-io/strot) \u2014 you need to know **exactly where things break** and **how to fix them fast**.\\r\\n\\r\\nWe built two systems that work together:\\r\\n\\r\\n<NumberedList items={[\\r\\n  \\"Real-time observability for live debugging with visual screenshots and LLM traces\\",\\r\\n  \\"Multi-level evaluation for quality assurance with component isolation\\",\\r\\n  \\"Structured logging foundation that powers both systems with zero overhead\\"\\r\\n]} />\\r\\n</TLDR>\\r\\n\\r\\nHere\'s how we did it.\\r\\n\\r\\n{/* truncate */}\\r\\n\\r\\n---\\r\\n\\r\\n## The Problem: Debugging AI at Scale\\r\\n\\r\\nStrot\'s pipeline has three AI components that can each fail uniquely across thousands of websites:\\r\\n\\r\\n| Component | What It Does | Example Failure |\\r\\n|-----------|--------------|-----------------|\\r\\n| **Request Detection** | Finds network calls loading data | Picks non-related network call |\\r\\n| **Parameter Analysis** | Detects pagination & filter params | Misses `cursor` param, breaks pagination |\\r\\n| **Structured Extraction** | Generates Python to parse responses | Wrong JSON key mapping |\\r\\n\\r\\nWe needed to see failures **as they happen** and test fixes **without running the whole pipeline**.\\r\\n\\r\\n---\\r\\n\\r\\n## Solution 1: Real-Time Observability Dashboard\\r\\n\\r\\n<TechnicalCard type=\\"implementation\\" title=\\"Visual Debugging Dashboard\\">\\r\\n\\r\\n**What we built:** A NextJS dashboard that shows exactly how the pipeline executes.\\r\\n\\r\\n**What it shows:**\\r\\n\\r\\n<NumberedList items={[\\r\\n  \\"Browser screenshots at each step\\",\\r\\n  \\"Every LLM request/response with costs\\", \\r\\n  \\"Generated code and validation results\\",\\r\\n  \\"Real-time token usage and spending\\"\\r\\n]} />\\r\\n\\r\\n**Why custom-built:** Off-the-shelf observability tools don\'t understand AI pipeline context like LLM reasoning, browser automation steps, or parameter detection results.\\r\\n\\r\\n</TechnicalCard>\\r\\n\\r\\nimport observability_analysis_report_view from \\"./observability-analysis-report-view.png\\";\\r\\nimport observability_raw_log_view from \\"./observability-raw-log-view.png\\";\\r\\nimport observability_request_detection_step_view from \\"./observability-request-detection-step-view.png\\";\\r\\nimport observability_parameter_detection_view from \\"./observability-parameter-detection-view.png\\";\\r\\nimport observability_structured_extraction_view from \\"./observability-structured-extraction-view.png\\";\\r\\n\\r\\n<ImageSlider \\r\\n  images={[\\r\\n    { \\r\\n      src: observability_analysis_report_view, \\r\\n      caption: \\"Analysis report overview\\" \\r\\n    },\\r\\n    { \\r\\n      src: observability_raw_log_view, \\r\\n      caption: \\"Raw logs\\" \\r\\n    },\\r\\n    { \\r\\n      src: observability_request_detection_step_view, \\r\\n      caption: \\"Request detection step\\" \\r\\n    },\\r\\n    { \\r\\n      src: observability_parameter_detection_view, \\r\\n      caption: \\"Parameter detection step\\" \\r\\n    },\\r\\n    { \\r\\n      src: observability_structured_extraction_view, \\r\\n      caption: \\"Structured extraction step\\" \\r\\n    }\\r\\n  ]}\\r\\n/>  \\r\\n\\r\\n**Why it matters:** \\r\\n- Debug visually \u2014 see the exact browser state when AI makes decisions\\r\\n- Track costs live \u2014 optimize model usage immediately\\r\\n- Diagnose failures instantly \u2014 no waiting for pipeline completion\\r\\n\\r\\n---\\r\\n\\r\\n## Solution 2: Multi-Level Evaluation\\r\\n\\r\\nWe evaluate at three levels:\\r\\n\\r\\n### 1. Component Tests\\r\\n\\r\\nTest each AI step in isolation:\\r\\n\\r\\n```bash\\r\\necho \'[{\\r\\n  \\"request\\": {\\r\\n    \\"url\\": \\"https://api.example.com/products\\",\\r\\n    \\"queries\\": {\\"page\\": \\"2\\", \\"limit\\": \\"50\\"},\\r\\n    \\"post_data\\": {\\"sort\\": \\"price\\", \\"filter\\": \\"new\\"}\\r\\n  },\\r\\n  \\"expected_pagination_keys\\": [\\"page\\", \\"limit\\"],\\r\\n  \\"expected_dynamic_keys\\": [\\"sort\\", \\"filter\\"]\\r\\n}]\' | uv run stroteval\\r\\n```\\r\\n\\r\\n**Result:** 10\xd7 faster debugging, every fix becomes a regression test.\\r\\n\\r\\nimport component_eval_parameter_detection_first_view from \'./component-eval-parameter-detection-first-view.png\'\\r\\nimport component_eval_parameter_detection_second_view from \'./component-eval-parameter-detection-second-view.png\'\\r\\n\\r\\n<ImageSlider \\r\\n  images={[\\r\\n    { \\r\\n      src: component_eval_parameter_detection_first_view, \\r\\n      caption: \\"Parameter detection request input\\" \\r\\n    },\\r\\n    { \\r\\n      src: component_eval_parameter_detection_second_view, \\r\\n      caption: \\"Parameter detection comparison and comment\\" \\r\\n    }\\r\\n  ]}\\r\\n/>\\r\\n\\r\\n<TechnicalCard type=\\"performance\\" title=\\"Real Examples: Rapid Feature Development\\">\\r\\n\\r\\nComponent isolation enabled us to quickly ship two major features:\\r\\n\\r\\n**SSR Request Detection:** Found that sites like `baseballamerica.myshopify.com` were causing infinite loops because we only captured AJAX calls, missing server-side rendered requests. Collected examples, implemented the fix, then used component evaluation to iterate on edge cases \u2014 all without running full pipelines.\\r\\n\\r\\n**Dynamic Filter Parameters:** Previously only supported pagination params. Users couldn\'t override sort/filter parameters. Collected request objects from previous runs, crafted an LLM prompt for unified parameter detection, then iterated on the prompt using component evaluation until all tests passed.\\r\\n\\r\\nWithout component testing, each iteration would have required full pipeline runs \u2014 turning hours of testing into minutes.\\r\\n\\r\\n</TechnicalCard>\\r\\n\\r\\n### 2. End-to-End Tests\\r\\n\\r\\nValidate the complete pipeline:\\r\\n\\r\\n```bash\\r\\necho \'[{\\r\\n  \\"job_id\\": \\"existing-job-uuid\\",\\r\\n  \\"expected_source\\": \\"https://api.example.com/reviews\\",\\r\\n  \\"expected_pagination_keys\\": [\\"cursor\\", \\"limit\\"],\\r\\n  \\"expected_entity_count\\": 243\\r\\n},\\r\\n{\\r\\n  \\"site_url\\": \\"https://example.com/category/abc\\",\\r\\n  \\"query\\": \\"Listed products with name and prices\\",\\r\\n  \\"expected_source\\": \\"https://api.example.com/products\\",\\r\\n  \\"expected_pagination_keys\\": [\\"limit\\", \\"offset\\"],\\r\\n  \\"expected_entity_count\\": 100\\r\\n}] | uv run stroteval\\r\\n```\\r\\n\\r\\n**Result:** Catch system-level regressions before deployment.\\r\\n\\r\\nimport e2e_metrics_view from \\"./e2e-metrics-view.png\\";\\r\\nimport e2e_metrics_fail_success_comments_view from \\"./e2e-metrics-fail-success-comments-view.png\\";\\r\\nimport e2e_metrics_analysis_steps_view from \\"./e2e-metrics-analysis-steps-view.png\\";\\r\\n\\r\\n<ImageSlider \\r\\n  images={[\\r\\n    { \\r\\n      src: e2e_metrics_view, \\r\\n      caption: \\"End-to-end metrics\\" \\r\\n    },\\r\\n    { \\r\\n      src: e2e_metrics_fail_success_comments_view, \\r\\n      caption: \\"End-to-end metrics failure and success comments\\" \\r\\n    },\\r\\n    { \\r\\n      src: e2e_metrics_analysis_steps_view, \\r\\n      caption: \\"End-to-end metrics analysis steps\\" \\r\\n    }\\r\\n  ]}\\r\\n/>\\r\\n\\r\\n### 3. Production Feedback Loop\\r\\n\\r\\nWhen E2E failures occur:\\r\\n1. Full context is logged (screenshots, LLM traces) \\r\\n2. We identify the failed component in the dashboard\\r\\n3. We manually create a targeted component test\\r\\n\\r\\n**Result:** The test suite gets stronger with every bug we investigate.\\r\\n\\r\\n---\\r\\n\\r\\n<DeepDive title=\\"The Secret: Structured JSON Logs\\">\\r\\n\\r\\n**This is the key architectural decision** that makes both systems possible.\\r\\n\\r\\nBoth systems read the same structured JSONL logs:\\r\\n\\r\\n```json\\r\\n{\\"event_type\\": \\"analysis\\", \\"step_count\\": 1, \\"status\\": \\"success\\", \\"sub_events\\": [...]}\\r\\n{\\"event_type\\": \\"parameter-detection\\", \\"status\\": \\"success\\", \\"pagination_keys\\": [...]}\\r\\n{\\"event_type\\": \\"structured-extraction\\", \\"code\\": \\"...\\", \\"default_entity_count\\": 25}\\r\\n```\\r\\n\\r\\n**What goes into evaluation metrics:** The request-detection analysis steps that show exactly how the API endpoint was discovered.\\r\\n\\r\\n**Why this changed everything:**\\r\\n\\r\\n<NumberedList items={[\\r\\n  \\"One log format powers both real-time UI and evaluation metrics\\",\\r\\n  \\"Zero overhead \u2014 observability comes for free\\", \\r\\n  \\"Easy extensibility \u2014 new features just parse existing structure\\"\\r\\n]} />\\r\\n\\r\\n</DeepDive>\\r\\n\\r\\n---\\r\\n\\r\\n## Storing Results: Airtable for Metrics\\r\\n\\r\\nEvaluation results flow into structured tables:\\r\\n\\r\\n- **Component results** \u2014 Success rates per AI step\\r\\n- **E2E metrics** \u2014 Full pipeline performance\\r\\n- **Execution traces** \u2014 Screenshots and step details\\r\\n\\r\\n**Why Airtable?**\\r\\nVisual dashboards, team collaboration, automatic schema creation.\\r\\n\\r\\n---\\r\\n\\r\\n## Results\\r\\n\\r\\nSince implementing this dual approach:\\r\\n\\r\\n- **95% success rate** across 50+ website architectures\\r\\n- **10\xd7 faster debugging** through visual observability\\r\\n- **3\xd7 faster iteration** with component isolation\\r\\n- **Zero regressions** with systematic test coverage\\r\\n\\r\\n---\\r\\n\\r\\n## Key Takeaways\\r\\n\\r\\n<NumberedList items={[\\r\\n  \\"See everything \u2014 Visual debugging beats reading logs\\",\\r\\n  \\"Test in layers \u2014 Isolate failures before running full pipelines\\", \\r\\n  \\"Close the loop \u2014 Production failures guide new component tests\\",\\r\\n  \\"One log format \u2014 Structure once, observe everywhere\\",\\r\\n  \\"Make it simple \u2014 CLI tools everyone can use\\"\\r\\n]} />\\r\\n\\r\\n**The framework transforms AI development from \\"hope it works\\" to \\"know it works.\\"**\\r\\n\\r\\n---\\r\\n\\r\\n\ud83d\udd17 **Code**: [github.com/vertexcover-io/strot](https://github.com/vertexcover-io/strot)  \\r\\n\ud83d\udcc4 **Docs**: [Evaluation Guide](https://github.com/vertexcover-io/strot#-evaluation)"},{"id":"speed-up-docker-builds-on-github-actions","metadata":{"permalink":"/speed-up-docker-builds-on-github-actions","source":"@site/blog/2025-08-20-docker-build-times.mdx","title":"Speed up Docker Builds on Github actions","description":"Tricks to speed up docker builds","date":"2025-08-20T00:00:00.000Z","tags":[],"readingTime":5.12,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"speed-up-docker-builds-on-github-actions","title":"Speed up Docker Builds on Github actions","description":"Tricks to speed up docker builds","date":"2025-08-20T00:00:00.000Z","tags":[],"draft":false},"unlisted":false,"prevItem":{"title":"How We Built Observability and Evaluation for Strot","permalink":"/ai-observability-evaluation-framework"},"nextItem":{"title":"Video Rendering Pipeline for an AI-Driven Text-to-Video Platform","permalink":"/video-rendering-pipeline-for-an-ai-driven-text-to-video-platform"}},"content":"import TLDR from \'@site/src/components/TLDR\';\\nimport NumberedList from \'@site/src/components/NumberedList\';\\nimport FlexibleNumberedList from \'@site/src/components/FlexibleNumberedList\';\\n\\n<TLDR>\\n<NumberedList items={[\\n  \\"Turn on BuildKit & Buildx everywhere\\",\\n  \\"Reorder Dockerfile: copy package files first, then rest of code\\", \\n  \\"Use cache-mounts with buildkit-cache-dance action\\",\\n  \\"Pick the right cache backend (inline for speed, registry for large images)\\",\\n  \\"Add tmpfs + unsafe-io flags for package installs\\"\\n]} />\\n\\n| Scenario | Avg. wall-clock |\\n| --- | --- |\\n| No caching | **1 h 10 m** |\\n| Layer-cache hit | **6 m** |\\n| Layer-cache miss (deps change) | **52 m** |\\n| Cache-mount + Cache-Dance | **8 m** |\\n\\nStop rebuilding the world on every pull-request\u2014turn on these flags and ship faster.\\n</TLDR>\\n\\n{/* truncate */}\\n\\n\\n### Why this matters\\n\\n> 70 min \u279c 6 min (deps unchanged) or 8 min (deps changed).\\n> \\n> \\n> Those are the real numbers we saw after switching our Node + Python monorepo to the techniques below.\\n> \\n\\nSlow builds waste CI minutes, break focus, and block deploys.\\n\\n---\\n\\n## 1. Turn on BuildKit & Buildx everywhere\\n\\n```yaml\\n# .github/workflows/build.yml\\n- uses: docker/setup-buildx-action@v3   # spins up an isolated BuildKit builder\\n- run: echo \\"DOCKER_BUILDKIT=1\\" >> $GITHUB_ENV\\n\\n```\\n\\nBuildKit unlocks layer caching, cache-mounts, `RUN --mount`, and multi-platform bake ([BuildKit documentation](https://docs.docker.com/build/ci/github-actions/cache/)).\\n\\n---\\n\\n## 2. Trim the build context\\n\\nAdd a `.dockerignore` that excludes `node_modules/`, `docs/`, test data, and build artefacts. The runner uploads the entire context before **any** Docker layer executes; shrinking 500 MB of junk can save 30-90 s ([build optimization guide](https://docs.docker.com/build/cache/optimize/)).\\n\\n---\\n\\n## 3. Re-order your Dockerfile\\n\\n```Dockerfile\\n# syntax=docker/dockerfile:1.7\\nFROM node:20-slim AS deps\\nWORKDIR /app\\n\\n# 1\ufe0f\u20e3 copy only manifests\\nCOPY package.json yarn.lock ./\\nRUN --mount=type=cache,target=/root/.cache/yarn \\\\\\n    yarn install --frozen-lockfile    # re-runs only when the lock-file changes\\n\\n# 2\ufe0f\u20e3 now copy the rest\\nCOPY . .\\n\\n```\\n\\nBecause the dependency layer rarely changes, 60+ minutes of `yarn install` drops to < 6 minutes the next time a PR arrives.\\n\\n---\\n\\n## 4. Choose the *right* layer-cache backend\\n\\n| Exporter | What\u2019s stored | Cold restore on GH runner | Best when |\\n| --- | --- | --- | --- |\\n| **`type=inline`** | cache *metadata* embedded in the image | < 1 s (only tiny config pulled) | You already push the image anyway |\\n| **`type=registry`** | full layers in a `<image>-buildcache` tag | 5-30 s (downloads blobs) | Huge images, need `mode=max` granularity |\\n| **`type=gha`** | tarball in GitHub Actions Cache (10 GB limit) | 1-5 s (< 500 MB) | No private registry, branch-scoped caches |\\n\\n*Inline feels snappiest* because BuildKit needs only the image manifest; layer blobs are fetched lazily. Note, though, that inline supports only `mode=min`. For ARG/secret-heavy pipelines, flip to `registry` ([inline cache guide](https://docs.docker.com/build/cache/backends/inline/), [cache backends overview](https://docs.docker.com/build/cache/backends/), [GitHub Actions cache](https://docs.docker.com/build/cache/backends/gha/)).\\n\\n**Example call**\\n\\n```bash\\ndocker buildx build \\\\\\n  --push -t ghcr.io/acme/web:sha-$GITHUB_SHA \\\\\\n  --cache-from type=registry,ref=ghcr.io/acme/web:buildcache \\\\\\n  --cache-to   type=inline .\\n\\n```\\n\\n---\\n\\n## 5. Cache-mounts + the \u201cBuildKit Cache Dance\u201d\\n\\n```dockerfile\\nRUN --mount=type=cache,target=/var/cache/apt     \\\\\\n    --mount=type=cache,target=/root/.cache/pip   \\\\\\n    pip install -r requirements.txt\\n\\n```\\n\\n`type=cache` keeps bulky package folders **outside** the image graph, so later layer changes don\u2019t obliterate them. On GitHub-hosted runners those volumes disappear after each job\u2014unless you use **buildkit-cache-dance** to export/import them between runs:\\n\\n```yaml\\n- uses: reproducible-containers/buildkit-cache-dance@v2\\n```\\n\\nResult: *52 min \u279c 8 min* even when `package.json` *does* change ([buildkit-cache-dance repo](https://github.com/reproducible-containers/buildkit-cache-dance), [BuildKit cache issue discussion](https://github.com/moby/buildkit/issues/1673)).\\n\\n---\\n\\n## 6. tmpfs + \u201cunsafe-io\u201d = < 90 s package installs\\n\\n```dockerfile\\nRUN --mount=type=tmpfs,target=/var/lib/apt/lists  \\\\\\n    --mount=type=cache,target=/var/cache/apt      \\\\\\n    apt-get -o DPkg::Options::=\\"--force-unsafe-io\\" \\\\\\n        update && apt-get install -y git\\n\\n```\\n\\n<FlexibleNumberedList>\\n  <span><code>tmpfs</code> keeps apt\'s index in RAM\u2014zero disk writes.</span>\\n  <span><code>-force-unsafe-io</code> turns off every fsync in <code>dpkg</code>, a safe bet in throw-away CI VMs. Ubuntu\'s base images already apply a partial version, but passing the flag still yields 15-30% extra speed.</span>\\n</FlexibleNumberedList>\\n\\n([Dockerfile RUN reference](https://docs.docker.com/reference/dockerfile/), [APT speed optimization discussion](https://www.reddit.com/r/debian/comments/1dz4jxk/how_can_you_speed_up_apt/), [unsafe-io examples](https://gist.github.com/reegnz/990d0b01b5f5e8670f78257875d8daa8)).\\n\\n---\\n\\n## 7. Other micro-wins\\n\\n- **Pin base images by digest** to avoid surprise cache busts.\\n- **`buildx bake`** builds amd64+arm64 (or dev+prod variants) in parallel while sharing one cache ([buildx bake guide](https://docs.docker.com/guides/bake/)).\\n- **Garbage-collect** with `buildx prune --filter keep-storage=20GB` ([cache management guide](https://docs.docker.com/build/ci/github-actions/cache/)).\\n- **Self-hosted SSD runners** keep the entire BuildKit store between workflows\u2014zero network latency.\\n\\n---\\n\\n## References\\n\\n<ul style={{ paddingLeft: \'20px\', margin: \'16px 0\', lineHeight: \'1.6\' }}>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://docs.docker.com/build/cache/backends/inline/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Inline cache documentation</a> - Docker Docs\\n  </li>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://docs.docker.com/build/cache/backends/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Cache backends overview</a> - Docker Docs\\n  </li>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://docs.docker.com/build/cache/backends/gha/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">GitHub Actions cache backend</a> - Docker Docs\\n  </li>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://docs.docker.com/build/cache/optimize/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Build cache optimization guide</a> - Docker Docs\\n  </li>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://github.com/reproducible-containers/buildkit-cache-dance\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">BuildKit Cache Dance repository</a> - GitHub\\n  </li>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://docs.docker.com/reference/dockerfile/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Dockerfile RUN --mount reference</a> - Docker Docs\\n  </li>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://www.reddit.com/r/debian/comments/1dz4jxk/how_can_you_speed_up_apt/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">APT speed optimization discussion</a> - Reddit\\n  </li>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://gist.github.com/reegnz/990d0b01b5f5e8670f78257875d8daa8\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">DPKG unsafe-io examples</a> - GitHub Gist\\n  </li>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://docs.docker.com/guides/bake/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Buildx bake guide</a> - Docker Docs\\n  </li>\\n  <li style={{ marginBottom: \'8px\' }}>\\n    <a href=\\"https://docs.docker.com/build/ci/github-actions/cache/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Cache management on GitHub Actions</a> - Docker Docs\\n  </li>\\n</ul>\\n\\n---"},{"id":"video-rendering-pipeline-for-an-ai-driven-text-to-video-platform","metadata":{"permalink":"/video-rendering-pipeline-for-an-ai-driven-text-to-video-platform","source":"@site/blog/2025-08-06-video-rendering.mdx","title":"Video Rendering Pipeline for an AI-Driven Text-to-Video Platform","description":"Video Rendering Pipeline for an AI-Driven Text-to-Video Platform","date":"2025-08-06T00:00:00.000Z","tags":[{"inline":false,"label":"Infrastructure","permalink":"/tags/infra","description":"Infrastructure"}],"readingTime":4.15,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"video-rendering-pipeline-for-an-ai-driven-text-to-video-platform","title":"Video Rendering Pipeline for an AI-Driven Text-to-Video Platform","description":"Video Rendering Pipeline for an AI-Driven Text-to-Video Platform","date":"2025-08-06T00:00:00.000Z","tags":["infra"],"draft":false},"unlisted":false,"prevItem":{"title":"Speed up Docker Builds on Github actions","permalink":"/speed-up-docker-builds-on-github-actions"},"nextItem":{"title":"ML Infra design for the GPU Poor","permalink":"/ml-infra-for-the-gpu-poor"}},"content":"Replacing dual React-ffmpeg architecture with unified browser-based solution, achieving 100,000 daily video renders with consistent preview-output matching and halved development effort.\\n\\n{/* truncate  */}\\n\\n## Context\\n\\nA leading AI company offers a cloud-based video editor, its core product, enabling users to effortlessly convert text into video with human avatars. The editor supports a wide range of features, from backgrounds, media integration, and text overlays to animations, transitions, fonts, styling, and scene management. \\n\\nVideo generation operates as a pipeline: users begin by crafting a video using the browser-based tool, previewing their creation in real-time. Once satisfied, the video undergoes a render pipeline that culminates in the production of a full HD video, ensuring the final output mirrors the preview\'s fidelity. At its foundation, the frontend editor employs React, while the backend relies on a custom ffmpeg video renderer.\\n\\n## Problem\\n\\nThe company\'s video editor relied on a dual system: a React-based preview in the browser and an ffmpeg-driven backend for final video rendering. This architecture presented multiple challenges:\\n\\n- **Complexity of ffmpeg**: Using ffmpeg programmatically proved intricate due to its inherent complexities.\\n\\n- **Mismatch Between Preview and Final Render**: The separation of technologies (React for preview and ffmpeg for rendering) occasionally led to inconsistencies between the video preview and the final output.\\n\\n- **Duplication of Effort**: Introducing any new feature demanded double the effort: once for the preview in React and once more for the final render using ffmpeg. Moreover, ensuring consistency between these two stages added to the workload.\\n\\n- **Limitations with ffmpeg**: The use of ffmpeg imposed constraints on integrating advanced video editing features.\\n\\nFurthermore, the ideal solution needed to meet specific criteria:\\n\\n- Video generation time should align with the video\'s actual duration.\\n- The cost of rendering should not exceed 1Rs per minute of video.\\n- The solution should be feasible for relatively swift development and integration.\\n\\n## Challenges and Solution\\n\\n### Evaluating Browser-Based Solutions\\nDrawing from my prior experience with video rendering pipelines, it was evident that a browser-centric solution could address our functional requirements. However, constraints related to cost and development time necessitated further exploration.\\n\\n### Library Exploration for Time-Efficiency\\nAn open-source library promising the conversion of React components to video emerged as a viable solution for rapid deployment. Preliminary experiments ensured its compatibility with our feature set. An in-depth examination of the library\'s source code was undertaken to gauge its maintainability, ease of potential forking, and readiness for production scenarios. Direct discussions with the library\'s author and the wider community bolstered our confidence in its capabilities.\\n\\n### Benchmarking Library Performance on Lambda\\nThe library was designed to operate on AWS Lambda. A series of benchmarks provided insights into its performance metrics, both in terms of rendering time and cost implications.\\n\\n### Cost and Timing Optimization with Prototype Benchmarking\\nWhile the library met our immediate requirements, it presented cost-related challenges for the future, inherently rendering at a rate of 1fps. Recognizing this, I built a simplified clone of the open-source library to conduct focused benchmarking. The tests indicated that by leveraging the library\'s approach, we could achieve up to 4 FPS. The primary limitation was identified as the screenshot capture time, which took approximately 200ms per frame. This understanding offered clarity on the potential to optimize and align with the company\'s future cost and performance goals.\\n\\n### Addressing the VP8 Format Bottleneck\\nThe human avatar component of our rendering pipeline employed the VP8 video format for transparency support. While the chosen open-source platform accommodated VP8, it did so with suboptimal speeds. After experimenting with ffmpeg\'s WebAssembly version to disintegrate the video into frame sequences, we decided to restructure the text-to-video stage. Instead of producing videos, it was reconfigured to generate images directly, yielding twin advantages in performance and cost.\\n\\n### Custom ffmpeg Integration\\nOur use case demanded specific ffmpeg flags and a customized ffmpeg version \u2013 elements not supported natively by the library. To bridge this gap, I employed pnpm patching and automated the building of a custom ffmpeg version from its source.\\n\\n### Code Reusability for Consistency\\nA design approach was adopted where the developed code functioned as a reusable library. This allowed its integration into both the frontend for previews and the backend for final video rendering, ensuring uniformity across stages.\\n\\n### Animation and Transition Integration\\nThe final development phase involved the addition of animation and scene transition capabilities to the platform.\\n\\n## Impact\\n\\n- **Stable Production Implementation**: The solution has been integrated into production and operates with minimal disruptions. It is written in 100% typescript with ~90% test coverage.\\n\\n- **Consistent Previews and Renders**: The disparity between previews and final video renders, previously a pain point, has been eradicated, ensuring consistent user expectations.\\n\\n- **Swift Feature Integration**: The new system facilitated the rapid introduction of enhancements like custom fonts, animations, and scene transitions, effectively halving the time and effort previously required for such additions.\\n\\n- **Seamless Scaling**: With the implemented improvements, the platform now comfortably handles the rendering of up to 100,000 videos daily without necessitating additional adjustments or overhead."},{"id":"ml-infra-for-the-gpu-poor","metadata":{"permalink":"/ml-infra-for-the-gpu-poor","source":"@site/blog/2025-07-31-gpu-poor.mdx","title":"ML Infra design for the GPU Poor","description":"ML Infra design for the GPU Poor","date":"2025-07-31T00:00:00.000Z","tags":[{"inline":false,"label":"Infrastructure","permalink":"/tags/infra","description":"Infrastructure"}],"readingTime":4.22,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"ml-infra-for-the-gpu-poor","title":"ML Infra design for the GPU Poor","description":"ML Infra design for the GPU Poor","date":"2025-07-31T00:00:00.000Z","tags":["infra"],"draft":false},"unlisted":false,"prevItem":{"title":"Video Rendering Pipeline for an AI-Driven Text-to-Video Platform","permalink":"/video-rendering-pipeline-for-an-ai-driven-text-to-video-platform"},"nextItem":{"title":"Strot - The API Scraper","permalink":"/strot-is-a-api-scraper"}},"content":"import TLDR from \'@site/src/components/TLDR\';\\nimport QueueingTheoryGraph from \'@site/src/components/QueueingTheoryGraph\';\\nimport QueueWaitChart from \'@site/src/components/QueueWaitChart\';\\nimport MultiQueueSystem from \'@site/src/components/MultiQueueSystem\';\\nimport SmartScalingSimulator from \'@site/src/components/SmartScalingSimulator\';\\n\\n## Taming the Beast: How to Design a Queueing System for GPU-Intensive Workloads\\n\\n<TLDR>\\nWhen designing for scale, the limiting factor is the GPU availability. So all rate limits / queueing must be designed around GPU availability.\\n</TLDR> \\n\\n{/* truncate */}\\n\\n**A guide for the aspiring software engineer on managing demand when your most critical resource is scarce.**\\n\\nImagine you\'re building a revolutionary video generation service. Your platform is a hit, but you have a problem\u2014a good problem, but a problem nonetheless. You have three types of customers, all knocking on your door at once:\\n\\n*   **B2C Customers:** Individuals using your web app to create short, fun videos. They expect results in seconds.\\n*   **B2B Giants:** Large enterprise clients who want to process massive workloads of tens of thousands of videos via your API.\\n\\nHere\u2019s the catch: the heart of your operation, the GPU, is a fixed and expensive resource. Unlike CPU and memory, which are elastic and can be scaled on demand, your GPU capacity is limited. One B2B giant\'s request could monopolize your entire system, leaving your B2C users staring at a loading spinner for hours.\\n\\nHow do you design a system that ingests all these requests, keeps every customer type happy, and doesn\'t crumble under the pressure of this GPU bottleneck? The answer lies in the mathematical study of waiting lines: **Queueing Theory**.\\n\\n### The Counter-Intuitive Truth About Being Busy\\n\\nBefore we design our system, we must understand a fundamental insight from queueing theory: **wait times skyrocket as utilization exceeds 80%**.\\n\\nIn simple terms, queueing theory uses a few key variables:\\n*   **Arrival Rate (\u03bb - Lambda):** How quickly tasks enter the queue.\\n*   **Service Rate (\u03bc - Mu):** How quickly a server can process tasks.\\n*   **Utilization (\u03c1 - Rho):** How busy a server is (\u03bb/\u03bc).\\n\\n<QueueWaitChart />\\n\\n### The Architect\'s Solution: From One Big Line to a Multi-Lane Highway\\n\\n#### 1. The Foundational Shift: The \\"Video-Minute\\" as the True Unit of Work\\n\\nThe system was re-architected around the concept of a \\"video-minute.\\" Instead of measuring queue length by the number of jobs, it was measured by the total duration of all videos waiting to be processed. This immediately provided a far more accurate picture of the outstanding load on the GPU fleet.\\n\\n#### 2. Architecture: The Two-Lane Highway\\nThe core idea of separating UI and batch users was validated and implemented.\\nAction: Two completely parallel infrastructures were created, each with its own dedicated queue and GPU fleet. This prevented any possibility of a \\"noisy neighbor\\" from the batch API world affecting the premium UI user experience. The UI queue was fed by a small, reserved pool of GPUs, while the batch queue was serviced by an auto-scaling fleet.\\n\\n\\n#### 3. Smart Scaling: Beyond Simple Queue Length\\n\\n<SmartScalingSimulator />\\n\\nYes, we should auto-scale the GPU, but how much? There are few places we can look for data to make informed choice.\\n\\n- Historical Learning: The scaling logic was made parametric to learn from historical provisioning times, helping it make smarter predictions about when to initiate scaling.\\n- Metric-Driven Scaling: The auto-scaler was triggered based on \\"video-minutes\\".\\n- Cold Start problem: There is 2-4 minute cold-start time for GPUs. So, provisioning a GPU only makes sense if the `video-minutes` to be processed are `X` times of cold start time. That way, frequent GPU provisioning does not lead inefficient compute. It\'s economically inefficient to provision a new GPU that takes 3 minutes to start, only to process a 2-minute video.\\n\\n#### 4. Predictability Through Pragmatism: Working backwards from limits\\n\\nThe team acknowledged a hard truth: their ability to autoscale was not infinite. Based on historical data and cloud provider limits, they identified a realistic maximum number of GPUs they could reliably provision.\\nThe fixed maximum capacity (Max GPUs * video-minutes per GPU) became the system\'s total throughput budget. This budget was then divided among the maximum number of potential concurrent batch users.\\nResult: This calculation yielded a crucial metric: the maximum video-minutes per minute that could be allocated to a single user. This became the foundation for providing realistic SLAs.\\n\\n#### 5. The SLAs: Per-Tenant Throttling and Dynamic ETAs\\n\\nThis per-user capacity allocation was not just a theoretical number; it was enforced at the ingestion layer.\\nAction: A user-aware rate limiter was placed at the front of the batch queue. If a user was allocated a capacity of \\"1 video-minute per minute\\", this gatekeeper would only release that amount of work from that user\'s batch into the main queue each minute.\\nAccurate SLAs: This system allowed for incredibly predictable SLAs. If a user with a 1 video-minute/minute capacity submitted a batch of 100 jobs totaling 200 video-minutes, the system could confidently return an ETA of ~200 minutes plus a safety buffer, because it knew exactly how fast that user\'s jobs would be fed to the processors."},{"id":"strot-is-a-api-scraper","metadata":{"permalink":"/strot-is-a-api-scraper","source":"@site/blog/2025-07-28-strot-review-parser.mdx","title":"Strot - The API Scraper","description":"Strot (Sanskrit meaning source) is an AI agent which scrapes web api:","date":"2025-07-28T00:00:00.000Z","tags":[],"readingTime":8.12,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"strot-is-a-api-scraper","title":"Strot - The API Scraper","date":"2025-07-28T00:00:00.000Z","draft":false},"unlisted":false,"prevItem":{"title":"ML Infra design for the GPU Poor","permalink":"/ml-infra-for-the-gpu-poor"},"nextItem":{"title":"Teaching Claude Code to Work Independently","permalink":"/claude-code-context-engineering-v2"}},"content":"import ComparisonTable from \'../src/components/ComparisonTable\';\\nimport NumberedList from \'../src/components/NumberedList\';\\nimport TLDR from \'../src/components/TLDR\';\\nimport HierarchySection from \'../src/components/HierarchySection\';\\nimport SubSection from \'../src/components/SubSection\';\\nimport DeepDive from \'../src/components/DeepDive\';\\nimport TechnicalCard from \'../src/components/TechnicalCard\';\\nimport YouTubeEmbed from \'../src/components/YouTubeEmbed\';\\n\\n\\n<TLDR>\\nStrot (Sanskrit meaning source) is an AI agent which scrapes web api:\\n\\n<NumberedList items={[\\n  \\"Instead of scraping the dom, identifies the right api call.\\",\\n  \\"Fast, reliable, complete data scraping for listing data is possible via API scraping. \\",\\n  \\"Strot figures the api call so you don\'t have to.\\"\\n]} />\\n</TLDR>\\n\\n[Try out Strot!](https://github.com/vertexcover-io/strot)\\n\\n{/* truncate */}\\n\\n\\n<YouTubeEmbed videoId=\\"OCL3rWG9mDs\\" title=\\"Strot - The API Scraper Demo\\" />\\n\\n## What if ... ?\\n\\nWhen working with clients, we figured that they frequently need to scrape listing data like reviews and product listings. The DOM Scraping solutions don\'t give complete data.\\n\\nWhen is comes to scraping reviews, things CAN BE slightly different than running playwright scripts. That\'s the idea we started with! \\n\\nIt stems from the insights on browsing shopify. Since, shopify has plugin ecosystem, a lot of reviews come from plugins. They give the reviews in html / json form via api that the page renders. \\nSince, reviews - if they are worthwhile - would be in 100s for a given product, it would be very unlikely that someone with good web-dev practices will send all of them on one shot. There MUST be a second call (pagination) that requests for further reviews for the given product. \\n\\nWhat if ... we could scrape listing data via intercepting AJAX calls that are made from the browser itself? \\n\\n\\n## The Genesis \\n\\nHence, an experiment is born - called `strot`. In sanskrit it means source. We are trying to get to the ajax calls that gets us listing data. \\n\\nThe review scraping problem sits at a unique position. There are always MANY reviews which either causes cost ballooning no matter which existing approach you take.\\n\\nStrot is the only project which can scrape the data available as api call - at all - using natural language.\\n\\n\\n<ComparisonTable \\n  data={[\\n    {\\n      feature: \'Cost\',\\n      playwright: { text: \'Infrastructure costs for automation\', score: \'medium\' },\\n      llmScraper: { text: \'Pay per page + LLM inference\', score: \'poor\' },\\n      strot: { text: \'One-time API discovery\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Maintenance\',\\n      playwright: { text: \'High - UI changes break scripts\', score: \'poor\' },\\n      llmScraper: { text: \'Medium - prompt engineering needed\', score: \'medium\' },\\n      strot: { text: \'Low - APIs rarely change\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Intelligence Required\',\\n      playwright: { text: \'High - manual scraper\', score: \'poor\' },\\n      llmScraper: { text: \'High - per page analysis\', score: \'poor\' },\\n      strot: { text: \'One-time - for API discovery\', score: \'medium\' }\\n    },\\n    {\\n      feature: \'Scalability\',\\n      playwright: { text: \'Poor - loads full pages\', score: \'poor\' },\\n      llmScraper: { text: \'Expensive - each page costs $\', score: \'poor\' },\\n      strot: { text: \'Excellent - pagination via API\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Setup Complexity\',\\n      playwright: { text: \'Medium - browser automation\', score: \'medium\' },\\n      llmScraper: { text: \'Low - plug and play\', score: \'good\' },\\n      strot: { text: \'Low - plug and play\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Uniqueness\',\\n      playwright: { text: \'Common approach\', score: \'poor\' },\\n      llmScraper: { text: \'Common approach\', score: \'poor\' },\\n      strot: { text: \'Unique to us\', score: \'good\' }\\n    }\\n  ]}\\n  columns={[\'playwright\', \'llmScraper\', \'strot\']}\\n  columnHeaders={[\'Playwright\', \'LLM Scraper\', \'Strot (Ours)\']}\\n/>\\n\\nSo, IF we could find the ajax call \u2192 we only have to spend time figuring out the API call. and voila! You can simply call API - this is cheapest, fastest, most reliable approach of all. \\n\\nAnd the unique to us. Hence, we took a bite. \\n\\nBut the challenge is which api call is most relevnt for scraping reviews ? \\nwe solve this by capturing screenshot, visually analysing if review is preseng in the screenshot. Then we find a matching ajax call that has a similar content. Voila! \\n\\n\\n## The challenges of current approaches \\n\\nEach traditional approach faces fundamental limitations that compound at scale:\\n\\n<TechnicalCard type=\\"edge-case\\" title=\\"The Listing Data has Volume Problem\\">\\n\\nMost products with worthwhile reviews have 100-10,000+ reviews. Traditional approaches cost:\\n\\n- **Playwright**: 100 reviews \xd7 3 pages \xd7 10s load time = 30+ minutes\\n- **LLM Scraper**: 100 reviews \xd7 $0.10 per page = $10+ per product\\n- **Manual API**: 100 reviews \xd7 15min engineer time = $200+ per product\\n\\nStrot\'s approach: 100 reviews \xd7 0.1s API call = 10 seconds total.\\n\\n</TechnicalCard>\\n\\n\\n# Technical Details \\n\\n\\n<DeepDive title=\\"System Architecture & Technical Approach\\">\\n\\nStrot\'s architecture consists of three main components working in sequence:\\n\\n**1. HAR File Analysis Pipeline**\\n- Captures network traffic using browser automation (Playwright/Puppeteer)\\n- Filters AJAX/XHR requests that match review-related patterns\\n- Builds a candidates list of potential review API endpoints\\n\\n**2. Visual-Content Correlation Engine**\\n- Takes screenshot of the page showing reviews\\n- Uses vision models (Claude-4-sonnet) to extract review text from screenshots\\n- Performs fuzzy matching between screenshot content and API response data\\n\\n**3. Pagination Strategy Detection**\\n- Analyzes request parameters to identify pagination patterns\\n- Common patterns: `page`, `offset`, `cursor`, `after`, `next_token`\\n- Reverse engineers pagination logic from multiple API calls\\n\\n</DeepDive>\\n\\n<TechnicalCard type=\\"architecture\\" title=\\"Why AJAX Interception vs DOM Scraping\\">\\n\\nTraditional DOM scraping faces several fundamental limitations:\\n\\n- **Rate Limiting**: Websites throttle based on page loads, not API calls\\n- **Bot Detection**: Full page loads trigger anti-bot measures (Cloudflare, etc.)\\n- **Resource Overhead**: Loading entire DOM + assets vs lightweight JSON responses\\n- **Maintenance Burden**: CSS selectors break with UI changes, APIs rarely change\\n\\nAJAX interception bypasses these by operating at the data layer, not presentation layer.\\n\\n</TechnicalCard> \\n\\n<TechnicalCard type=\\"implementation\\" title=\\"AJAX Call Matching Algorithm\\">\\n\\nOur matching algorithm works in stages:\\n\\n1. **Content Extraction**: Extract review text from screenshots using OCR + Vision LLM\\n2. **API Response Analysis**: Parse all captured AJAX responses for text content\\n3. **Fuzzy String Matching**: Use algorithms like Levenshtein distance, Jaccard similarity\\n4. **Confidence Scoring**: \\n   - Text overlap ratio (>90% = high confidence)\\n   - JSON structure analysis (nested objects, review-like fields)\\n   - Response size correlation with visible review count\\n\\n**Edge Case Handling**: \\n- Paginated responses (partial matches expected)\\n- Localized content (different languages)\\n- Dynamic timestamps/IDs (filter out volatile fields)\\n\\n</TechnicalCard>\\n\\n<TechnicalCard type=\\"performance\\" title=\\"Why Screenshot Analysis vs Pure Network Analysis\\">\\n\\nSince we have to do the hard part of finding the right AJAX call ONLY once, it means that we can use augmentation from vision understanding to make it more accurate. \\n\\nThis helps by:\\n- Confirms which API calls actually render user-visible content. \\n- Handles cases where multiple API calls contain review data\\n- Eliminates false positives from internal/admin API calls\\n\\n</TechnicalCard>\\n\\n\\n### The codegen \\n\\nWe internally represent all the relevant information as json. This helps us to generate http client in any language of choice. Calling this API endpoint in succession will give all the reviews.\\n\\n<DeepDive title=\\"Code Generation Pipeline & Implementation\\">\\n\\nOur codegen system transforms discovered API patterns into production-ready code:\\n\\n**1. API Pattern Extraction**\\n```json\\n{\\n  \\"endpoint\\": \\"https://example.com/api/reviews\\",\\n  \\"method\\": \\"POST\\",\\n  \\"headers\\": {\\n    \\"content-type\\": \\"application/json\\",\\n    \\"x-requested-with\\": \\"XMLHttpRequest\\"\\n  },\\n  \\"pagination\\": {\\n    \\"type\\": \\"offset\\",\\n    \\"param\\": \\"offset\\",\\n    \\"limit_param\\": \\"limit\\",\\n    \\"max_per_request\\": 20\\n  }\\n}\\n```\\n\\n**2. Language-Specific Generation**\\n- **Python**: Uses `requests`. Can use `rnet` if want to bypass TLS fingerprinting.\\n- **Node.js**: `axios` with async/await patterns\\n- **cURL**: Direct shell commands with proper escaping\\n\\n</DeepDive>\\n\\n<HierarchySection title=\\"Iteratively Improving via Error Analysis and Evals\\">\\n\\nEvals were central to us while building this. We had three tiered system for evals that served us well.\\n\\n<NumberedList items={[\\n  \\"Does the analysis identify the right ajax call?\\",\\n  \\"Is the pagination strategy detection done correctly?\\",\\n  \\"Is the http api (codegen) able to get all the reviews?\\"\\n]} />\\n\\n<DeepDive title=\\"Comprehensive Evaluation System Architecture\\">\\n\\nOur evaluation system operates at three levels with automated scoring:\\n\\n**Level 1: AJAX Call Detection**\\n- **Ground Truth**: Manual verification of correct API endpoints (part of **golden dataset** for evals)\\n- **Automated Checks**:\\n  - Response contains review-like JSON structure (text, rating, author)\\n  - Content overlap with screenshot text >85%\\n\\n**Level 2: Pagination Strategy**\\n- **Test Suite**: Tested pagination patterns (cursor, limit, offset) across major e-commerce sites\\n- **Validation Logic**:\\n  - Do we hit natural boundaries (page N returns empty/error)?\\n\\n**Level 3: End-to-End Codegen**\\n- **Integration Tests**: Generated code fetches reviews from 10 test products\\n- **Quality Metrics**:\\n  - Duplicate detection (same review across pages)\\n  - Data completeness (rating, text, date, author)\\n\\n</DeepDive>\\n\\n<SubSection title=\\"Error Analysis\\">\\n\\nThis was the most consuming element for us until we vibe-coded initial version of this dashboard to see the logs as data.\\n\\n<TechnicalCard type=\\"implementation\\" title=\\"Observability Dashboard\\">\\n\\nOur observability dashboard processes shows data from:\\n\\n\\n<NumberedList items={[\\n  \\"Application logs (structured JSON)\\",\\n  \\"LLM API call traces (OpenAI/Anthropic)\\",\\n  \\"Playwright screenshots\\"\\n]} />\\n\\n\\nThis helps to do the error analysis and improve the system iteratively.\\n</TechnicalCard>\\n\\nHere is a quick peek over the dashboard we made to do the error analysis:\\n\\nThis shows us the cost and token usage:\\n<a href=\\"/img/strot-dashboard-analysis.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-analysis.png\\" alt=\\"Strot dashboard cost and token analysis\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\nThis shows us the step by step observability into what is happening:\\n<a href=\\"/img/strot-dashboard-step.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-step.png\\" alt=\\"Strot dashboard step by step analysis\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\n\\n</SubSection>\\n\\n<SubSection title=\\"Pagination\\">\\n\\n<a href=\\"/img/strot-dashboard-pagination.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-pagination.png\\" alt=\\"Strot dashboard pagination\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\n</SubSection>\\n \\n<SubSection title=\\"Codegen\\">\\n\\n<a href=\\"/img/strot-dashboard-codegen.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-codegen.png\\" alt=\\"Strot dashboard codegen\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\n</SubSection>\\n\\n</HierarchySection>\\n\\n\\nWe are also learning. This was our attempt at scraping the webpage and using the visual understanding of image models to get to the reviews faster. if you feel this can be improved, feel free to share those with us on [github-issues](https://github.com/vertexcover-io/strot/issues).\\n\\n\\n## Roadmap \\n\\n<NumberedList items={[\\n  \\"We are already on a path to make this a generic scraper. Currently, our evals sets show the tracking progress on various websites. It will be released soon.\\",\\n  \\"if you would like us to host this as a service for you, we are more than happy to. Come chat with us at [contact email]\\"\\n]} />"},{"id":"claude-code-context-engineering-v2","metadata":{"permalink":"/claude-code-context-engineering-v2","source":"@site/blog/2025-07-26-claude-code.mdx","title":"Teaching Claude Code to Work Independently","description":"Problem: You\'re trapped micromanaging Claude Code instead of building","date":"2025-07-21T00:00:00.000Z","tags":[{"inline":false,"label":"Context Engineering","permalink":"/tags/context-engineering","description":"Context Engineering"}],"readingTime":6.94,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"claude-code-context-engineering-v2","title":"Teaching Claude Code to Work Independently","date":"2025-07-21T00:00:00.000Z","authors":["abeeshake"],"tags":["context-engineering"],"draft":false},"unlisted":false,"prevItem":{"title":"Strot - The API Scraper","permalink":"/strot-is-a-api-scraper"},"nextItem":{"title":"AI for End-to-End Tests (Mobile too!) with Auto Healing","permalink":"/ai-agent-end-to-end-automated-tests-mobile"}},"content":"import TLDR from \'@site/src/components/TLDR\';\\nimport TipsSection from \'@site/src/components/TipsSection\';\\nimport MinimalDetails from \'@site/src/components/MinimalDetails\';\\nimport ScriptAttribution from \'@site/src/components/ScriptAttribution\';\\nimport CalendlyButton from \'@site/src/components/CalendlyButton\';\\n\\n\\n\\n<TLDR>\\n\\n**Problem:** You\'re trapped micromanaging Claude Code instead of building\\n\\n**Solution:** Train Claude Code once, then let it work autonomously  \\n\\n**How:** Strategic context engineering: CLAUDE.md files, systematic workflows, and learning capture\\n\\n**Result:** Claude Code becomes your autonomous teammate\\n\\n</TLDR>\\n\\nHere is [CLAUDE.md](https://gist.github.com/tripathi456/bfaf9add4b70bff131cd574c2f93cfac)\\n\\n\\n{/* truncate */}\\n\\n## Day 0: Just moved from cursor to claude code\\n\\n*It\'s 11 PM on a Tuesday. My token budget just hit zero. Claude Code is asking me the same question for the fourth time: \\"What coding style does this project use?\\" I\'ve spent 6 hours being a glorified copy-paste machine, explaining the same context over and over.*\\n\\n*Sound familiar?*\\n\\nThat night, I realized something critical: **Claude Code is teachable**. But I was the worst teacher on Earth.\\n\\n## Day 1: Fighting Claude Code (The Problem)\\n\\nEvery conversation was Groundhog Day:\\n\\n- \\"What\'s our testing framework again?\\"\\n- \\"How do we name files in this project?\\"  \\n- \\"What\'s the deployment process?\\"\\n\\nI was Claude Code\'s personal Wikipedia. **This had to change.**\\n\\n## Day 2: The Teaching Template (The Mentor)\\n\\n> What if Claude Code could remember your project like a team member who\'s been there for years?\\n\\nInstead of explaining everything every time, I wrote a template. The gist is:\\n\\n```md\\n# Communication: Be concise, reference past learnings from docs/work/\\n# File Naming: YYYY-MM-DD-[001]-[category]-[summary].md  \\n# Never code without checking docs/work/ for similar past solutions\\n```\\n\\nThat\'s it. **This eliminated hours of repetitive context.**\\n\\n<MinimalDetails summary=\\"See the prompt \u2192\\">\\n\\n```md\\n# Project: [Your Project Name]\\n\\n## Tech Stack & Tooling\\n- **Language**: Python 3.11+\\n- **Package Manager**: `uv` (use `uv add <dependency>`, `uv run <script>`)\\n- **Testing**: pytest with coverage\\n- **Linting**: ruff + mypy\\n\\n## Systematic File Naming\\nFormat: `YYYY-MM-DD-[001-999]-[category]-[four-word-summary].md`\\nFolder: `docs/work/`\\nCategories: `bug` | `feature` | `task` | `research` | `learnings`\\n\\nExamples:\\n- `2025-07-18-001-feature-user-authentication-system.md`\\n- `2025-07-18-002-bug-database-connection-timeout.md`\\n\\n## Communication Style\\n- **Concise**: No fluff, direct responses\\n- **Evidence-based**: Show, don\'t just tell\\n- **Contextual**: Reference past learnings from `docs/work/`\\n\\n## Planning Protocol\\n1. **Context Gathering**: Check `docs/work/` for relevant past decisions\\n2. **Assumption Documentation**: Explicit assumptions in plan files\\n3. **Execution Gate**: Only proceed after planning is complete\\n```\\n\\n</MinimalDetails>\\n\\n**The transformation was instant.** Claude Code started referencing past decisions, avoiding repeated mistakes, and building on previous work. **It finally felt like working with a teammate.**\\n\\n## Day 3: The Learning Moment - 30% context - (The Training)\\n\\n> What if Claude Code could learn from every mistake and never repeat it?\\n\\nWhen your context hits 30%, you have one chance to crystallize everything learned. Miss it, and you go back to day 1.\\n\\n\\n\\n<MinimalDetails summary=\\"See the prompt \u2192\\">\\n\\n```md\\n\\nWhen context drops below 30%: \\n\\n1. Document every decision made\\n2. List what failed (with code snippets)  \\n3. Note what worked brilliantly\\n4. Write handoff notes for next session\\n\\nUse the `Systematic File Naming` given above.\\n```\\n\\n</MinimalDetails>\\n\\n\\n## Day 4: The Autonomous Engineer (The Victory)\\n\\nI saw this tweet - [@svs used Claude Code as an MCP client to write an MCP server](https://x.com/_svs_/status/1928753160337637726). \\n\\nSomething magical discovered: **When put in verifiable workflows, Claude Code started working much better!**:\\n\\n- Write code \u2192 Run tests \u2192 Fix failures \u2192 Repeat\\n- Build feature \u2192 Deploy to staging \u2192 Check logs \u2192 Iterate  \\n- Analyze data \u2192 Generate insights \u2192 Verify against sources \u2192 Summarize\\n\\n**I wasn\'t micromanaging anymore. I was collaborating.**\\n\\n## Stories from other folks\\n\\nMet awesome folks at [Fifth Elephant Conference](https://hasgeek.com/fifthelephant/2025/) where we shared claude code learnings.\\n\\n<TipsSection contributor=\\"Rajesh\\" contributorUrl=\\"https://www.linkedin.com/in/codingnirvana/\\">\\n\\n### The Token Budget Hack That Doubled My Productivity\\n\\nRajesh was burning through tokens like crazy at his startup. Then he discovered something weird about Claude\'s [5-hour windows](https://support.anthropic.com/en/articles/11145838-using-claude-code-with-your-pro-or-max-plan).\\n\\n**His discovery:** Start sessions at 7 AM instead of 9 AM.\\n\\nWhy? The overlapping windows create a \\"double token zone\\" during peak hours:\\n\\n**Before:** 9am-2pm, 2pm-7pm (standard)  \\n**After:** 7am-12pm, 12pm-5pm, 5pm-10pm (overlapping)\\n\\n**Result:** Between 9am-5pm = **double tokens available**\\n\\n\\n\\n<img src=\\"/img/timeline.svg\\" />\\n\\n\\n\\n### The CSV Strategy That Saved Hours\\n\\nRajesh\'s team was dealing with lots of data analysis requests. CSV files everywhere. Claude kept hitting context limits trying to process raw data.\\n\\n**The breakthrough:** Stop feeding Claude data. Feed it scripts.\\n\\n**Old way:** \\"Here\'s a 10MB CSV, analyze it\\"  \\n**New way:** \\"Write a script to analyze this CSV type, then run it\\"\\n\\n**Why it works:** Scripts are tiny. Results are focused. Claude guides itself using its own analysis output. \\n\\n</TipsSection>\\n\\n<TipsSection contributor=\\"Ashwant\\" contributorUrl=\\"https://www.shelfradar.ai/\\">\\n\\n### The Accidental Discovery That Changed Everything\\n\\nAshwant was debugging a frustrating session. In a moment of rage, he accidentally hit ESC four times.\\n\\n**What happened next blew his mind.**\\n\\nClaude Code showed him a prompt history he\'d never seen before. Every conversation. Every context. **Time travel for developers.**\\n\\n**The magic combo:** ESC + ESC + ESC + ESC = Prompt history navigation\\n\\n**Game changer:** You can resurrect any previous session state instantly.\\n\\n### Claude Code as Your Database Whisperer\\n\\nAt ShelfRadar, Ashwant deployed Claude Code as their internal SQL agent. \\n\\n**The setup:** Claude Code + database schema = autonomous query optimizer\\n\\n**The result:** \\n\\n- Ad-hoc queries refined automatically\\n- Schema changes don\'t break queries  \\n- Claude evolves with your database\\n\\n</TipsSection>\\n\\n**The question isn\'t whether Claude Code is teachable.**  \\n**The question is: Are you ready to become its teacher?**\\n\\nUpdate 1: Claude code introduced hooks. Here is a short collection of hooks that I find useful for my workflow.\\n\\n1. bash script to notify the end of claude code turns. \\n\\n\\n<MinimalDetails summary=\\"Full bash script \u2192\\">\\n```bash\\n#!/bin/bash\\n\\n# Read hook input data\\nINPUT=$(cat)\\nSESSION_DIR=$(basename \\"$(pwd)\\")\\n\\n# Extract message from transcript if available\\nTRANSCRIPT_PATH=$(echo \\"$INPUT\\" | jq -r \'.transcript_path\')\\nif [ -f \\"$TRANSCRIPT_PATH\\" ]; then\\n  MSG=$(tail -10 \\"$TRANSCRIPT_PATH\\" |\\n    jq -r \'select(.message.role == \\"assistant\\") | .message.content[0].text\' |\\n    tail -1 | tr \'\\\\n\' \' \' | cut -c1-60)\\n  MSG=${MSG:-\\"Task completed\\"}\\nelse\\n  MSG=\\"Task completed\\"\\nfi\\n\\n# Show Linux desktop notification (requires notify-send)\\nnotify-send \\"Claude Code ($SESSION_DIR) Done\\" \\"$MSG\\"\\n```\\n\\n</MinimalDetails>\\n\\n<ScriptAttribution\\n  title=\\"Useful Claude Code Hooks & Scripts\\"\\n  description=\\"\\"\\n  links={[\\n    {\\n      url: \\"https://gist.github.com/glennmatlin/fadc41edc3bb9ff68ff9cfa5d6b8aca7\\",\\n      title: \\"Making Claude use uv instead of pip\\",\\n      description: \\"Script for making Claude use uv package manager instead of pip\\"\\n    },\\n    {\\n      url: \\"https://conductor.build/\\",\\n      title: \\"Running multiple Claude Code sessions in parallel\\",\\n      description: \\"Using git worktrees for parallel Claude Code sessions\\"\\n    },\\n    {\\n      url: \\"https://www.reddit.com/r/ClaudeAI/comments/1loodjn/claude_code_now_supports_hooks/\\",\\n      title: \\"Claude Code Hooks Discussion\\",\\n      description: \\"Reddit discussion about Claude Code hooks support\\"\\n    }\\n  ]}\\n/>\\n\\n\\n---\\n\\n*This revolution moves fast. By the time you read this, someone\'s already teaching Claude Code to do things we haven\'t imagined yet.*\\n\\n**Further Reading:**\\n- [Context Engineering by Manus](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus) \\n- [Hrishi\'s Claude Code Analysis](https://southbridge-research.notion.site/claude-code-an-agentic-cleanroom-analysis)\\n- [Claude Code Sub-Agents Documentation](https://docs.anthropic.com/en/docs/claude-code/sub-agents)\\n- [Claude Code Hooks Guide](https://docs.anthropic.com/en/docs/claude-code/hooks-guide)\\n\\n---\\n\\n## Are Your Developers Working FOR Claude Code? Or Is Claude Code Working FOR Them?\\n\\nRight now, your engineers spend 30% of their AI time being Claude\'s personal assistants - explaining context, re-describing architecture, and hand-holding every request. **Meanwhile, Anthropic\'s teams reduced research time by 80% and debug 3x faster** because Claude Code works FOR them.\\n\\n**The Reality Check:** Your team bought the most powerful coding AI ever built, then turned themselves into its unpaid interns.\\n\\n**The Transformation:** Stop being Claude\'s employee. Make Claude Code your team\'s autonomous coding partner that knows your codebase better than your junior developers.\\n\\n**Flip the Script: Make Claude Code Work FOR Your Team**\\n\\nTransform your developers from AI babysitters into AI commanders:\\n- Setting up CLAUDE.md files that eliminate context re-explaining  \\n- Building verifiable workflows that turn debugging from hours to minutes\\n- Token optimization strategies that double your effective usage\\n- Advanced automation that makes Claude Code your team\'s autonomous teammate\\n\\n*Read how [Anthropic\'s teams achieve these results](https://www.anthropic.com/news/how-anthropic-teams-use-claude-code) and learn to implement the same strategies for your startup.*\\n\\n<CalendlyButton \\n  url=\\"https://calendly.com/abhishek-vertexcover/claude-code\\"\\n  text=\\"Book Your 30-Minute Consultation Call\\"\\n  description=\\"Transform from Claude Code user to Claude Code teacher. Your future autonomous development workflow starts with one conversation.\\"\\n/>"},{"id":"ai-agent-end-to-end-automated-tests-mobile","metadata":{"permalink":"/ai-agent-end-to-end-automated-tests-mobile","source":"@site/blog/2025-07-10-flowtest/index.mdx","title":"AI for End-to-End Tests (Mobile too!) with Auto Healing","description":"AI for End-to-End Tests (Mobile too!)","date":"2025-07-10T00:00:00.000Z","tags":[{"inline":false,"label":"AI Agents","permalink":"/tags/ai-agents","description":"AI Agents"},{"inline":false,"label":"End-to-End Test","permalink":"/tags/end-to-end-test","description":"End-to-End Test"}],"readingTime":3.67,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"ai-agent-end-to-end-automated-tests-mobile","title":"AI for End-to-End Tests (Mobile too!) with Auto Healing","description":"AI for End-to-End Tests (Mobile too!)","date":"2025-07-10T00:00:00.000Z","tags":["ai-agents","end-to-end-testing"]},"unlisted":false,"prevItem":{"title":"Teaching Claude Code to Work Independently","permalink":"/claude-code-context-engineering-v2"}},"content":"### AI Agent for End-to-End Testing to Deliver Flawless Digital Experiences\\n\\n---\\n\\nWhat if Ai Agent could write tests for your codebase? End-to-end? and for mobile too? and it auto heals / auto-adjusts when your codebase changes?\\n\\nWe share nuggets we learnt while building an AI Agent to solve one of the most persistent challenges in software development: making UI test automation accessible, reliable, and scalable across platforms and devices.\\n\\n{/* truncate */}\\n\\n\\n### The Problem\\n\\nTest automation has historically been:\\n\\n- **Too technical**: requiring code expertise\\n- **Time-consuming**: for authoring and maintaining scripts\\n- **Platform-limited**: with fragmented support for web vs. mobile\\n- **Fragile**: breaking with minor UI changes or incomplete user flows\\n\\nExisting tools were not built for the demands of today\'s fast-moving, multi-platform development cycles. They struggled particularly with hybrid apps, dynamic interfaces, and gesture-driven experiences.\\n\\n---\\n\\n### The Solution: AI Agent\\n\\nAI agent designed from the ground up as an intelligent, prompt-driven automation system with key capabilities:\\n\\n### Natural Language to Automation Code\\n\\nUsers describe test scenarios in plain English. It translates them into precise, executable test scripts\u2014including test data, validations, and edge cases.\\n\\n![flow-diagram](flow-diagram.png)\\n\\n### Web & Mobile App Testing\\n\\nSupports both web (Selenium, Playwright) and mobile (Appium) frameworks, making it one of the few solutions that bridges the gap between platforms seamlessly.\\n\\n### Multi-Language Support\\n\\nGenerates scripts in Java, Python, JavaScript, and other frameworks\u2014tailored to the team\'s existing tech stack.\\n\\n### Smart Debugging\\n\\nExecutes each script line in real-time as it\'s generated, identifying and correcting issues on the fly.\\n\\n### Cross-Device Execution\\n\\nRun tests instantly across 5,000+ combinations of real browsers and devices (when connected to cloud infrastructure).\\n\\n### Self-Healing Automation\\n\\nDetects and updates selectors and steps automatically as the application evolves, eliminating the need for manual maintenance.\\n\\n---\\n\\n### Core Technical Challenges Solved\\n\\n### UI Automation for Flutter Web and Hybrid Mobile Apps\\n\\nMost automation tools break down on platforms like Flutter Web, where the UI is rendered inside a `<canvas>` instead of standard HTML elements, and on hybrid apps without accessible DOM trees.\\n\\n**Our agent solved this by enabling interaction with non-standard UIs using a combination of visual, contextual, and heuristic techniques**\u2014delivering true end-to-end automation where no other solution worked.\\n\\n---\\n\\n### Accurate and Reusable Script Generation\\n\\nRunning LLMs for every test execution is expensive and error-prone.\\n\\nThis AI Agent implemented a **novel templating and generation system** that decouples script generation from execution. This allowed:\\n\\n- Complete and correct scripts on the first pass\\n- Reusability across test runs and frameworks\\n- Fast, low-cost, scalable test creation\\n\\n---\\n\\n### Complex Gestures and UI Behaviors\\n\\nSimulating gestures like pinch, zoom, drag-and-drop, or multi-touch is notoriously hard\u2014especially in custom components.\\n\\nThis agent provided **fine-grained control** over gesture simulation, going beyond the abstractions of typical libraries, enabling accurate testing of sliders, carousels, maps, and more.\\n\\n---\\n\\n### Mapping Natural Language to UI Actions Reliably\\n\\nNatural language like \u201cClick the Pay button\u201d can be ambiguous\u2014especially in large, dynamic UIs.\\n\\n**Being very smart, it combined multiple modalities\u2014DOM structure, visual layout, and semantic cues**\u2014to reliably identify elements even when conventional locators failed. This enabled it to handle vague prompts and incomplete context with high precision.\\n\\n---\\n\\n### Stability in Dynamic & Incomplete User Flows\\n\\nIn real-world apps, pop-ups appear unexpectedly, elements load asynchronously, and flows can be interrupted.\\n\\nIt was was designed to **recover intelligently** from such cases using retry logic, timeout strategies, and partial flow handling. This brought production-grade resilience to end-to-end test execution.\\n\\n---\\n\\n### Scalable Evaluation & Debugging Infrastructure\\n\\nA major limitation of LLM-based systems is the lack of robust evaluation.\\n\\nAI Agent addressed this by building a **custom evaluation framework** that:\\n\\n- Validated script correctness at each step\\n- Enabled partial re-execution of scripts\\n- Provided fine-grained feedback for model improvement\\n    \\nThis drastically accelerated iteration speed and allowed for deeper validation of system accuracy.\\n    \\n\\n---\\n\\n### Conclusion\\n\\nAI Agent redefines what\'s possible in test automation. By combining the reasoning power of LLMs with robust engineering for execution, gesture control, and UI resilience, it makes test automation accessible to non-engineers while retaining power for experts.\\n\\nFrom tackling the hardest UI platforms like Flutter Web to enabling precise, reusable test generation and execution at scale, our custom AI agent is a leap forward in the world of quality engineering.\\n\\nIt\'s not just a tool\u2014it\'s a full-stack AI agent that understands, adapts, and evolves with your application."}]}}')}}]);