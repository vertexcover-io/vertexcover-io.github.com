"use strict";(self.webpackChunkblog_vertexcover=self.webpackChunkblog_vertexcover||[]).push([[1859],{7797:e=>{e.exports=JSON.parse('{"permalink":"/learnings/2025-10-ranjan-week1","source":"@site/learnings/2025-10-ranjan-week1.mdx","title":"Weekly learnings: Week1","description":"- Container Checkpoint and Restore","date":"2025-10-10T00:00:00.000Z","tags":[],"readingTime":6.94,"hasTruncateMarker":true,"authors":[{"name":"Ranjan Ojha","title":"Software Engineer","url":"https://github.com/hungerarray","page":{"permalink":"/learnings/authors/ranjan"},"socials":{"github":"https://github.com/hungerarray","linkedin":"https://www.linkedin.com/in/ojha-ranjan/"},"imageURL":"https://github.com/hungerarray.png","key":"ranjan"}],"frontMatter":{"title":"Weekly learnings: Week1","date":"2025-10-10T00:00:00.000Z","type":"weekly","authors":["ranjan"],"tags":[],"draft":false},"unlisted":false,"nextItem":{"title":"AI Video Cutter - Week 1 Learnings","permalink":"/learnings/2025/01/20/ai-video-cutter-week1"}}')},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(6540);const s={},r=i.createContext(s);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:n},e.children)}},9285:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var i=t(7797),s=t(4848),r=t(8453);const a={title:"Weekly learnings: Week1",date:new Date("2025-10-10T00:00:00.000Z"),type:"weekly",authors:["ranjan"],tags:[],draft:!1},o="Container Checkpoint and Restore",l={authorsImageUrls:[void 0]},c=[{value:"Limitations",id:"limitations",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"#container-checkpoint-and-restore",children:"Container Checkpoint and Restore"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#limitations",children:"Limitations"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#cpu-stalls",children:"CPU Stalls"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsxs)(n.a,{href:"#making-syscalls-in-linux",children:["Making ",(0,s.jsx)(n.code,{children:"syscalls"})," in Linux"]})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#some-tools-used-to-deploy-model",children:"Some tools used to deploy model"})}),"\n"]}),"\n","\n",(0,s.jsx)(n.p,{children:"Container checkpoint and restore is a facility by which any running Linux container is saved to memory\nand later restored at a later time from the same point at which the checkpoint was created. This restore\nis not limited to a single device and can also be in a different remote computer."}),"\n",(0,s.jsxs)(n.p,{children:['This was a project started by some "mad russians", and later merged into Linux Kernel, and is called\n',(0,s.jsx)(n.a,{href:"https://criu.org/Main_Page",children:(0,s.jsx)(n.code,{children:"Checkpoint / Restore in Userspace (CRIU)"})}),". The sheer idea of saving a process that is currently\nrunning and restoring it later in time is why this process was called mad, much like how the cryogenic\nsleep for humans is considered a fiction. Infact, this feature comes with lots of limitations mentioned\nlater. However, this is not an unproven technology. Such a thing has been done with VMs in the past.\nDespite all the limitations and issues that might come up, it's still widely used for the simple fact that\nit expedites the startup process."]}),"\n",(0,s.jsx)(n.p,{children:"To create a checkpoint we have to create what is known as a container memory snapshot.\nA Container memory snapshot is when we basically take a copy of the entire state of a Linux Container.\nFor this we need to copy the entire container's filesystem, and process tree. Process tree itself contains\nall the memory mappings, file descriptor tables, registers, environment variables, process IDs, etc."}),"\n",(0,s.jsxs)(n.p,{children:["Before ",(0,s.jsx)(n.code,{children:"CRIU"})," was officially available, to create such a snapshot, it was necessary for users to maintain\ntheir own custom variant of the kernel with required features. However, with ",(0,s.jsx)(n.code,{children:"CRIU"})," available in the mainline\nLinux Kernel, most of the container runtimes now do offer this facility. Of particular mention is ",(0,s.jsx)(n.code,{children:"gVisor"}),". The\ncontainer runtime utility, ",(0,s.jsx)(n.code,{children:"runsc"})," which stands for ",(0,s.jsx)(n.code,{children:"run sandboxed container"}),", to it's counterpart ",(0,s.jsx)(n.code,{children:"runc"}),", has\nadded functionality for checkpoint and restore. Given that ",(0,s.jsx)(n.code,{children:"gVisor"})," has a usermode kernel functionality, it can\ninfact, offer more granularity and features for checkpoint and restore."]}),"\n",(0,s.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"While the restore can be made in different computer, it has to mimic the original system as much as possible. Otherwise the invariants that program expect to be maintained will be broken, and can lead to some nasty surprises down the line."}),"\n",(0,s.jsx)(n.li,{children:"The CPU that the restore is made on has to match the instruction set, where the snapshot was taken, otherwise there can be runtime issues with invalid opcode."}),"\n",(0,s.jsx)(n.li,{children:"A container that utilizes GPU, is sensitive to differences in NVIDIA driver versions and in addition to container runtime versions."}),"\n",(0,s.jsx)(n.li,{children:"Programs need to account for the fact that the machine IP might change in between restore."}),"\n",(0,s.jsxs)(n.li,{children:["A Problem documented in Modal documentation is that, there are some functions in ",(0,s.jsx)(n.code,{children:"torch.cuda"})," that after restoring from snapshot will initialize CUDA as having zero GPU devices. The only fix is to reinitialize ",(0,s.jsx)(n.code,{children:"torch.cuda"})," again."]}),"\n",(0,s.jsxs)(n.li,{children:["In particular, when loading a huge snapshot, there is a significant CPU pressure as hundreds of 4KiB pages are getting loaded into memory. Such a particular workload is particularly affected by ",(0,s.jsx)(n.a,{href:"#cpu-stalls",children:"CPU Stalls"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"cpu-stalls",children:"CPU Stalls"}),"\n",(0,s.jsx)(n.p,{children:"To explain a CPU stall, we first need to understand that any given process is either running on the CPU or it's not.\nThere can never be in between. So what happens when we have 3 processes each granted 0.3 of CPU running on the system."}),"\n",(0,s.jsx)(n.p,{children:"For the above process, lets make a few assumptions. First is our CPU currently only has a single core and a single thread. Now, it's not really possible for us to divide this thread into 0.3 sections and grant each of the process\nrunning a section. As previously stated, a process is either running or not running, and when it is running it\nutilizes 100% of the thread and when it's not running it utilizes 0% of the thread. So instead, CPU makes the\ndivision in time instead. For our example, lets consider that the CPU segments 100ms of CPU time window. So,\nwith our 3 processes, in this 100ms window, each process gets 30ms of execution time, with the last 10ms being\nwasted and non of our processes running. Again when a new 100ms window is created all 3 process are allowed to continue their run."}),"\n",(0,s.jsx)(n.p,{children:"Now, the issue is if you consider a compute intensive workload, it would really benefit off of that extra 10ms of\nCPU time, but currently the way Linux schedular works, and also how the default CPU limits on pods by\nkubernetes works, each of the process are allocated equal precedence. Hence, for performance critical processes,\nit makes sense to also tune your application to get more CPU time."}),"\n",(0,s.jsx)(n.p,{children:"However, in real world, we don't have a single CPU core, and most commercial CPU's nowadays offer capabilities for running 2 threads. Also of note, most of our applications when parallelized, will spawn additional threads along\nwith the main thread inside the running process. However, the CPU time granted to each is still at process level.\nThat means, if say our hypothetical application has 3 threads each running in the same CPU time window, now each of\nour thread will get 10ms of actual CPU time down from 30ms of time our single thread was getting. The problem is worsened if we have 10 threads and somehow all 10 threads are granted run in the same time, in different cores, then\neach thread only gets about 3ms of worktime."}),"\n",(0,s.jsx)(n.p,{children:"This situation of having potentially adequate CPU power, but still being unable to utilize 100% of the CPU for\ncompute is known as CPU stall."}),"\n",(0,s.jsx)(n.p,{children:"In a single thread situation, the best way to deal with this issue is to grant higher priority to the process to\nallow it to gain more CPU time. And in the case of multi threaded situation, infact it's actually much more\nbeneficial to pin the process to a certain CPU core. Infact, many games actually see quite a significant performance\nboost when their processes are pinned to a few cores rather than allowing them access to all the cores."}),"\n",(0,s.jsxs)(n.h1,{id:"making-syscalls-in-linux",children:["Making ",(0,s.jsx)(n.code,{children:"syscalls"})," in Linux"]}),"\n",(0,s.jsxs)(n.p,{children:["Most of the program we write, ultimately have to make a ",(0,s.jsx)(n.code,{children:"syscall"})," to do any operation on a system. Understandably, I\nhad a misconception that the ",(0,s.jsx)(n.code,{children:"syscalls"})," are themselves exposed over ",(0,s.jsx)(n.code,{children:"C-API"})," and that any language that wishes to make\na ",(0,s.jsx)(n.code,{children:"syscall"})," had to at minimum link to a lower level ",(0,s.jsx)(n.code,{children:"C library"})," like ",(0,s.jsx)(n.code,{children:"libc"})," that does the work for them. However,\na particular note was we can have ",(0,s.jsx)(n.code,{children:"golang"})," applications without ",(0,s.jsx)(n.code,{children:"C-go"}),". While at the time, I hadn't connected the\ndots, I was recently watching some video on getting the smallest kernel, and there I chanced upon the fact that to\nmake a ",(0,s.jsx)(n.code,{children:"syscall"})," you don't really need to bind to ",(0,s.jsx)(n.code,{children:"C library"}),". You just need to be able to emit specific assembly instructions."]}),"\n",(0,s.jsxs)(n.p,{children:["A ",(0,s.jsx)(n.code,{children:"syscall"})," is triggered by writing the particular ",(0,s.jsx)(n.code,{children:"syscall"})," number into a particular register. Then calling the\n",(0,s.jsx)(n.code,{children:"syscall"}),"  instruction in CPU. This internally triggers a trap request, which is captured by Kernel."]}),"\n",(0,s.jsxs)(n.p,{children:["Given that it's assembly code, the following description is for how to perform a ",(0,s.jsx)(n.code,{children:"syscall"})," in Linux, particular\nin x86_64 system."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["First place the system call number into the ",(0,s.jsx)(n.code,{children:"rax"})," register. ",(0,s.jsx)(n.code,{children:"write syscall"})," is for instance number 1, and ",(0,s.jsx)(n.code,{children:"read syscall"})," is number 0. For more ",(0,s.jsx)(n.code,{children:"syscall"})," numbers visit ",(0,s.jsx)(n.a,{href:"https://elixir.bootlin.com/linux/v6.13/source/arch/x86/entry/syscalls/syscall_64.tbl",children:"here"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Place the arguments for the system call into the designated registers,"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"rdi"})," (first argument)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"rsi"})," (second argument)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"rdx"})," (third argument)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"r10"})," (fourth argument)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"r8"})," (fifth argument)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"r9"})," (sixth argument)"]}),"\n",(0,s.jsx)(n.p,{children:"This order is specified by the calling convention."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Invoke the systemcall by executing the ",(0,s.jsx)(n.code,{children:"syscall"})," instruction."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The return value is placed inside ",(0,s.jsx)(n.code,{children:"rax"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"some-tools-used-to-deploy-model",children:"Some tools used to deploy model"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://www.ray.io/",children:"ray"})," Allows for distributed GPU computation"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/kubernetes/index.html",children:"kuberay"})," to deploy ray in kubernetes"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://www.kubeflow.org/",children:"Kubeflow"})}),"\n",(0,s.jsx)(n.p,{children:"A CNCF (Cloud Native Computing Formation) project, which is the foundation of tools for AI Platforms on Kubernetes. Of particular note is that recently in a conference, Ubucon, a speaker was telling that kubeflow is almost an industry standard for deploying AI in kubernetes."}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);