"use strict";(self.webpackChunkblog_vertexcover=self.webpackChunkblog_vertexcover||[]).push([[5997],{7360:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025-10-ranjan-week1","metadata":{"permalink":"/learnings/2025-10-ranjan-week1","source":"@site/learnings/2025-10-ranjan-week1.mdx","title":"Weekly learnings: Week1","description":"- Container Checkpoint and Restore","date":"2025-10-10T00:00:00.000Z","tags":[],"readingTime":6.94,"hasTruncateMarker":true,"authors":[{"name":"Ranjan Ojha","title":"Software Engineer","url":"https://github.com/hungerarray","page":{"permalink":"/learnings/authors/ranjan"},"socials":{"github":"https://github.com/hungerarray","linkedin":"https://www.linkedin.com/in/ojha-ranjan/"},"imageURL":"https://github.com/hungerarray.png","key":"ranjan"}],"frontMatter":{"title":"Weekly learnings: Week1","date":"2025-10-10T00:00:00.000Z","type":"weekly","authors":["ranjan"],"tags":[],"draft":false},"unlisted":false,"nextItem":{"title":"AI Video Cutter - Week 1 Learnings","permalink":"/learnings/2025/01/20/ai-video-cutter-week1"}},"content":"- [Container Checkpoint and Restore](#container-checkpoint-and-restore)\\n  - [Limitations](#limitations)\\n- [CPU Stalls](#cpu-stalls)\\n- [Making `syscalls` in Linux](#making-syscalls-in-linux)\\n- [Some tools used to deploy model](#some-tools-used-to-deploy-model)\\n\\n{/* truncate */}\\n\\n# Container Checkpoint and Restore\\n\\nContainer checkpoint and restore is a facility by which any running Linux container is saved to memory \\nand later restored at a later time from the same point at which the checkpoint was created. This restore\\nis not limited to a single device and can also be in a different remote computer.\\n\\nThis was a project started by some \\"mad russians\\", and later merged into Linux Kernel, and is called \\n[`Checkpoint / Restore in Userspace (CRIU)`](https://criu.org/Main_Page). The sheer idea of saving a process that is currently \\nrunning and restoring it later in time is why this process was called mad, much like how the cryogenic\\nsleep for humans is considered a fiction. Infact, this feature comes with lots of limitations mentioned \\nlater. However, this is not an unproven technology. Such a thing has been done with VMs in the past.\\nDespite all the limitations and issues that might come up, it\'s still widely used for the simple fact that\\nit expedites the startup process.\\n\\nTo create a checkpoint we have to create what is known as a container memory snapshot.\\nA Container memory snapshot is when we basically take a copy of the entire state of a Linux Container. \\nFor this we need to copy the entire container\'s filesystem, and process tree. Process tree itself contains\\nall the memory mappings, file descriptor tables, registers, environment variables, process IDs, etc.\\n\\nBefore `CRIU` was officially available, to create such a snapshot, it was necessary for users to maintain\\ntheir own custom variant of the kernel with required features. However, with `CRIU` available in the mainline\\nLinux Kernel, most of the container runtimes now do offer this facility. Of particular mention is `gVisor`. The \\ncontainer runtime utility, `runsc` which stands for `run sandboxed container`, to it\'s counterpart `runc`, has \\nadded functionality for checkpoint and restore. Given that `gVisor` has a usermode kernel functionality, it can \\ninfact, offer more granularity and features for checkpoint and restore.\\n\\n## Limitations\\n\\n- While the restore can be made in different computer, it has to mimic the original system as much as possible. Otherwise the invariants that program expect to be maintained will be broken, and can lead to some nasty surprises down the line. \\n- The CPU that the restore is made on has to match the instruction set, where the snapshot was taken, otherwise there can be runtime issues with invalid opcode.\\n- A container that utilizes GPU, is sensitive to differences in NVIDIA driver versions and in addition to container runtime versions.\\n- Programs need to account for the fact that the machine IP might change in between restore.\\n- A Problem documented in Modal documentation is that, there are some functions in `torch.cuda` that after restoring from snapshot will initialize CUDA as having zero GPU devices. The only fix is to reinitialize `torch.cuda` again.\\n- In particular, when loading a huge snapshot, there is a significant CPU pressure as hundreds of 4KiB pages are getting loaded into memory. Such a particular workload is particularly affected by [CPU Stalls](#cpu-stalls).\\n  \\n# CPU Stalls\\n\\nTo explain a CPU stall, we first need to understand that any given process is either running on the CPU or it\'s not. \\nThere can never be in between. So what happens when we have 3 processes each granted 0.3 of CPU running on the system.\\n\\nFor the above process, lets make a few assumptions. First is our CPU currently only has a single core and a single thread. Now, it\'s not really possible for us to divide this thread into 0.3 sections and grant each of the process\\nrunning a section. As previously stated, a process is either running or not running, and when it is running it \\nutilizes 100% of the thread and when it\'s not running it utilizes 0% of the thread. So instead, CPU makes the \\ndivision in time instead. For our example, lets consider that the CPU segments 100ms of CPU time window. So, \\nwith our 3 processes, in this 100ms window, each process gets 30ms of execution time, with the last 10ms being \\nwasted and non of our processes running. Again when a new 100ms window is created all 3 process are allowed to continue their run.\\n\\nNow, the issue is if you consider a compute intensive workload, it would really benefit off of that extra 10ms of \\nCPU time, but currently the way Linux schedular works, and also how the default CPU limits on pods by \\nkubernetes works, each of the process are allocated equal precedence. Hence, for performance critical processes, \\nit makes sense to also tune your application to get more CPU time. \\n\\nHowever, in real world, we don\'t have a single CPU core, and most commercial CPU\'s nowadays offer capabilities for running 2 threads. Also of note, most of our applications when parallelized, will spawn additional threads along \\nwith the main thread inside the running process. However, the CPU time granted to each is still at process level. \\nThat means, if say our hypothetical application has 3 threads each running in the same CPU time window, now each of\\nour thread will get 10ms of actual CPU time down from 30ms of time our single thread was getting. The problem is worsened if we have 10 threads and somehow all 10 threads are granted run in the same time, in different cores, then\\neach thread only gets about 3ms of worktime. \\n\\nThis situation of having potentially adequate CPU power, but still being unable to utilize 100% of the CPU for \\ncompute is known as CPU stall.\\n\\nIn a single thread situation, the best way to deal with this issue is to grant higher priority to the process to \\nallow it to gain more CPU time. And in the case of multi threaded situation, infact it\'s actually much more\\nbeneficial to pin the process to a certain CPU core. Infact, many games actually see quite a significant performance\\nboost when their processes are pinned to a few cores rather than allowing them access to all the cores.\\n\\n# Making `syscalls` in Linux\\n\\nMost of the program we write, ultimately have to make a `syscall` to do any operation on a system. Understandably, I\\nhad a misconception that the `syscalls` are themselves exposed over `C-API` and that any language that wishes to make\\na `syscall` had to at minimum link to a lower level `C library` like `libc` that does the work for them. However, \\na particular note was we can have `golang` applications without `C-go`. While at the time, I hadn\'t connected the \\ndots, I was recently watching some video on getting the smallest kernel, and there I chanced upon the fact that to\\nmake a `syscall` you don\'t really need to bind to `C library`. You just need to be able to emit specific assembly instructions.\\n\\nA `syscall` is triggered by writing the particular `syscall` number into a particular register. Then calling the \\n`syscall`  instruction in CPU. This internally triggers a trap request, which is captured by Kernel.\\n\\nGiven that it\'s assembly code, the following description is for how to perform a `syscall` in Linux, particular\\nin x86_64 system.\\n\\n- First place the system call number into the `rax` register. `write syscall` is for instance number 1, and `read syscall` is number 0. For more `syscall` numbers visit [here](https://elixir.bootlin.com/linux/v6.13/source/arch/x86/entry/syscalls/syscall_64.tbl).\\n\\n- Place the arguments for the system call into the designated registers, \\n  - `rdi` (first argument)\\n  - `rsi` (second argument)\\n  - `rdx` (third argument)\\n  - `r10` (fourth argument)\\n  - `r8` (fifth argument)\\n  - `r9` (sixth argument)\\n  \\n    This order is specified by the calling convention.\\n    \\n- Invoke the systemcall by executing the `syscall` instruction.\\n\\n- The return value is placed inside `rax`.\\n\\n# Some tools used to deploy model\\n\\n- [ray](https://www.ray.io/) Allows for distributed GPU computation\\n  - [kuberay](https://docs.ray.io/en/latest/cluster/kubernetes/index.html) to deploy ray in kubernetes\\n  \\n- [Kubeflow](https://www.kubeflow.org/) \\n\\n  A CNCF (Cloud Native Computing Formation) project, which is the foundation of tools for AI Platforms on Kubernetes. Of particular note is that recently in a conference, Ubucon, a speaker was telling that kubeflow is almost an industry standard for deploying AI in kubernetes."},{"id":"/2025/01/20/ai-video-cutter-week1","metadata":{"permalink":"/learnings/2025/01/20/ai-video-cutter-week1","source":"@site/learnings/2025-01-20-ai-video-cutter-week1.mdx","title":"AI Video Cutter - Week 1 Learnings","description":"Been working on this video cutting tool that uses AI to find optimal cut points. Some things worked, some things I had to completely rewrite. Here\'s what went down.","date":"2025-01-20T00:00:00.000Z","tags":[{"inline":true,"label":"video-processing","permalink":"/learnings/tags/video-processing"},{"inline":true,"label":"video clips","permalink":"/learnings/tags/video-clips"},{"inline":true,"label":"video cuts","permalink":"/learnings/tags/video-cuts"},{"inline":true,"label":"whisper","permalink":"/learnings/tags/whisper"},{"inline":true,"label":"python","permalink":"/learnings/tags/python"}],"readingTime":4.11,"hasTruncateMarker":true,"authors":[{"name":"Aman Kumar Singh","title":"Software Engineer Intern","url":"https://github.com/amankumarsingh77","page":{"permalink":"/learnings/authors/aksdev"},"socials":{"linkedin":"https://www.linkedin.com/in/aksdev/","github":"https://github.com/amankumarsingh77"},"imageURL":"https://github.com/amankumarsingh77.png","key":"aksdev"}],"frontMatter":{"title":"AI Video Cutter - Week 1 Learnings","date":"2025-01-20T00:00:00.000Z","type":"weekly","authors":["aksdev"],"tags":["video-processing","video clips","video cuts","whisper","python"]},"unlisted":false,"prevItem":{"title":"Weekly learnings: Week1","permalink":"/learnings/2025-10-ranjan-week1"},"nextItem":{"title":"Example Daily Learning","permalink":"/learnings/2025/01/15/example-learning"}},"content":"Been working on this video cutting tool that uses AI to find optimal cut points. Some things worked, some things I had to completely rewrite. Here\'s what went down.\\n\\n{/* truncate */}\\n\\n## Misunderstood the problem statement\\n\\nBuilt this whole `find_optimal_cut_point()` function that finds the optimal cutpoint in a video considering the Start point and End Point given by the user. Used Whisper for speech detection, OpenCV for scene changes, added motion analysis. Worked great.\\n\\nProblem: Understood the problem statement wrong. It was \\"Find the optimal frame to cut near the timestamp that the user gives\\". Which means that the user already knows where to cut but is not precise at a frame level, So we just ask for the estimated timestamp and then look around to find the best frame to cut.\\n\\nLearnings : Always clarify the problem statement by repeating it to the person. So you both are on the same page.\\n\\n## Weighted scoring doesn\'t work for hard constraints\\n\\nFirst version used a points system: sentence boundary +40pts, pause +30pts, scene change +30pts, etc. Add them up, pick highest score.\\n\\nBug: System kept cutting mid-word because a scene change (30pts) + low motion (20pts) outscored a sentence boundary (40pts) by itself.\\n\\nRealized some rules aren\'t negotiable. You literally cannot cut while someone is speaking. It\'s not a preference, it\'s a constraint.\\n\\nRewrote to use strict priority filtering:\\n\\n```python\\n# Priority 1: Remove all frames mid-speech (absolute)\\n# Priority 2: Remove frames with motion > 2.5 (strict)  \\n# Priority 3: Remove frames with blinks/open mouth (best effort)\\n```\\n\\nEach filter runs sequentially. If all frames fail a priority, fallback to previous level. This will be changed as if there are no frames left after filtering then its not a good range to extract any cut point.\\n\\nIf you catch yourself using weighted scoring to \\"compensate\\" for bad choices, you probably need filters instead.\\n\\n## Caching saves 90 seconds per request\\n\\nWhisper transcription: ~60s for 5min video. Scene detection: ~30s. Runs on every cut request.\\n\\nPeople want multiple clips from the same video. Transcribing the same video 5 times = 5 minutes wasted.\\n\\nAdded basic caching with video file hashes. First cut takes 90s, subsequent cuts from same video take 0.2s. Just stores the analysis results in `.video_cache/`.\\n\\n```python\\npipeline = FastPreprocessingPipeline(use_cache=True)\\nanalysis = pipeline.preprocess_video(\\"video.mp4\\")  # Slow first time, instant after\\n```\\n\\nShould\'ve done this from day one.\\n\\n## Integration is 80% of the work\\n\\nIndividual pieces are straightforward:\\n- Whisper gives word timestamps\\n- OpenCV detects scene changes\\n- Optical flow tracks motion\\n- MediaPipe finds faces/blinks\\n- Librosa analyzes audio\\n\\nMaking them work together without contradicting each other is some interesting job which I am still figuring out.\\n\\nHad to add a validation layer that checks if the selected range:\\n- Has complete sentences (not cut mid-thought)\\n- Doesn\'t have jarring visual jumps\\n- Maintains topic coherence\\n- Has stable audio (no clipping)\\n\\n## What\'s broken right now\\n\\nFace detection in \\"full\\" mode takes forever. Like 70% of total processing time. Need to profile this.\\n\\nSemantic analysis doesn\'t actually detect topic boundaries well. Using basic sentence embeddings but they\'re not good enough. Might need to try a different approach or just remove this feature.\\n\\nError messages are terrible. When the system can\'t find a good cut point it just says \\"Priority 2 failed\\" which means nothing to users.\\n\\n## Stuff that actually helped\\n\\nWhisper\'s tiny model works better than expected. Word-level timestamps are pretty accurate even with background noise and is fast enough. Might be because we are focusing on talking head/podcast videos.\\n\\nCyclopts for CLI was good. Better than argparse, handles nested commands cleanly.\\n\\nPySceneDetect saved time. Didn\'t have to write scene detection from scratch.\\n\\nMediaPipe face detection works but is slow. Might need to sample frames instead of processing every single one.\\n\\n## What I should\'ve done differently\\n\\nShould\'ve built the CLI interface first. Forces you to think about actual usage before implementation.\\n\\nShould\'ve started with the simplest possible version - just find pauses and cut there. Then add complexity. Instead I went straight to \\"multi-modal AI fusion\\" which took hours.\\n\\nShould\'ve added batch evaluation for accuracy testing from the start. Added it late and it\'s now one of the most useful features.\\n\\nShould\'ve added way more logging. When a cut looks wrong I have no idea why the system chose it. About to do it next week.\\n\\n## Random win\\n\\nThe batch evaluation system (process multiple videos, generate accuracy reports) wasn\'t even planned. Built it for testing. Turns out it\'s really useful for seeing how the system performs across different video types.\\n\\nSometimes the tools you build to test your code end up being features.\\n\\n## Next steps\\n\\nNeed to fix face detection performance. Probably sampling frames instead of processing all of them.\\n\\nSemantic analysis either needs better models or needs to be removed.\\n\\nBetter error messages so people know why a cut point was chosen or why it failed.\\n\\nAdd logging with jsonl and try to visualize each filter as to how they reject/accept a frame."},{"id":"/2025/01/15/example-learning","metadata":{"permalink":"/learnings/2025/01/15/example-learning","source":"@site/learnings/2025-01-15-example-learning.mdx","title":"Example Daily Learning","description":"Today learned that using const enum can improve bundle size by inlining values at compile time instead of generating JavaScript objects.","date":"2025-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"typescript","permalink":"/learnings/tags/typescript"},{"inline":true,"label":"performance","permalink":"/learnings/tags/performance"}],"readingTime":0.28,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Example Daily Learning","date":"2025-01-15T00:00:00.000Z","type":"weekly","tags":["typescript","performance"]},"unlisted":false,"prevItem":{"title":"AI Video Cutter - Week 1 Learnings","permalink":"/learnings/2025/01/20/ai-video-cutter-week1"}},"content":"Today learned that using `const enum` can improve bundle size by inlining values at compile time instead of generating JavaScript objects.\\n\\n{/* truncate */}\\n\\n## Key Takeaway\\n- Regular enums generate objects \u2192 runtime overhead\\n- `const enum` inlines values \u2192 zero runtime cost\\n- Trade-off: Can\'t use computed/dynamic access\\n\\n## Example\\n```typescript\\nconst enum Direction {\\n  Up,\\n  Down\\n}\\n// Compiles to: console.log(0)\\nconsole.log(Direction.Up);\\n```"}]}}')}}]);