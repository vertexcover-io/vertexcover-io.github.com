"use strict";(self.webpackChunkblog_vertexcover=self.webpackChunkblog_vertexcover||[]).push([[5997],{7360:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025-10-ranjan-week2","metadata":{"permalink":"/learnings/2025-10-ranjan-week2","source":"@site/learnings/2025-10-ranjan-week2.mdx","title":"Weekly learnings: Week2","description":"Developed a comprehensive benchmarking framework for comparing container image formats (regular vs eStargz) for large LLM workloads using containerd and stargz-snapshotter.","date":"2025-10-22T00:00:00.000Z","tags":[],"readingTime":13.88,"hasTruncateMarker":true,"authors":[{"name":"Ranjan Ojha","title":"Software Engineer","url":"https://github.com/hungerarray","page":{"permalink":"/learnings/authors/ranjan"},"socials":{"github":"https://github.com/hungerarray","linkedin":"https://www.linkedin.com/in/ojha-ranjan/"},"imageURL":"https://github.com/hungerarray.png","key":"ranjan"}],"frontMatter":{"title":"Weekly learnings: Week2","date":"2025-10-22T00:00:00.000Z","type":"weekly","authors":["ranjan"],"tags":[],"draft":false},"unlisted":false,"nextItem":{"title":"AI Video Cutter - Week 2 Learnings","permalink":"/learnings/2025-10-aman-week2"}},"content":"Developed a comprehensive benchmarking framework for comparing container image formats (regular vs eStargz) for large LLM workloads using containerd and stargz-snapshotter.\\n\\n{/* truncate */}\\n\\n### Key Findings\\n\\n**Lazy pulling with eStargz provides dramatic startup improvements:**\\n- **150x faster pull times** (9.2s \u2192 0.06s)\\n- **13.9x faster cold starts** (9.4s \u2192 0.67s)\\n- **Zero disk storage overhead**\\n\\n**But reveals a critical trade-off for data-intensive workloads:**\\n- **1.5-2x slower total completion** when accessing >30% of image data\\n- Stress test (8GB sequential read): Overlayfs 45-54s vs Stargz 79-88s\\n- **Working set size determines which approach is faster**\\n\\n**Bottom line:** Lazy pulling optimizes for startup latency (ideal for inference/serving), while eager loading optimizes for total completion time (better for training/batch processing).\\n\\n---\\n\\n## Key Results\\n\\n### Cold Start Performance (Small Working Set - 2GB Image)\\n\\n| Metric | Regular Pull | Lazy Pull (eStargz) | Improvement |\\n|--------|--------------|---------------------|-------------|\\n| **Pull time** | 9.178s | 0.061s | **150x faster** |\\n| **Container start + ready** | 199ms | 587ms | Slower (on-demand fetch) |\\n| **Total cold start** | 9.401s | 0.675s | **13.9x faster** |\\n| **Data downloaded at pull** | 2.0 GB | ~9 KB | **99.9% reduction** |\\n| **Disk usage after pull** | 2.0 GB cached | 0 bytes | **100% savings** |\\n\\n**Scenario**: Application with small working set (~1-5% of image accessed)\\n\\n### Total Workload Completion (Large Working Set - 8GB Image) \u26a0\ufe0f\\n\\n**CRITICAL TRADE-OFF**: When workloads access significant portions of the image, lazy pulling is **SLOWER** for total completion time.\\n\\n**Stress Test Results** (sequential read of 8GB data):\\n\\n| Mode | Registry | File Pattern | Total Time | vs Overlayfs |\\n|------|----------|--------------|------------|--------------|\\n| **Overlayfs** | localhost | many-small | 52s | baseline |\\n| **Overlayfs** | localhost | few-large | 54s | baseline |\\n| **Overlayfs** | 172.17.0.2 | many-small | 45s | baseline |\\n| **Overlayfs** | 172.17.0.2 | few-large | 45s | baseline |\\n| **Stargz** | localhost | many-small | **88s** | **1.7x slower** \u274c |\\n| **Stargz** | localhost | few-large | **79s** | **1.5x slower** \u274c |\\n| **Stargz** | 172.17.0.2 | many-small | **82s** | **1.8x slower** \u274c |\\n| **Stargz** | 172.17.0.2 | few-large | **83s** | **1.8x slower** \u274c |\\n\\n**Why Lazy Pulling is Slower for Data-Intensive Workloads:**\\n\\n```\\nOverlayfs (eager loading):\\n  Bulk download: 45-53s (parallel, full bandwidth)\\n  Workload execution: Fast (all data local on SSD)\\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n  Total: 45-54s\\n\\nStargz (lazy loading):\\n  Metadata pull: &lt;1s\\n  Workload execution: 78-87s (many serialized HTTP range requests)\\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n  Total: 79-88s (1.5-2x SLOWER!)\\n```\\n\\n**Performance Breakdown:**\\n- **Pull phase**: Stargz wins (150x faster) \u2705\\n- **Execution phase**: Overlayfs wins (on-demand HTTP requests slower than local disk) \u2705\\n- **Total time**: Depends on working set size and access pattern\\n\\n### Key Insights\\n\\n1. **Cold start time \u2260 Total completion time**\\n   - Lazy pulling optimizes startup latency\\n   - BUT penalizes total workload completion when data access is substantial\\n\\n2. **Working set size is critical**\\n   - Small working set (&lt;10%): Lazy pulling wins dramatically (13.9x faster)\\n   - Large working set (>30%): Eager loading wins (1.5-2x faster)\\n\\n3. **File pattern sensitivity**\\n   - Many small files: Worse for lazy pulling (88s vs 79s)\\n   - Each file = separate HTTP request = more latency overhead\\n\\n4. **Network vs disk I/O trade-off**\\n   - Bulk parallel download: ~45-53s for 8GB\\n   - Serialized on-demand fetches: ~78-87s for same data\\n   - Local disk reads >> HTTP range requests for large data access\\n\\n---\\n\\n## Technical Architecture\\n\\n### Core Components\\n\\n1. **Containerd Benchmark Framework** (`containerd-bench/`)\\n   - Pure Go API integration with containerd\\n   - Programmatic control over container lifecycle\\n   - JSON Lines logging for performance analysis\\n   - Operations: PullImage, RPullImage (lazy), CreateContainer, StartContainer, etc.\\n\\n2. **Lazy Pulling with eStargz**\\n   - Uses stargz-snapshotter plugin for on-demand layer fetching\\n   - HTTP range requests to fetch only needed chunks\\n   - FUSE filesystem for transparent lazy loading\\n   - Zero disk storage overhead\\n\\n3. **Startup Benchmarking Tool** (`startup-bench/`)\\n   - Cold and warm start measurements\\n   - Container readiness detection (not just process start)\\n   - Auto-detection of eStargz images by `:esgz` suffix\\n   - Support for plain HTTP registries\\n\\n---\\n\\n## How Lazy Pulling Works\\n\\n### Phase 1: Metadata Fetch (~0.06s for 2GB image)\\n```\\nDownload index (290B) + manifest (2.6KB) + config (6.3KB) = ~9KB\\nRegister layers with stargz-snapshotter as \\"remote\\"\\n```\\n\\n### Phase 2: Container Creation (~0.03s)\\n```\\nStargz-snapshotter creates remote snapshot mounts\\nFUSE filesystem presents layer contents virtually\\nContainer starts WITHOUT waiting for layer downloads\\n```\\n\\n### Phase 3: On-Demand Fetching (during container runtime)\\n```\\nApplication reads /app/data/file.dat\\n  \u2193\\nFUSE intercepts read()\\n  \u2193\\nHTTP GET with Range: bytes=1024-2048 to registry\\n  \u2193\\nData returned (cached in memory, NOT disk)\\n```\\n\\n**Result**: For 2GB image with small working set (~20-30MB accessed), only those chunks are fetched.\\n\\n### Performance Implications\\n\\n**Small Working Set (&lt;10% of image):**\\n```\\nPull: &lt;1s (metadata only)\\nRuntime: Fast (few on-demand fetches)\\nTotal: 13.9x faster than eager loading \u2705\\n```\\n\\n**Large Working Set (>30% of image):**\\n```\\nPull: &lt;1s (metadata only)\\nRuntime: 78-87s (many serialized HTTP requests)\\nTotal: 1.5-2x SLOWER than eager loading \u274c\\n\\nWhy slower:\\n- Bulk parallel download: 45-53s for 8GB\\n- On-demand serial fetches: 78-87s for same 8GB\\n- Each file access = network round-trip\\n- FUSE overhead + HTTP request overhead\\n```\\n\\n**Trade-off:** Fast startup vs total completion time depends on working set size.\\n\\n---\\n\\n## Implementation Highlights\\n\\n### RPullImage Operation\\n\\nUsed `source.AppendDefaultLabelsHandlerWrapper()` from stargz-snapshotter:\\n\\n```go\\nimport (\\n    \\"github.com/containerd/containerd/v2/client\\"\\n    \\"github.com/containerd/stargz-snapshotter/fs/source\\"\\n)\\n\\n// Create label handler - this enables lazy pulling!\\nlabelHandler := source.AppendDefaultLabelsHandlerWrapper(imageRef, prefetchSize)\\n\\npullOpts := []client.RemoteOpt{\\n    client.WithPullUnpack,\\n    client.WithImageHandlerWrapper(labelHandler),  // Essential for lazy pulling\\n    client.WithPullSnapshotter(\\"stargz\\"),\\n}\\n\\n_, err := containerdClient.Pull(ctx, imageRef, pullOpts...)\\n```\\n\\n**Critical Insight**: Regular `containerd.Pull()` downloads everything even with stargz snapshotter. The label handler wrapper is **essential** for true lazy pulling.\\n\\n---\\n\\n## Critical Bugs Discovered & Fixed\\n\\n### 1. Content Blob Caching\\n\\n**Problem**: Cold start iterations reused cached content blobs (48s \u2192 0.17s on iteration 2).\\n\\n**Root Cause**: Content blobs are globally shared across namespaces. Image removal only cleared metadata.\\n\\n**Solution**: Use `images.SynchronousDelete()` to trigger immediate garbage collection:\\n\\n```go\\ndeleteOpts := []images.DeleteOpt{images.SynchronousDelete()}\\nimageService.Delete(ctx, imageRef, deleteOpts...)\\n```\\n\\n### 2. Metadata Corruption\\n\\n**Problem**: Mixing regular `Pull()` and `rpull` caused \\"target snapshot already exists\\" errors.\\n\\n**Root Cause**: Content blobs retained `containerd.io/uncompressed` annotations from previous pulls.\\n\\n**Solution**: Clean content store before lazy pulling:\\n\\n```bash\\nsudo ctr-remote content ls | grep workload | awk \'{print $1}\' | \\\\\\n  xargs -I {} sudo ctr-remote content rm {}\\n```\\n\\n**Prevention**: Never mix pull methods - always use RPullImage for eStargz images.\\n\\n### 3. Plain HTTP Registry Support\\n\\n**Problem**: Custom Docker resolver breaks lazy pulling by forcing full downloads.\\n\\n**Solution for RPullImage**: Configure stargz-snapshotter daemon instead:\\n\\n```toml\\n# /etc/containerd-stargz-grpc/config.toml\\n[[resolver.host.\\"172.17.0.2:5000\\".mirrors]]\\nhost = \\"172.17.0.2:5000\\"\\ninsecure = true\\n```\\n\\n---\\n\\n## eStargz Format Verification\\n\\n### Key Characteristics\\n\\n**Media Type**: `application/vnd.oci.image.layer.v1.tar+gzip` (same as regular gzip!)\\n\\n**Distinguishing Features**:\\n1. **STARGZ footer** in blob (verify with `xxd`)\\n2. **TOC digest annotation**: `containerd.io/snapshot/stargz/toc.digest`\\n3. **Uncompressed size annotation**: `io.containers.estargz.uncompressed-size`\\n\\n### Creating eStargz Images\\n\\nUse ctr-remote workflow (NOT docker buildx):\\n\\n```bash\\n# 1. Build regular image\\ndocker buildx build -t localhost:5000/image:base --push .\\n\\n# 2. Pull to containerd\\nsudo ctr-remote image pull localhost:5000/image:base\\n\\n# 3. Optimize to eStargz\\nsudo ctr-remote image optimize --no-optimize --oci \\\\\\n  localhost:5000/image:base localhost:5000/image:esgz\\n\\n# 4. Push eStargz image\\nsudo ctr-remote images push --plain-http localhost:5000/image:esgz\\n```\\n\\n### Verification\\n\\n```bash\\n# Check manifest annotations\\ncurl -s http://localhost:5000/v2/image/manifests/esgz | \\\\\\n  jq \'.layers[].annotations\'\\n\\n# Verify STARGZ footer in blob\\nsudo tail -c 100 /var/lib/containerd/.../blobs/sha256/... | xxd | tail -3\\n# Look for: \\"STARGZ\\" marker\\n```\\n\\n---\\n\\n## Best Practices\\n\\n### Decision Framework: Lazy Pulling vs Eager Loading\\n\\n**The critical factor is WORKING SET SIZE:**\\n\\n```\\nWorking Set < 10% of image:\\n  \u2192 Use lazy pulling (13.9x faster startup)\\n\\nWorking Set > 30% of image:\\n  \u2192 Use eager loading (1.5-2x faster total completion)\\n\\nWorking Set 10-30% of image:\\n  \u2192 Depends on whether you optimize for startup or total time\\n```\\n\\n### When to Use Lazy Pulling \u2705\\n\\n**Best for startup latency optimization:**\\n\\n1. **Small working set** (&lt;10% of image accessed)\\n   - Example: Web API loading libraries (100MB of 2GB image)\\n   - Result: 13.9x faster cold start\\n\\n2. **Ephemeral workloads** - short-lived containers\\n   - Containers that start, perform task, exit quickly\\n   - Don\'t benefit from caching anyway\\n\\n3. **Cold start critical** - startup time is the bottleneck\\n   - Serverless functions\\n   - Auto-scaling scenarios\\n   - Development/testing iterations\\n\\n4. **Limited disk space** - can\'t cache full images\\n   - Edge devices\\n   - Multi-tenant nodes with many images\\n\\n5. **High bandwidth, low latency** to registry\\n   - On-demand fetches need fast network\\n   - Registry co-located with compute\\n\\n**Example Use Case:**\\n```\\nLLM Inference API:\\n- Image: 4GB (model weights)\\n- Working set: 300MB (actively loaded model portion)\\n- Access pattern: Load once, serve many requests\\n\\nLazy pulling: 1-2s startup vs 18s eager\\nResult: 9-18x faster! \u2705\\n```\\n\\n### When NOT to Use Lazy Pulling \u274c\\n\\n**Eager loading is faster when:**\\n\\n1. **Large working set** (>30% of image accessed)\\n   - Example: Batch processing reading 8GB of 8GB image\\n   - Result: Lazy pulling 1.5-2x SLOWER for total completion\\n\\n2. **Data-intensive workloads** - process significant data\\n   - Training jobs accessing entire dataset\\n   - ETL pipelines reading many files\\n   - Stress tests (like our benchmark)\\n\\n3. **Sequential file access** - many files read in order\\n   - Each file = separate HTTP request with lazy pulling\\n   - Bulk download is much faster (parallel, large chunks)\\n\\n4. **Small images** (&lt;100MB) - overhead not worth it\\n   - Metadata overhead dominates\\n\\n5. **Slow/high-latency network** - on-demand fetches will be slow\\n   - Each file access waits for network round-trip\\n\\n6. **Offline/air-gapped environments** - no registry access\\n\\n**Example Use Case:**\\n```\\nLLM Training/Fine-tuning:\\n- Image: 8GB (dataset + checkpoints)\\n- Working set: 7GB (accessing most data during training)\\n- Access pattern: Read many files sequentially\\n\\nLazy pulling: 79-88s total time\\nEager loading: 45-54s total time\\nResult: Eager 1.5-2x faster! \u2705\\n```\\n\\n### Performance Optimization Matrix\\n\\n| Metric to Optimize | Image Size | Working Set | Recommendation |\\n|-------------------|------------|-------------|----------------|\\n| **Cold start time** | Large (>1GB) | Small (&lt;10%) | Lazy pulling \u2705 |\\n| **Cold start time** | Large (>1GB) | Large (>30%) | Lazy pulling \u2705 (startup only) |\\n| **Total completion time** | Large (>1GB) | Small (&lt;10%) | Lazy pulling \u2705 |\\n| **Total completion time** | Large (>1GB) | Large (>30%) | Eager loading \u2705 |\\n| **Disk usage** | Any | Any | Lazy pulling \u2705 |\\n| **Network bandwidth** | Large (>1GB) | Small (&lt;10%) | Lazy pulling \u2705 |\\n| **Network bandwidth** | Large (>1GB) | Large (>30%) | Eager loading \u2705 |\\n\\n---\\n\\n## Architecture Insights\\n\\n### Containerd Design Principles\\n\\n1. **Content Store is Global**\\n   - Image metadata: namespaced \u2705\\n   - Container metadata: namespaced \u2705\\n   - Content blobs: **GLOBAL** (shared across namespaces) \u274c\\n\\n2. **Snapshotter Abstraction**\\n   - Each snapshotter has unique requirements\\n   - Stargz needs special label handlers for lazy pulling\\n   - Not as simple as just switching a snapshotter flag\\n\\n3. **Trade-offs**\\n   - **Startup latency**: Lazy pulling dramatically faster (13.9x)\\n   - **Total completion**: Depends on working set size\\n     - Small working set (&lt;10%): Lazy pulling wins (13.9x faster)\\n     - Large working set (>30%): Eager loading wins (1.5-2x faster)\\n   - **Network vs Disk I/O**:\\n     - Bulk parallel download: ~45-53s for 8GB\\n     - Serialized on-demand fetches: ~78-87s for same 8GB data\\n     - Local disk reads >> HTTP range requests for large data access\\n   - **Zero storage overhead** vs traditional caching benefits\\n   - **Network-dependent performance** - requires good bandwidth/latency\\n\\n### Version Compatibility\\n\\n**Critical**: Match library versions with system installations\\n\\n```bash\\n# Check system version\\ncontainerd --version  # v2.1.4\\n\\n# Use matching library version\\ngo get github.com/containerd/containerd/v2@v2.1.4\\n```\\n\\n---\\n\\n## Debugging Techniques\\n\\n### 1. Check Content Store Annotations\\n```bash\\nsudo ctr-remote content ls | grep image\\n# Look for: containerd.io/uncompressed annotations\\n```\\n\\n### 2. Verify Lazy Pulling is Active\\n```bash\\n# Inside container, check for stargz metadata\\nls /.stargz-snapshotter/\\ncat /.stargz-snapshotter/*.json\\n```\\n\\n### 3. Binary Verification\\n```bash\\n# Check STARGZ footer (authoritative proof)\\nsudo tail -c 100 /var/lib/containerd/.../blobs/sha256/... | xxd | tail -3\\n```\\n\\n### 4. Monitor On-Demand Fetches\\n```bash\\n# Watch stargz-snapshotter logs\\nsudo journalctl -u stargz-snapshotter -f\\n```\\n\\n---\\n\\n## Quick Reference Commands\\n\\n### Setup\\n```bash\\n# Start local registry\\ndocker run -d --name registry -p 5000:5000 registry:2\\n\\n# Start stargz-snapshotter\\nsudo systemctl start stargz-snapshotter\\n```\\n\\n### Benchmarking\\n```bash\\n# Build tool\\ncd startup-bench && go build -o startup-bench main.go\\n\\n# Cold start with lazy pulling (auto-detected via :esgz suffix)\\nsudo ./startup-bench \\\\\\n  -image=localhost:5000/workload-2048mb-few-large:esgz \\\\\\n  -snapshotter=stargz \\\\\\n  -mode=cold \\\\\\n  -iterations=3\\n\\n# Cold start with regular pull\\nsudo ./startup-bench \\\\\\n  -image=localhost:5000/workload-2048mb-few-large:latest \\\\\\n  -snapshotter=overlayfs \\\\\\n  -mode=cold \\\\\\n  -iterations=3\\n```\\n\\n### Cleanup\\n```bash\\n# Clean content store for true cold starts\\nsudo ctr-remote content ls | grep workload | awk \'{print $1}\' | \\\\\\n  xargs -I {} sudo ctr-remote content rm {}\\n\\n# Restart services\\nsudo systemctl restart stargz-snapshotter\\nsudo systemctl restart containerd\\n```\\n\\n---\\n\\n## External Resources\\n\\n### Official Documentation\\n\\n**Stargz-Snapshotter**:\\n- [Project Overview](https://github.com/containerd/stargz-snapshotter/blob/main/docs/overview.md)\\n- [Getting Started Guide](https://github.com/containerd/stargz-snapshotter/blob/main/docs/getting-started.md)\\n- [ctr-remote CLI Tool](https://github.com/containerd/stargz-snapshotter/blob/main/docs/ctr-remote.md)\\n- [Registry Configuration](https://github.com/containerd/stargz-snapshotter/blob/main/docs/overview.md#registry-mirrors-and-insecure-connection)\\n- [GitHub Repository](https://github.com/containerd/stargz-snapshotter)\\n\\n**Containerd**:\\n- [Official Documentation](https://github.com/containerd/containerd/tree/main/docs)\\n- [Garbage Collection](https://github.com/containerd/containerd/blob/main/docs/garbage-collection.md)\\n- [Content Store Design](https://github.com/containerd/containerd/blob/main/docs/content-flow.md)\\n- [Namespaces](https://github.com/containerd/containerd/blob/main/docs/namespaces.md)\\n- [Snapshotters](https://github.com/containerd/containerd/blob/main/docs/snapshotters/README.md)\\n- [GitHub Repository](https://github.com/containerd/containerd)\\n\\n**OCI Specifications**:\\n- [OCI Image Spec](https://github.com/opencontainers/image-spec)\\n- [Media Types](https://github.com/opencontainers/image-spec/blob/main/media-types.md)\\n- [Image Layer Spec](https://github.com/opencontainers/image-spec/blob/main/layer.md)\\n\\n**eStargz Format**:\\n- [eStargz Paper](https://github.com/containerd/stargz-snapshotter/blob/main/docs/estargz.md)\\n- [Format Specification](https://github.com/containerd/stargz-snapshotter/blob/main/docs/stargz-estargz.md)\\n\\n### Related Tools & Projects\\n\\n**Container Runtimes**:\\n- [containerd](https://containerd.io/) - Industry-standard container runtime\\n- [runc](https://github.com/opencontainers/runc) - OCI container runtime\\n\\n**Image Optimization**:\\n- [Buildkit](https://github.com/moby/buildkit) - Concurrent, cache-efficient build toolkit\\n- [Nydus](https://github.com/dragonflyoss/nydus) - Alternative lazy-pulling solution by Dragonfly\\n\\n**Registries**:\\n- [Docker Registry](https://docs.docker.com/registry/) - Open-source registry implementation\\n- [Distribution Spec](https://github.com/opencontainers/distribution-spec) - OCI distribution specification\\n\\n### Research Papers & Articles\\n\\n**Lazy Pulling & Container Startup**:\\n- [FAST \'20: Startup Containers in Lightning Speed with Lazy Image Distribution](https://www.usenix.org/conference/fast20/presentation/li)\\n- [Slacker: Fast Distribution with Lazy Docker Containers](https://www.usenix.org/conference/fast16/technical-sessions/presentation/harter)\\n\\n**Container Image Optimization**:\\n- [USENIX ATC \'19: Packer: Toward Million-fold Container Image Optimization](https://www.usenix.org/conference/atc19/presentation/zhao-jian)\\n\\n### Community & Support\\n\\n**Issue Trackers**:\\n- [Stargz-Snapshotter Issues](https://github.com/containerd/stargz-snapshotter/issues)\\n- [Containerd Issues](https://github.com/containerd/containerd/issues)\\n- [Nydus Lazy Pulling Issue #1527](https://github.com/dragonflyoss/nydus/issues/1527) - Related metadata corruption\\n\\n**Communication**:\\n- [Containerd Slack](https://slack.containerd.io/) - Community chat\\n- [CNCF Slack #containerd](https://cloud-native.slack.com/messages/containerd) - Technical discussions\\n\\n### Tutorials & Guides\\n\\n**Getting Started**:\\n- [Containerd Getting Started](https://containerd.io/docs/getting-started/)\\n- [Stargz-Snapshotter Quick Start](https://github.com/containerd/stargz-snapshotter/blob/main/docs/getting-started.md)\\n\\n**Advanced Topics**:\\n- [Lazy Pulling Deep Dive](https://github.com/containerd/stargz-snapshotter/blob/main/docs/overview.md#lazy-pulling-feature)\\n- [Building eStargz Images](https://github.com/containerd/stargz-snapshotter/blob/main/docs/ctr-remote.md#building-estargz)\\n\\n---\\n\\n## Key Takeaways\\n\\n### Technical Insights\\n\\n1. **Lazy pulling is essential for large images** - 150x pull speedup for 2GB images\\n2. **Label handlers are critical** - Regular containerd.Pull() doesn\'t enable lazy pulling\\n3. **Content store is global** - Shared across namespaces, requires explicit cleanup\\n4. **Never mix pull methods** - Causes metadata corruption\\n5. **eStargz verification** - Check STARGZ footer in blobs, not just media type\\n\\n### Performance Characteristics\\n\\n1. **Speedup scales with image size** - Larger images benefit more\\n2. **Network-dependent** - Requires good bandwidth/latency to registry\\n3. **Working set matters** - Only fetches accessed files\\n4. **Trade-off exists** - Faster pull, slightly slower start\\n5. **Zero storage overhead** - No disk caching of full layers\\n\\n### Development Best Practices\\n\\n1. **Performance-driven debugging** - Timing anomalies reveal bugs\\n2. **Binary-level verification** - Source of truth for format validation\\n3. **Read upstream source code** - Reveals exact implementation details\\n4. **Test with realistic workloads** - Small images don\'t show benefits\\n5. **Match system versions** - Library versions should align with binaries\\n\\n---\\n\\n## Conclusion\\n\\nThis project demonstrates that **eStargz with lazy pulling provides dramatic performance improvements for startup latency**, but reveals a **critical trade-off with total workload completion time** that depends on working set size.\\n\\n### Key Findings\\n\\n**\u2705 Lazy Pulling Wins for Startup Latency:**\\n- 150x faster pull time (9.2s \u2192 0.06s)\\n- 13.9x faster cold start (9.4s \u2192 0.67s)\\n- Zero disk storage overhead\\n- Ideal for small working sets (&lt;10% of image)\\n\\n**\u26a0\ufe0f Eager Loading Wins for Data-Intensive Workloads:**\\n- 1.5-2x faster total completion for large working sets (>30% of image)\\n- Bulk parallel downloads faster than serialized on-demand fetches\\n- Better for batch processing, training, ETL pipelines\\n- Stress test (8GB sequential read): Overlayfs 45-54s vs Stargz 79-88s\\n\\n### Decision Framework\\n\\n**The critical question: What percentage of your image does the workload access?**\\n\\n```\\nSmall working set (&lt;10%):\\n  \u2192 Lazy pulling essential (13.9x faster)\\n  \u2192 Example: LLM inference API loading 300MB of 4GB image\\n\\nLarge working set (>30%):\\n  \u2192 Eager loading faster (1.5-2x faster total time)\\n  \u2192 Example: Training job accessing 7GB of 8GB image\\n\\nOptimize for startup time:\\n  \u2192 Always use lazy pulling\\n\\nOptimize for total completion time:\\n  \u2192 Use lazy pulling only for small working sets\\n```\\n\\n### Production Recommendations\\n\\n1. **LLM Inference/Serving** - Use lazy pulling \u2705\\n   - Small working set, startup critical\\n   - 10-20x faster cold start for auto-scaling\\n\\n2. **LLM Training/Fine-tuning** - Use eager loading \u2705\\n   - Large working set, total time matters\\n   - Avoid 1.5-2x penalty for on-demand fetches\\n\\n3. **Development/Testing** - Use lazy pulling \u2705\\n   - Fast iteration cycles\\n   - Disk space savings\\n\\n### Technical Validation\\n\\nThe implementation validates that:\\n- Proper integration with stargz-snapshotter enables true lazy pulling\\n- Label handlers (`AppendDefaultLabelsHandlerWrapper`) are essential\\n- Content store management critical for accurate benchmarking\\n- Working set size is the primary performance factor\\n- Network vs disk I/O trade-off is significant for large data access"},{"id":"/2025-10-aman-week2","metadata":{"permalink":"/learnings/2025-10-aman-week2","source":"@site/learnings/2025-10-aman-week2.mdx","title":"AI Video Cutter - Week 2 Learnings","description":"{/ truncate /}","date":"2025-10-21T00:00:00.000Z","tags":[{"inline":true,"label":"video-processing","permalink":"/learnings/tags/video-processing"},{"inline":true,"label":"video clips","permalink":"/learnings/tags/video-clips"},{"inline":true,"label":"video cuts","permalink":"/learnings/tags/video-cuts"},{"inline":true,"label":"python","permalink":"/learnings/tags/python"}],"readingTime":4.83,"hasTruncateMarker":true,"authors":[{"name":"Aman Kumar Singh","title":"Software Engineer Intern","url":"https://github.com/amankumarsingh77","page":{"permalink":"/learnings/authors/aksdev"},"socials":{"linkedin":"https://www.linkedin.com/in/aksdev/","github":"https://github.com/amankumarsingh77"},"imageURL":"https://github.com/amankumarsingh77.png","key":"aksdev"}],"frontMatter":{"title":"AI Video Cutter - Week 2 Learnings","date":"2025-10-21T00:00:00.000Z","type":"weekly","authors":["aksdev"],"tags":["video-processing","video clips","video cuts","python"]},"unlisted":false,"prevItem":{"title":"Weekly learnings: Week2","permalink":"/learnings/2025-10-ranjan-week2"},"nextItem":{"title":"Weekly learnings: Week1","permalink":"/learnings/2025-10-ranjan-week1"}},"content":"{/* truncate */}\\n\\n# Why Thresholding Approach Doesn\'t Work Well\\n\\nThe original approach relied heavily on fixed thresholds to determine whether a frame was suitable for cutting. For example, we would check if motion was below 2.5 pixels/frame, or if mouth openness was below 0.08 aspect ratio. While this seemed reasonable in theory, several fundamental issues emerged:\\n\\n**The Binary Decision Problem**: Thresholds create hard boundaries - a frame with motion at 2.4 is \\"good\\" while one at 2.6 is \\"bad\\". This ignores the fact that quality exists on a spectrum. We were discarding potentially excellent cut points simply because one metric was slightly above threshold, even if all other metrics were perfect.\\n\\n**The Context Blindness Problem**: A threshold of 2.5 pixels/frame might be perfectly fine for one video but completely wrong for another. A talking head video shot on a stabilized camera will have very different motion characteristics than one shot handheld. The threshold approach had no way to understand relative quality within a video\'s context.\\n\\n**The Combinatorial Explosion Problem**: With 5+ metrics (eye openness, motion, expression, pose, sharpness), we faced an impossible tuning challenge. What if a frame had excellent eye openness and stability but slightly high motion? Which threshold should win? We ended up with increasingly complex boolean logic that was brittle and hard to reason about.\\n\\n**The \\"No Good Frame\\" Scenario**: In some video segments, no frame passed all thresholds. The system would then either fail completely or pick arbitrarily, defeating the purpose of quality analysis. The threshold approach couldn\'t say \\"this frame is better than that one\\" - it could only say \\"pass\\" or \\"fail\\".\\n\\nThese limitations made it clear that we needed a fundamentally different approach that could handle relative quality, context-awareness, and graceful degradation.\\n\\n# Moving to a Pure Ranking Algorithm\\n\\nInstead of asking \\"does this frame pass?\\", we now ask \\"which frame is best?\\". The ranking algorithm in `try/cut_point_ranker/` implements a pure ranking approach with no thresholds whatsoever. Every frame gets scored, and we simply pick the highest-ranked ones.\\n\\n## Multi-Factor Scoring System\\n\\nThe ranker combines 5 key metrics with configurable weights:\\n\\n1. **Eye Openness** - Uses Eye Aspect Ratio (EAR) detection based on Soukupov\xe1 & \u010cech (2016). We want eyes fully open, not blinking or partially closed.\\n\\n2. **Motion Stability** - Uses Farneback optical flow to measure frame-to-frame motion. Lower motion is better (inverse scoring).\\n\\n3. **Expression Neutrality** - Analyzes MediaPipe facial blendshapes to detect mouth movement and eyebrow activity. We prefer neutral expressions (inverse scoring).\\n\\n4. **Pose Stability** - Measures head pose deviation from neutral position. Stable head position is better (inverse scoring).\\n\\n5. **Visual Sharpness** - Laplacian variance to ensure the frame isn\'t blurry.\\n\\nThe beauty of weighted scoring is that it allows trade-offs. A frame with perfect eye openness but slightly higher motion can still score well, whereas with thresholds it might have been rejected entirely.\\n\\n## Multi-Stage Ranking Pipeline\\n\\nThe scoring happens in multiple stages to ensure quality:\\n\\n**Stage 1: Feature Extraction** - Extract all raw metrics from every frame in the time range. This is done in a single pass through the video for efficiency.\\n\\n**Stage 2: Normalization** - Normalize all metrics to [0, 1] range across the entire segment. This is crucial - the normalization happens relative to the video segment being analyzed, making it context-aware.\\n\\n**Stage 3: Quality Gating** - Instead of hard thresholds, we use percentile-based penalties. If a frame\'s expression activity is above the 75th percentile for the segment, it receives a penalty multiplier. The penalty is proportional to how extreme the outlier is, not a binary yes/no.\\n\\n**Stage 4: Local Stability Boost** - Frames that are part of stable sequences (low variance in a 5-frame window) receive a boost multiplier. This rewards temporal coherence.\\n\\n**Stage 5: Context Window Smoothing** - Apply a sliding window average over composite scores. A frame is better if its neighbors are also good, ensuring we don\'t pick an isolated good frame in a bad sequence.\\n\\n# Research-Based Feature Extraction\\n\\nRather than inventing metrics from scratch, the implementation uses proven computer vision algorithms:\\n\\n- **Eye Aspect Ratio (EAR)**: Based on the 2016 paper by Soukupov\xe1 & \u010cech for blink detection. Calculates ratio of vertical to horizontal eye landmark distances.\\n\\n- **Optical Flow**: Uses Farneback dense optical flow, a well-established method for motion estimation in video.\\n\\n- **MediaPipe FaceMesh**: Google\'s solution providing 478 facial landmarks plus 52 blendshape coefficients for detailed expression analysis.\\n\\n- **Laplacian Variance**: Standard technique for measuring image sharpness by analyzing high-frequency content.\\n\\n# Word-Level Pause Detection\\n\\nImproved the speech analysis to detect pauses at word-level granularity rather than just sentence boundaries. This allows us to find natural cut points mid-sentence during natural speaking pauses, which is especially important for AI-generated talking head videos where sentences can be quite long.\\n\\nThe transcription analysis now tracks word-level timestamps and identifies pauses between words that are long enough to indicate a natural break in speech flow.\\n\\n# Adaptive Padding for Cut Points\\n\\nImplemented an adaptive padding calculator that analyzes visual stability around a cut point to determine optimal padding amounts. Instead of fixed padding values, the system now:\\n\\n- Analyzes frame similarity before/after the cut point\\n- Measures face movement and general motion\\n- Detects scene changes that would affect padding needs\\n\\nThis ensures we add just enough padding for smooth transitions without including unstable or transitional frames, with padding amounts ranging from configured minimum to maximum based on actual video content stability.\\n\\n# References and Resources\\n\\n- **Eye Aspect Ratio (EAR)**: [Real-Time Eye Blink Detection using Facial Landmarks](http://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf)\\n- **Farneback Optical Flow**: [Two-Frame Motion Estimation Based on Polynomial Expansion](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf)\\n- **MediaPipe FaceMesh**: [Official Documentation](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker)\\n- **Laplacian Variance**: [PyImageSearch Blur Detection Tutorial](https://pyimagesearch.com/2015/09/07/blur-detection-with-opencv/)"},{"id":"/2025-10-ranjan-week1","metadata":{"permalink":"/learnings/2025-10-ranjan-week1","source":"@site/learnings/2025-10-ranjan-week1.mdx","title":"Weekly learnings: Week1","description":"{/ truncate /}","date":"2025-10-10T00:00:00.000Z","tags":[],"readingTime":6.77,"hasTruncateMarker":true,"authors":[{"name":"Ranjan Ojha","title":"Software Engineer","url":"https://github.com/hungerarray","page":{"permalink":"/learnings/authors/ranjan"},"socials":{"github":"https://github.com/hungerarray","linkedin":"https://www.linkedin.com/in/ojha-ranjan/"},"imageURL":"https://github.com/hungerarray.png","key":"ranjan"}],"frontMatter":{"title":"Weekly learnings: Week1","date":"2025-10-10T00:00:00.000Z","type":"weekly","authors":["ranjan"],"tags":[],"draft":false},"unlisted":false,"prevItem":{"title":"AI Video Cutter - Week 2 Learnings","permalink":"/learnings/2025-10-aman-week2"},"nextItem":{"title":"AI Video Cutter - Week 1 Learnings","permalink":"/learnings/2025/01/20/ai-video-cutter-week1"}},"content":"{/* truncate */}\\n\\n# Container Checkpoint and Restore\\n\\nContainer checkpoint and restore is a facility by which any running Linux container is saved to memory \\nand later restored at a later time from the same point at which the checkpoint was created. This restore\\nis not limited to a single device and can also be in a different remote computer.\\n\\nThis was a project started by some \\"mad russians\\", and later merged into Linux Kernel, and is called \\n[`Checkpoint / Restore in Userspace (CRIU)`](https://criu.org/Main_Page). The sheer idea of saving a process that is currently \\nrunning and restoring it later in time is why this process was called mad, much like how the cryogenic\\nsleep for humans is considered a fiction. Infact, this feature comes with lots of limitations mentioned \\nlater. However, this is not an unproven technology. Such a thing has been done with VMs in the past.\\nDespite all the limitations and issues that might come up, it\'s still widely used for the simple fact that\\nit expedites the startup process.\\n\\nTo create a checkpoint we have to create what is known as a container memory snapshot.\\nA Container memory snapshot is when we basically take a copy of the entire state of a Linux Container. \\nFor this we need to copy the entire container\'s filesystem, and process tree. Process tree itself contains\\nall the memory mappings, file descriptor tables, registers, environment variables, process IDs, etc.\\n\\nBefore `CRIU` was officially available, to create such a snapshot, it was necessary for users to maintain\\ntheir own custom variant of the kernel with required features. However, with `CRIU` available in the mainline\\nLinux Kernel, most of the container runtimes now do offer this facility. Of particular mention is `gVisor`. The \\ncontainer runtime utility, `runsc` which stands for `run sandboxed container`, to it\'s counterpart `runc`, has \\nadded functionality for checkpoint and restore. Given that `gVisor` has a usermode kernel functionality, it can \\ninfact, offer more granularity and features for checkpoint and restore.\\n\\n## Limitations\\n\\n- While the restore can be made in different computer, it has to mimic the original system as much as possible. Otherwise the invariants that program expect to be maintained will be broken, and can lead to some nasty surprises down the line. \\n- The CPU that the restore is made on has to match the instruction set, where the snapshot was taken, otherwise there can be runtime issues with invalid opcode.\\n- A container that utilizes GPU, is sensitive to differences in NVIDIA driver versions and in addition to container runtime versions.\\n- Programs need to account for the fact that the machine IP might change in between restore.\\n- A Problem documented in Modal documentation is that, there are some functions in `torch.cuda` that after restoring from snapshot will initialize CUDA as having zero GPU devices. The only fix is to reinitialize `torch.cuda` again.\\n- In particular, when loading a huge snapshot, there is a significant CPU pressure as hundreds of 4KiB pages are getting loaded into memory. Such a particular workload is particularly affected by [CPU Stalls](#cpu-stalls).\\n  \\n# CPU Stalls\\n\\nTo explain a CPU stall, we first need to understand that any given process is either running on the CPU or it\'s not. \\nThere can never be in between. So what happens when we have 3 processes each granted 0.3 of CPU running on the system.\\n\\nFor the above process, lets make a few assumptions. First is our CPU currently only has a single core and a single thread. Now, it\'s not really possible for us to divide this thread into 0.3 sections and grant each of the process\\nrunning a section. As previously stated, a process is either running or not running, and when it is running it \\nutilizes 100% of the thread and when it\'s not running it utilizes 0% of the thread. So instead, CPU makes the \\ndivision in time instead. For our example, lets consider that the CPU segments 100ms of CPU time window. So, \\nwith our 3 processes, in this 100ms window, each process gets 30ms of execution time, with the last 10ms being \\nwasted and non of our processes running. Again when a new 100ms window is created all 3 process are allowed to continue their run.\\n\\nNow, the issue is if you consider a compute intensive workload, it would really benefit off of that extra 10ms of \\nCPU time, but currently the way Linux schedular works, and also how the default CPU limits on pods by \\nkubernetes works, each of the process are allocated equal precedence. Hence, for performance critical processes, \\nit makes sense to also tune your application to get more CPU time. \\n\\nHowever, in real world, we don\'t have a single CPU core, and most commercial CPU\'s nowadays offer capabilities for running 2 threads. Also of note, most of our applications when parallelized, will spawn additional threads along \\nwith the main thread inside the running process. However, the CPU time granted to each is still at process level. \\nThat means, if say our hypothetical application has 3 threads each running in the same CPU time window, now each of\\nour thread will get 10ms of actual CPU time down from 30ms of time our single thread was getting. The problem is worsened if we have 10 threads and somehow all 10 threads are granted run in the same time, in different cores, then\\neach thread only gets about 3ms of worktime. \\n\\nThis situation of having potentially adequate CPU power, but still being unable to utilize 100% of the CPU for \\ncompute is known as CPU stall.\\n\\nIn a single thread situation, the best way to deal with this issue is to grant higher priority to the process to \\nallow it to gain more CPU time. And in the case of multi threaded situation, infact it\'s actually much more\\nbeneficial to pin the process to a certain CPU core. Infact, many games actually see quite a significant performance\\nboost when their processes are pinned to a few cores rather than allowing them access to all the cores.\\n\\n# Making `syscalls` in Linux\\n\\nMost of the program we write, ultimately have to make a `syscall` to do any operation on a system. Understandably, I\\nhad a misconception that the `syscalls` are themselves exposed over `C-API` and that any language that wishes to make\\na `syscall` had to at minimum link to a lower level `C library` like `libc` that does the work for them. However, \\na particular note was we can have `golang` applications without `C-go`. While at the time, I hadn\'t connected the \\ndots, I was recently watching some video on getting the smallest kernel, and there I chanced upon the fact that to\\nmake a `syscall` you don\'t really need to bind to `C library`. You just need to be able to emit specific assembly instructions.\\n\\nA `syscall` is triggered by writing the particular `syscall` number into a particular register. Then calling the \\n`syscall`  instruction in CPU. This internally triggers a trap request, which is captured by Kernel.\\n\\nGiven that it\'s assembly code, the following description is for how to perform a `syscall` in Linux, particular\\nin x86_64 system.\\n\\n- First place the system call number into the `rax` register. `write syscall` is for instance number 1, and `read syscall` is number 0. For more `syscall` numbers visit [here](https://elixir.bootlin.com/linux/v6.13/source/arch/x86/entry/syscalls/syscall_64.tbl).\\n\\n- Place the arguments for the system call into the designated registers, \\n  - `rdi` (first argument)\\n  - `rsi` (second argument)\\n  - `rdx` (third argument)\\n  - `r10` (fourth argument)\\n  - `r8` (fifth argument)\\n  - `r9` (sixth argument)\\n  \\n    This order is specified by the calling convention.\\n    \\n- Invoke the systemcall by executing the `syscall` instruction.\\n\\n- The return value is placed inside `rax`.\\n\\n# Some tools used to deploy model\\n\\n- [ray](https://www.ray.io/) Allows for distributed GPU computation\\n  - [kuberay](https://docs.ray.io/en/latest/cluster/kubernetes/index.html) to deploy ray in kubernetes\\n  \\n- [Kubeflow](https://www.kubeflow.org/) \\n\\n  A CNCF (Cloud Native Computing Formation) project, which is the foundation of tools for AI Platforms on Kubernetes. Of particular note is that recently in a conference, Ubucon, a speaker was telling that kubeflow is almost an industry standard for deploying AI in kubernetes."},{"id":"/2025/01/20/ai-video-cutter-week1","metadata":{"permalink":"/learnings/2025/01/20/ai-video-cutter-week1","source":"@site/learnings/2025-01-20-ai-video-cutter-week1.mdx","title":"AI Video Cutter - Week 1 Learnings","description":"Been working on this video cutting tool that uses AI to find optimal cut points. Some things worked, some things I had to completely rewrite. Here\'s what went down.","date":"2025-01-20T00:00:00.000Z","tags":[{"inline":true,"label":"video-processing","permalink":"/learnings/tags/video-processing"},{"inline":true,"label":"video clips","permalink":"/learnings/tags/video-clips"},{"inline":true,"label":"video cuts","permalink":"/learnings/tags/video-cuts"},{"inline":true,"label":"whisper","permalink":"/learnings/tags/whisper"},{"inline":true,"label":"python","permalink":"/learnings/tags/python"}],"readingTime":4.11,"hasTruncateMarker":true,"authors":[{"name":"Aman Kumar Singh","title":"Software Engineer Intern","url":"https://github.com/amankumarsingh77","page":{"permalink":"/learnings/authors/aksdev"},"socials":{"linkedin":"https://www.linkedin.com/in/aksdev/","github":"https://github.com/amankumarsingh77"},"imageURL":"https://github.com/amankumarsingh77.png","key":"aksdev"}],"frontMatter":{"title":"AI Video Cutter - Week 1 Learnings","date":"2025-01-20T00:00:00.000Z","type":"weekly","authors":["aksdev"],"tags":["video-processing","video clips","video cuts","whisper","python"]},"unlisted":false,"prevItem":{"title":"Weekly learnings: Week1","permalink":"/learnings/2025-10-ranjan-week1"}},"content":"Been working on this video cutting tool that uses AI to find optimal cut points. Some things worked, some things I had to completely rewrite. Here\'s what went down.\\n\\n{/* truncate */}\\n\\n## Misunderstood the problem statement\\n\\nBuilt this whole `find_optimal_cut_point()` function that finds the optimal cutpoint in a video considering the Start point and End Point given by the user. Used Whisper for speech detection, OpenCV for scene changes, added motion analysis. Worked great.\\n\\nProblem: Understood the problem statement wrong. It was \\"Find the optimal frame to cut near the timestamp that the user gives\\". Which means that the user already knows where to cut but is not precise at a frame level, So we just ask for the estimated timestamp and then look around to find the best frame to cut.\\n\\nLearnings : Always clarify the problem statement by repeating it to the person. So you both are on the same page.\\n\\n## Weighted scoring doesn\'t work for hard constraints\\n\\nFirst version used a points system: sentence boundary +40pts, pause +30pts, scene change +30pts, etc. Add them up, pick highest score.\\n\\nBug: System kept cutting mid-word because a scene change (30pts) + low motion (20pts) outscored a sentence boundary (40pts) by itself.\\n\\nRealized some rules aren\'t negotiable. You literally cannot cut while someone is speaking. It\'s not a preference, it\'s a constraint.\\n\\nRewrote to use strict priority filtering:\\n\\n```python\\n# Priority 1: Remove all frames mid-speech (absolute)\\n# Priority 2: Remove frames with motion > 2.5 (strict)  \\n# Priority 3: Remove frames with blinks/open mouth (best effort)\\n```\\n\\nEach filter runs sequentially. If all frames fail a priority, fallback to previous level. This will be changed as if there are no frames left after filtering then its not a good range to extract any cut point.\\n\\nIf you catch yourself using weighted scoring to \\"compensate\\" for bad choices, you probably need filters instead.\\n\\n## Caching saves 90 seconds per request\\n\\nWhisper transcription: ~60s for 5min video. Scene detection: ~30s. Runs on every cut request.\\n\\nPeople want multiple clips from the same video. Transcribing the same video 5 times = 5 minutes wasted.\\n\\nAdded basic caching with video file hashes. First cut takes 90s, subsequent cuts from same video take 0.2s. Just stores the analysis results in `.video_cache/`.\\n\\n```python\\npipeline = FastPreprocessingPipeline(use_cache=True)\\nanalysis = pipeline.preprocess_video(\\"video.mp4\\")  # Slow first time, instant after\\n```\\n\\nShould\'ve done this from day one.\\n\\n## Integration is 80% of the work\\n\\nIndividual pieces are straightforward:\\n- Whisper gives word timestamps\\n- OpenCV detects scene changes\\n- Optical flow tracks motion\\n- MediaPipe finds faces/blinks\\n- Librosa analyzes audio\\n\\nMaking them work together without contradicting each other is some interesting job which I am still figuring out.\\n\\nHad to add a validation layer that checks if the selected range:\\n- Has complete sentences (not cut mid-thought)\\n- Doesn\'t have jarring visual jumps\\n- Maintains topic coherence\\n- Has stable audio (no clipping)\\n\\n## What\'s broken right now\\n\\nFace detection in \\"full\\" mode takes forever. Like 70% of total processing time. Need to profile this.\\n\\nSemantic analysis doesn\'t actually detect topic boundaries well. Using basic sentence embeddings but they\'re not good enough. Might need to try a different approach or just remove this feature.\\n\\nError messages are terrible. When the system can\'t find a good cut point it just says \\"Priority 2 failed\\" which means nothing to users.\\n\\n## Stuff that actually helped\\n\\nWhisper\'s tiny model works better than expected. Word-level timestamps are pretty accurate even with background noise and is fast enough. Might be because we are focusing on talking head/podcast videos.\\n\\nCyclopts for CLI was good. Better than argparse, handles nested commands cleanly.\\n\\nPySceneDetect saved time. Didn\'t have to write scene detection from scratch.\\n\\nMediaPipe face detection works but is slow. Might need to sample frames instead of processing every single one.\\n\\n## What I should\'ve done differently\\n\\nShould\'ve built the CLI interface first. Forces you to think about actual usage before implementation.\\n\\nShould\'ve started with the simplest possible version - just find pauses and cut there. Then add complexity. Instead I went straight to \\"multi-modal AI fusion\\" which took hours.\\n\\nShould\'ve added batch evaluation for accuracy testing from the start. Added it late and it\'s now one of the most useful features.\\n\\nShould\'ve added way more logging. When a cut looks wrong I have no idea why the system chose it. About to do it next week.\\n\\n## Random win\\n\\nThe batch evaluation system (process multiple videos, generate accuracy reports) wasn\'t even planned. Built it for testing. Turns out it\'s really useful for seeing how the system performs across different video types.\\n\\nSometimes the tools you build to test your code end up being features.\\n\\n## Next steps\\n\\nNeed to fix face detection performance. Probably sampling frames instead of processing all of them.\\n\\nSemantic analysis either needs better models or needs to be removed.\\n\\nBetter error messages so people know why a cut point was chosen or why it failed.\\n\\nAdd logging with jsonl and try to visualize each filter as to how they reject/accept a frame."}]}}')}}]);