<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.1">
<title data-rh="true">Building SceneFlow: The Intelligent Video Cutter | Blog - Vertexcover</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://blog.vertexcover.io/img/vertexcover.png"><meta data-rh="true" name="twitter:image" content="https://blog.vertexcover.io/img/vertexcover.png"><meta data-rh="true" property="og:url" content="https://blog.vertexcover.io/ai-video-cutter-technical-deep-dive"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Building SceneFlow: The Intelligent Video Cutter | Blog - Vertexcover"><meta data-rh="true" name="description" content="SceneFlow isn&#x27;t just a video cutter, it&#x27;s an intelligent editor. By combining signal processing with Multimodal AI, it finds the perfect cut points, ensuring your videos never end with awkward freezes, mid-sentence chops, or blinking eyes."><meta data-rh="true" property="og:description" content="SceneFlow isn&#x27;t just a video cutter, it&#x27;s an intelligent editor. By combining signal processing with Multimodal AI, it finds the perfect cut points, ensuring your videos never end with awkward freezes, mid-sentence chops, or blinking eyes."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-11-23T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/amankumarsingh77"><meta data-rh="true" property="article:tag" content="Video Processing,AI,Python,Automation"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://blog.vertexcover.io/ai-video-cutter-technical-deep-dive"><link data-rh="true" rel="alternate" href="https://blog.vertexcover.io/ai-video-cutter-technical-deep-dive" hreflang="en"><link data-rh="true" rel="alternate" href="https://blog.vertexcover.io/ai-video-cutter-technical-deep-dive" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://blog.vertexcover.io/ai-video-cutter-technical-deep-dive","mainEntityOfPage":"https://blog.vertexcover.io/ai-video-cutter-technical-deep-dive","url":"https://blog.vertexcover.io/ai-video-cutter-technical-deep-dive","headline":"Building SceneFlow: The Intelligent Video Cutter","name":"Building SceneFlow: The Intelligent Video Cutter","description":"SceneFlow isn't just a video cutter, it's an intelligent editor. By combining signal processing with Multimodal AI, it finds the perfect cut points, ensuring your videos never end with awkward freezes, mid-sentence chops, or blinking eyes.","datePublished":"2025-11-23T00:00:00.000Z","author":{"@type":"Person","name":"Aman Kumar Singh","description":"Software Engineer Intern","url":"https://github.com/amankumarsingh77","image":"https://github.com/amankumarsingh77.png"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://blog.vertexcover.io/","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/rss.xml" title="Blog - Vertexcover RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Blog - Vertexcover Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JS91KNQLR"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0JS91KNQLR",{})</script>




<link rel="alternate" type="application/rss+xml" href="/learnings/rss.xml" title="Blog - Vertexcover RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/learnings/atom.xml" title="Blog - Vertexcover Atom Feed">
<meta name="generator" content="vertex cover"><link rel="stylesheet" href="/assets/css/styles.9909b52a.css">
<script src="/assets/js/runtime~main.4e5a35ed.js" defer="defer"></script>
<script src="/assets/js/main.22422905.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><link rel="preload" as="image" href="https://github.com/amankumarsingh77.png"><link rel="preload" as="image" href="/img/before_last_frame.jpg"><link rel="preload" as="image" href="/img/before_last_1.5s.jpg"><link rel="preload" as="image" href="/img/sceneflow.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Vertexcover Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Vertexcover Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Vertexcover</b></a><a class="navbar__item navbar__link" href="/">Blog</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a class="navbar__item navbar__link" href="/learnings">Learnings</a><a class="navbar__item navbar__link" href="/about-us">About Us</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/vertexcover-io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/ai-video-cutter-technical-deep-dive">Building SceneFlow: The Intelligent Video Cutter</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/why-nodejs-python-avoid-linux-aio">Why Node.js and Python Don&#x27;t Use Linux&#x27;s Native Async I/O API</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/linkedin-scraper-mcp-integration">Building a LinkedIn Scraper (and Turning It Into an MCP Integration)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/strot-ai-observability-evaluation-system">How We Do Evals &amp; Observability for Agentic Systems</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/speed-up-docker-builds-on-github-actions">Speed up Docker Builds on Github actions</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/video-rendering-pipeline-for-an-ai-driven-text-to-video-platform">Video Rendering Pipeline for an AI-Driven Text-to-Video Platform</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/ml-infra-for-the-gpu-poor">ML Infra design for the GPU Poor</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/strot-is-a-api-scraper">Strot - The API Scraper</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/claude-code-context-engineering-v2">Teaching Claude Code to Work Independently</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/ai-agent-end-to-end-automated-tests-mobile">AI for End-to-End Tests (Mobile too!) with Auto Healing</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Building SceneFlow: The Intelligent Video Cutter</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-11-23T00:00:00.000Z">November 23, 2025</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/authors/aksdev"><img class="avatar__photo authorImage_XqGP" src="https://github.com/amankumarsingh77.png" alt="Aman Kumar Singh"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/authors/aksdev"><span class="authorName_yefp" translate="no">Aman Kumar Singh</span></a></div><small class="authorTitle_nd0D" title="Software Engineer Intern">Software Engineer Intern</small><div class="authorSocials_rSDt"><a href="https://www.linkedin.com/in/aksdev/" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" preserveAspectRatio="xMidYMid" viewBox="0 0 256 256" style="--dark:#0a66c2;--light:#ffffffe6" class="authorSocialIcon_XYv3 linkedinSvg_FCgI"><path d="M218.123 218.127h-37.931v-59.403c0-14.165-.253-32.4-19.728-32.4-19.756 0-22.779 15.434-22.779 31.369v60.43h-37.93V95.967h36.413v16.694h.51a39.907 39.907 0 0 1 35.928-19.733c38.445 0 45.533 25.288 45.533 58.186l-.016 67.013ZM56.955 79.27c-12.157.002-22.014-9.852-22.016-22.009-.002-12.157 9.851-22.014 22.008-22.016 12.157-.003 22.014 9.851 22.016 22.008A22.013 22.013 0 0 1 56.955 79.27m18.966 138.858H37.95V95.967h37.97v122.16ZM237.033.018H18.89C8.58-.098.125 8.161-.001 18.471v219.053c.122 10.315 8.576 18.582 18.89 18.474h218.144c10.336.128 18.823-8.139 18.966-18.474V18.454c-.147-10.33-8.635-18.588-18.966-18.453"></path></svg></a><a href="https://github.com/amankumarsingh77" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 256 250" preserveAspectRatio="xMidYMid" style="--dark:#000;--light:#fff" class="authorSocialIcon_XYv3 githubSvg_Uu4N"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z"></path></svg></a></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><div class="bg-gray-50 dark:bg-gray-800 border border-gray-200 dark:border-gray-700 rounded-lg p-4 my-5 border-l-4 border-l-blue-500 dark:border-l-blue-400"><strong class="text-blue-600 dark:text-blue-400 text-sm uppercase tracking-wide font-semibold">TL;DR:</strong><div class="mt-2 text-gray-800 dark:text-gray-200"><p><strong>SceneFlow</strong> isn&#x27;t just a video cutter, it&#x27;s an intelligent editor. By combining signal processing with <strong>Multimodal AI</strong>, it finds the <em>perfect</em> cut points, ensuring your videos never end with awkward freezes, mid-sentence chops, or blinking eyes.</p></div></div>
<p>If you&#x27;ve ever worked with AI-generated videos, you know the problem. The content is great, but the endings are... weird. The avatar freezes, stares blankly into the soul of the viewer, or cuts off mid-breath.</p>
<div style="display:flex;flex-direction:column;align-items:center;margin:20px 0"><video width="100%" style="max-width:200px;border-radius:8px" controls=""><source src="/video/before_last3s.mp4" type="video/mp4"><p>Your browser does not support the video tag.</p></video><p style="margin-top:10px;font-style:italic;color:var(--ifm-color-emphasis-600)">Notice the awkward movement of the subject at the end of this AI-generated video</p></div>
<p>To make these videos production-ready, you have to manually trim that awkward tail. For one video, it&#x27;s fine. For 1,000 personalized outreach videos? It&#x27;s a nightmare.</p>
<p>We wanted to automate this. But simple tools like <code>ffmpeg</code> or basic silence detectors aren&#x27;t enough. They don&#x27;t &quot;see&quot; the video. They don&#x27;t know if an avatar&#x27;s face looks unnatural.</p>
<p>So we built <strong>SceneFlow</strong>.</p>
<!-- -->
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-did-we-build-sceneflow">Why did we build SceneFlow<a href="#why-did-we-build-sceneflow" class="hash-link" aria-label="Direct link to Why did we build SceneFlow" title="Direct link to Why did we build SceneFlow" translate="no">​</a></h2>
<p>It started with a client project. We were working with AI-generated avatar videos, hundreds of them for personalized outreach campaigns. The content itself was impressive. The avatars looked natural, the scripts were on point, and the lip-sync was nearly flawless.</p>
<p>But there was one consistent problem: <strong>the endings were terrible</strong>.</p>
<p>Every video would finish with the avatar in some awkward state, mid-breath, eyes half-closed, mouth slightly open, or frozen in an unnatural pose. For a single video, you&#x27;d just trim it manually and move on. But when you&#x27;re processing hundreds of videos that need to be stitched together into sequences, those bad endings become a real problem. The transitions between clips looked jarring and unprofessional.</p>
<div style="display:flex;gap:20px;justify-content:center;flex-wrap:wrap;margin:20px 0"><div style="text-align:center"><p><strong>Actual Cut Frame</strong></p><img src="/img/before_last_frame.jpg" alt="Actual cut frame - awkward ending" style="max-width:250px;border-radius:8px"><p style="margin-top:8px;font-style:italic;color:var(--ifm-color-emphasis-600);font-size:0.9em">Awkward pose, eyes half-closed</p></div><div style="text-align:center"><p><strong>Expected Cut Frame</strong></p><img src="/img/before_last_1.5s.jpg" alt="Expected cut frame - natural ending" style="max-width:250px;border-radius:8px"><p style="margin-top:8px;font-style:italic;color:var(--ifm-color-emphasis-600);font-size:0.9em">Natural expression, proper ending</p></div></div>
<p>We searched for existing tools. Surely someone had solved this, right?</p>
<p>We tried silence detectors, they&#x27;d cut during pauses, but the avatar might still be moving. We experimented with motion-based cutters, they&#x27;d find still moments, but the face might look weird. Nothing understood the full picture: audio <em>and</em> video <em>and</em> facial expressions all at once.</p>
<p>So we built our own solution.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-core-intuition-what-makes-a-good-cut">The Core Intuition: What Makes a &quot;Good&quot; Cut?<a href="#the-core-intuition-what-makes-a-good-cut" class="hash-link" aria-label="Direct link to The Core Intuition: What Makes a &quot;Good&quot; Cut?" title="Direct link to The Core Intuition: What Makes a &quot;Good&quot; Cut?" translate="no">​</a></h2>
<p>Before writing any code, we asked ourselves a simple question: <em>What does a human editor actually look for when trimming a video?</em></p>
<p>Think about it. When you manually trim a video, you don&#x27;t just look at the waveform. You <em>watch</em> the person. You wait for them to finish their sentence. You check if their eyes are open. You make sure they&#x27;re not mid-gesture. You look for that brief moment of stillness where everything feels... complete.</p>
<p>That&#x27;s the intuition. <strong>A good cut happens when multiple signals align</strong>: the speaker has stopped talking, their face is composed, their body is still, and visually, the frame looks intentional rather than accidental.</p>
<p>The problem is that these signals don&#x27;t always agree. The audio might be silent, but the speaker is still moving. The face might look great, but they&#x27;re mid-word. Traditional tools optimize for <em>one</em> signal and ignore the rest.</p>
<p>SceneFlow&#x27;s approach is different: <strong>treat each signal as a vote, and find the moment where the most votes align.</strong></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-four-pillars-of-a-clean-cut">The Four Pillars of a Clean Cut<a href="#the-four-pillars-of-a-clean-cut" class="hash-link" aria-label="Direct link to The Four Pillars of a Clean Cut" title="Direct link to The Four Pillars of a Clean Cut" translate="no">​</a></h3>
<p>We identified four signals that together define a &quot;natural&quot; cut point:</p>
<p><strong>1. Speech Boundaries</strong></p>
<p>Are they still talking? We use <strong>Silero VAD</strong> (Voice Activity Detection) to detect exactly when speech ends. It gives us millisecond-accurate timestamps without needing full transcription. However, the VAD timestamps are not always very accurate, which we&#x27;ll discuss later in this blog.</p>
<p><strong>2. Motion Stillness</strong></p>
<p>Are they moving? We use optical flow to track pixel movement between frames. When the motion drops to the lowest 25%, we&#x27;ve found a moment of stillness.</p>
<p><strong>3. Facial Composure</strong></p>
<p>Do they look natural? Using <strong>InsightFace</strong>, we detect blinks (EAR), mouth openness (MAR), and head angle. If someone&#x27;s mid-blink or has their mouth open, it&#x27;s not a good frame.</p>
<p><strong>4. Visual Stability</strong></p>
<p>Is there a scene change? We detect hard cuts, fades, and dissolves. Cutting right before a transition looks accidental; cutting after looks intentional.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="handling-different-scenarios">Handling Different Scenarios<a href="#handling-different-scenarios" class="hash-link" aria-label="Direct link to Handling Different Scenarios" title="Direct link to Handling Different Scenarios" translate="no">​</a></h3>
<p>SceneFlow is optimized for two primary content types:</p>
<p><strong>Talking Head / Avatar Videos</strong> — Close-up shots with one speaker, minimal camera movement. Facial analysis dominates here. We&#x27;re strict about mouth closure, eye state, and subtle expressions. Motion thresholds are tight because any movement is noticeable in a close-up.</p>
<p><strong>Podcasts &amp; Interviews</strong> — Similar to talking heads, but with wider frames and more natural speech patterns. We weight speech boundaries heavily and allow slightly more tolerance for motion.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-sceneflow-implements-this">How SceneFlow Implements This<a href="#how-sceneflow-implements-this" class="hash-link" aria-label="Direct link to How SceneFlow Implements This" title="Direct link to How SceneFlow Implements This" translate="no">​</a></h2>
<p>Here&#x27;s the high-level flow:</p>
<p><strong>Step 1: Find the speech end</strong> — When you provide a video, we first run Silero VAD to detect the end timestamp of the last speech segment. This gives us a starting point for analysis.</p>
<p><strong>Step 2: Analyze frames visually</strong> — From that timestamp, we analyze each frame for:</p>
<ul>
<li><strong>EAR</strong> (Eye Aspect Ratio) — Are they blinking?</li>
<li><strong>MAR</strong> (Mouth Aspect Ratio) — Is their mouth open?</li>
<li><strong>Motion</strong> — Is the subject in motion?</li>
<li><strong>Stability</strong> — Is the frame stable?</li>
</ul>
<p><strong>Step 3: Score and rank</strong> — Each frame gets a composite score based on these metrics. We then rank all frames from 1 to n, where rank 1 is the best candidate and rank n is the worst.</p>
<p><strong>Step 4: Select the best</strong> — The highest-ranked frame becomes the cut point.</p>
<p>The ranking prioritizes frames where the speaker has finished talking, isn&#x27;t blinking, has their mouth closed, and is relatively still. If no perfect frame exists, we pick the best available.</p>
<img src="/img/sceneflow.png" alt="SceneFlow Architecture" style="width:70%;display:block;margin:0 auto">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="real-world-challenges-we-faced">Real-World Challenges We Faced<a href="#real-world-challenges-we-faced" class="hash-link" aria-label="Direct link to Real-World Challenges We Faced" title="Direct link to Real-World Challenges We Faced" translate="no">​</a></h2>
<p>Building SceneFlow wasn&#x27;t just about wiring up APIs. We hit some genuinely hard problems.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-vad-timestamp-inaccuracy-problem">The VAD Timestamp Inaccuracy Problem<a href="#the-vad-timestamp-inaccuracy-problem" class="hash-link" aria-label="Direct link to The VAD Timestamp Inaccuracy Problem" title="Direct link to The VAD Timestamp Inaccuracy Problem" translate="no">​</a></h3>
<p>Silero VAD&#x27;s end timestamps are always a few frames too late. This means we&#x27;re analyzing extra frames where the speaker is already quiet, potentially missing the perfect cut point.</p>
<p><strong>Our Fix: Energy-Based Refinement</strong></p>
<p>We use VAD as a rough guide, then look backwards for a sudden energy drop (≥8 dB) in the audio. This drop marks the actual speech end, giving us frame-level precision and ensuring we never miss that ideal cut moment.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-mid-gesture-problem">The &quot;Mid-Gesture&quot; Problem<a href="#the-mid-gesture-problem" class="hash-link" aria-label="Direct link to The &quot;Mid-Gesture&quot; Problem" title="Direct link to The &quot;Mid-Gesture&quot; Problem" translate="no">​</a></h3>
<p>Optical flow is great at detecting <em>large</em> motion, but bad at detecting <em>subtle</em> gestures. A speaker might be perfectly still from the waist up, but their hand is moving off-screen. Early versions of SceneFlow would happily cut during these moments, making the final frame look awkward.</p>
<p>We fixed this by implementing <strong>region-of-interest (ROI) weighting</strong>. The algorithm gives more weight to motion near the speaker&#x27;s face and less weight to the edges of the frame. Now it can distinguish between &quot;speaker is gesturing&quot; and &quot;a car drove by in the background.&quot;</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="when-to-use-sceneflow">When to Use SceneFlow<a href="#when-to-use-sceneflow" class="hash-link" aria-label="Direct link to When to Use SceneFlow" title="Direct link to When to Use SceneFlow" translate="no">​</a></h2>
<p>SceneFlow is designed for high-quality, automated video workflows. It excels in situations where precision and polish matter.</p>
<ul>
<li><strong>AI Avatar Videos</strong>: Perfect for cleaning up the awkward starts and stops of generated content from platforms like HeyGen or Synthesia.</li>
<li><strong>Webinar &amp; Podcast Clips</strong>: Extracting short, punchy clips from long-form content without cutting off speakers mid-sentence.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="see-it-in-action">See it in Action<a href="#see-it-in-action" class="hash-link" aria-label="Direct link to See it in Action" title="Direct link to See it in Action" translate="no">​</a></h2>
<p>Here&#x27;s a comparison of a raw AI-generated video versus one processed by SceneFlow. Notice how the &quot;After&quot; clip ends naturally without the awkward freeze.</p>
<div style="display:flex;gap:20px;justify-content:center;flex-wrap:wrap;margin-top:30px"><div style="text-align:center"><p><strong>Before (Raw)</strong></p><video width="100%" style="max-width:300px;border-radius:8px" controls=""><source src="/video/example_input4.mp4" type="video/mp4"><p>Your browser does not support the video tag.</p></video></div><div style="text-align:center"><p><strong>After (SceneFlow)</strong></p><video width="100%" style="max-width:300px;border-radius:8px" controls=""><source src="/video/example_output4.mp4" type="video/mp4"><p>Your browser does not support the video tag.</p></video></div></div>
<div style="display:flex;gap:20px;justify-content:center;flex-wrap:wrap;margin-top:30px"><div style="text-align:center"><p><strong>Before (Raw)</strong></p><video width="100%" style="max-width:300px;border-radius:8px" controls=""><source src="/video/example_input2.mp4" type="video/mp4"><p>Your browser does not support the video tag.</p></video></div><div style="text-align:center"><p><strong>After (SceneFlow)</strong></p><video width="100%" style="max-width:300px;border-radius:8px" controls=""><source src="/video/example_output2.mp4" type="video/mp4"><p>Your browser does not support the video tag.</p></video></div></div>
<div style="display:flex;gap:20px;justify-content:center;flex-wrap:wrap;margin-top:30px"><div style="text-align:center"><p><strong>Before (Raw)</strong></p><video width="100%" style="max-width:300px;border-radius:8px" controls=""><source src="/video/example_input3.mp4" type="video/mp4"><p>Your browser does not support the video tag.</p></video></div><div style="text-align:center"><p><strong>After (SceneFlow)</strong></p><video width="100%" style="max-width:300px;border-radius:8px" controls=""><source src="/video/example_output3.mp4" type="video/mp4"><p>Your browser does not support the video tag.</p></video></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-use-it">How to Use It<a href="#how-to-use-it" class="hash-link" aria-label="Direct link to How to Use It" title="Direct link to How to Use It" translate="no">​</a></h2>
<p>SceneFlow is built as a powerful CLI tool that fits right into your existing pipelines.</p>
<p>Check out the project on GitHub: <a href="https://github.com/vertexcover-io/sceneflow" target="_blank" rel="noopener noreferrer">https://github.com/vertexcover-io/sceneflow</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="basic-cut">Basic Cut<a href="#basic-cut" class="hash-link" aria-label="Direct link to Basic Cut" title="Direct link to Basic Cut" translate="no">​</a></h3>
<p>Cut a video at a specific timestamp, but let SceneFlow find the optimal spot nearby:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sceneflow cut single input.mp4 630.0 output.mp4 --mode balanced</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion" translate="no">​</a></h2>
<p>SceneFlow works exceptionally well with AI-generated podcasts and talking head videos, delivering clean, professional cuts every time.</p>
<p>Our goal is to expand this into a comprehensive rough-cut tool for real-world podcasts, making it the go-to solution for automated video editing.</p>
<p>The code is open source. Give it a spin and stop scrubbing through timelines manually.</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" title="Video Processing" class="tag_zVej tagRegular_sFm0" href="/tags/video-processing">Video Processing</a></li><li class="tag_QGVx"><a rel="tag" title="Artificial Intelligence" class="tag_zVej tagRegular_sFm0" href="/tags/ai">AI</a></li><li class="tag_QGVx"><a rel="tag" title="Python Programming" class="tag_zVej tagRegular_sFm0" href="/tags/python">Python</a></li><li class="tag_QGVx"><a rel="tag" title="Automation" class="tag_zVej tagRegular_sFm0" href="/tags/automation">Automation</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/why-nodejs-python-avoid-linux-aio"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Why Node.js and Python Don&#x27;t Use Linux&#x27;s Native Async I/O API</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#why-did-we-build-sceneflow" class="table-of-contents__link toc-highlight">Why did we build SceneFlow</a></li><li><a href="#the-core-intuition-what-makes-a-good-cut" class="table-of-contents__link toc-highlight">The Core Intuition: What Makes a &quot;Good&quot; Cut?</a><ul><li><a href="#the-four-pillars-of-a-clean-cut" class="table-of-contents__link toc-highlight">The Four Pillars of a Clean Cut</a></li><li><a href="#handling-different-scenarios" class="table-of-contents__link toc-highlight">Handling Different Scenarios</a></li></ul></li><li><a href="#how-sceneflow-implements-this" class="table-of-contents__link toc-highlight">How SceneFlow Implements This</a></li><li><a href="#real-world-challenges-we-faced" class="table-of-contents__link toc-highlight">Real-World Challenges We Faced</a><ul><li><a href="#the-vad-timestamp-inaccuracy-problem" class="table-of-contents__link toc-highlight">The VAD Timestamp Inaccuracy Problem</a></li><li><a href="#the-mid-gesture-problem" class="table-of-contents__link toc-highlight">The &quot;Mid-Gesture&quot; Problem</a></li></ul></li><li><a href="#when-to-use-sceneflow" class="table-of-contents__link toc-highlight">When to Use SceneFlow</a></li><li><a href="#see-it-in-action" class="table-of-contents__link toc-highlight">See it in Action</a></li><li><a href="#how-to-use-it" class="table-of-contents__link toc-highlight">How to Use It</a><ul><li><a href="#basic-cut" class="table-of-contents__link toc-highlight">Basic Cut</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/vertexcover-io" target="_blank" rel="noopener noreferrer" class="footer__link-item footer-github-link" aria-label="GitHub repository">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Codeshelf, Inc.</div></div></div></footer></div>
</body>
</html>